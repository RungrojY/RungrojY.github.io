[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "🙂👉Click here to download the PDF"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Runroj Yadbantung’s memos",
    "section": "",
    "text": "Bank Transactions using Kafka Streams\n\n\n\n\n\n\n\njava\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nWordCount using Kafka Streams\n\n\n\n\n\n\n\njava\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nUsing Apache Spark\n\n\n\n\n\n\n\nbash\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nDebugging a Hadoop Java Code\n\n\n\n\n\n\n\nbash\n\n\njava\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nUsing Hadoop\n\n\n\n\n\n\n\nbash\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nSetup Hadoop Cluster\n\n\n\n\n\n\n\nbash\n\n\nxml\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nUsing Redux in Next.js\n\n\n\n\n\n\n\ntypescript\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nCreate an API of Neo4j Database in Next.js\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nUsing Cypher in Python\n\n\n\n\n\n\n\ncypher\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nCypher Query\n\n\n\n\n\n\n\ncypher\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nMy TypeScript Journey\n\n\n\n\n\n\n\ntypescript\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nUsing Multipass\n\n\n\n\n\n\n\nbash\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nUsing rsync in Linux\n\n\n\n\n\n\n\nbash\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/neo4j-in-nextjs/index.html",
    "href": "posts/neo4j-in-nextjs/index.html",
    "title": "Connect to Neo4j Database in Node.js",
    "section": "",
    "text": "const neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"สมอ\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/neo4j-in-nextjs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "href": "posts/neo4j-in-nextjs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "title": "Connect to Neo4j Database in Node.js",
    "section": "2 Firstly, create simple API in pages/hello.ts",
    "text": "2 Firstly, create simple API in pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from 'next';\n\nexport default function handler(req: NextApiRequest, res: NextApiResponse) {\n\n  res.json({\n    message: 'Hello World!',\n  });\n}"
  },
  {
    "objectID": "posts/neo4j-in-nextjs/index.html#create-a-database-connection-pageshello.ts",
    "href": "posts/neo4j-in-nextjs/index.html#create-a-database-connection-pageshello.ts",
    "title": "Connect to Neo4j Database in Node.js",
    "section": "3 Create a database connection pages/hello.ts",
    "text": "3 Create a database connection pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from \"next\";\nimport neo4j, { Driver, Session, Result, Record } from \"neo4j-driver\";\nimport { log } from \"console\";\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\n// Define the type for properties\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\n// API route handler\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  const session: Session = driver.session();\n\n  try {\n    // Perform your Neo4j queries and logic here\n    const result = await session.run(\"MATCH (n) RETURN n LIMIT 5\");\n    \n    // const nodes: Record&lt;string, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n    const nodes: Dict&lt;PropertyKey, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n\n    console.log(nodes);\n\n    const jsonData = JSON.stringify(nodes);\n\n    res.status(200).json({ data: jsonData });\n\n  } catch (error) {\n    console.error(\"Error executing Neo4j query:\", error);\n    res.status(500).json({ error: \"Internal Server Error\" });\n  } finally {\n    session.close();\n  }\n}"
  },
  {
    "objectID": "posts/neo4j-in-nextjs/index.html#firstly-create-simple-api-in-node.js",
    "href": "posts/neo4j-in-nextjs/index.html#firstly-create-simple-api-in-node.js",
    "title": "Connect to Neo4j Database in Node.js",
    "section": "",
    "text": "const neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"สมอ\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html",
    "href": "posts/typescript_basic_usage/index.html",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User = {name: \"John\", age: \"30\", sex: \"male\"}\n\n\n\nconst data = {name: \"John\", age:30}\ntype User = typeof data;\n\nconst data2:User = {name: \"John\", age:30}\nconst data3:typeof data = {name: \"John\", age:30}\n\n\n\ntype User = {\n  name: string;\n  age: number | string;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User = {name: \"John\", age: \"30\"}\n\n\n\ntype User = {\n  name: string | undefined;\n  age?: number | string;\n};\n\nconst data1:User = {name: \"John\"}\nconst data2:User = {name: undefined}\nconst data3:User = {age: 1} // Type error, missing `name`\n\n\n\ntype User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User[\"name\"] = \"John\"\n\n\n\ntype User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\";\n};\n\ntype User2 = Omit&lt;User, \"sex\"&gt;;\n\n// Usage example:\nconst user: User2 = {\n  name: \"John\",\n  age: 25,\n};\n\n\n\ntype User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:Pick&lt;User, \"name\"&gt; = {name: \"John\"}\nconst data:Pick&lt;User, \"name\" | \"age\"&gt; = {name: \"John\", age: 30}\n\n\n\ntype User = {\n  name: string;\n  age: number;\n}\n\nconst data1: User = {age: 22, name: \"Jenny\"}\ndata1.age = 23\n\nconst data2: Readonly&lt;User&gt; = {age: 22, name: \"Jenny\"}\ndata2.age = 23 // type error\n\n\n\ntype Prop = PropertyKey // string | number | symbol\n\nconst data:Prop = \"John\"\nconst data1:Prop = 21\nconst data2:Prop = false // type error"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#define-type",
    "href": "posts/typescript_basic_usage/index.html#define-type",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User = {name: \"John\", age: \"30\", sex: \"male\"}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#define-type-1",
    "href": "posts/typescript_basic_usage/index.html#define-type-1",
    "title": "My TypeScript Journey",
    "section": "2.2 Define type",
    "text": "2.2 Define type\ntype Props&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\ntype Props2 = Props&lt;number, string&gt;\n// {\n//   [x: number]: string;\n// }\n\nconst data:Props2 = {\n  0: \"apple\",\n  1: \"orange\"\n}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#define-type-2",
    "href": "posts/typescript_basic_usage/index.html#define-type-2",
    "title": "TypeScript Journey",
    "section": "2.1 Define type",
    "text": "2.1 Define type\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};"
  },
  {
    "objectID": "posts/cypher-import-export/index.html",
    "href": "posts/cypher-import-export/index.html",
    "title": "Create Cypher for JSON",
    "section": "",
    "text": "match (n) return n\nSELECT * from mytable where id = 1\nSELECT * from mytable where id = 2\n\n\nterminal\n\nsudo ufw status\nsudo ufw reload\n\n\n\nmatplotlib.py\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n# highlight-style: arrow\nimport matplotlib.pyplot as plt\nprint(\"Hello\")\n```{python}\n1 + 1\n```\n\nlibrary(ggplot2)\ndat &lt;- data.frame(cond = rep(c(\"A\", \"B\"), each=10),\n                  xvar = 1:20 + rnorm(20,sd=3),\n                  yvar = 1:20 + rnorm(20,sd=3))\n\nggplot(dat, aes(x=xvar, y=yvar)) +\n  geom_point(shape=1) + \n  geom_smooth() \n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\nFigure 1: Temperature and ozone level."
  },
  {
    "objectID": "posts/cypher-import-export/index.html#firstly-create-simple-api-in-node.js",
    "href": "posts/cypher-import-export/index.html#firstly-create-simple-api-in-node.js",
    "title": "Create Cypher for JSON",
    "section": "",
    "text": "match (n) return n\nSELECT * from mytable where id = 1\nSELECT * from mytable where id = 2\n\n\nterminal\n\nsudo ufw status\nsudo ufw reload\n\n\n\nmatplotlib.py\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n# highlight-style: arrow\nimport matplotlib.pyplot as plt\nprint(\"Hello\")\n```{python}\n1 + 1\n```\n\nlibrary(ggplot2)\ndat &lt;- data.frame(cond = rep(c(\"A\", \"B\"), each=10),\n                  xvar = 1:20 + rnorm(20,sd=3),\n                  yvar = 1:20 + rnorm(20,sd=3))\n\nggplot(dat, aes(x=xvar, y=yvar)) +\n  geom_point(shape=1) + \n  geom_smooth() \n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\nFigure 1: Temperature and ozone level."
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#get-specific-type",
    "href": "posts/typescript_basic_usage/index.html#get-specific-type",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User[\"name\"] = \"John\""
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#get-specific-type-1",
    "href": "posts/typescript_basic_usage/index.html#get-specific-type-1",
    "title": "TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:Pick&lt;User, \"name\"&gt; = {name: \"John\"}\nconst data:Pick&lt;User, \"name\" | \"age\"&gt; = {name: \"John\", age: 30}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#limit-keys-object-with-pick",
    "href": "posts/typescript_basic_usage/index.html#limit-keys-object-with-pick",
    "title": "TypeScript Journey",
    "section": "2.2 Limit keys object with Pick",
    "text": "2.2 Limit keys object with Pick\nconst person = {\n  name: \"John\",\n  age: 31,\n  sex: \"male\",\n};\n\ndeclare function pick&lt;T, K extends keyof T&gt; (obj: T, ...keys: K[]): Pick&lt;T, K&gt;;\n\nconst data = pick(person, \"name\", \"age\");\n// data can be only `data.name` or `data.age`"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#get-type-from-data",
    "href": "posts/typescript_basic_usage/index.html#get-type-from-data",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "const data = {name: \"John\", age:30}\ntype User = typeof data;\n\nconst data2:User = {name: \"John\", age:30}\nconst data3:typeof data = {name: \"John\", age:30}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#add-more-choices",
    "href": "posts/typescript_basic_usage/index.html#add-more-choices",
    "title": "TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number | string;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User = {name: \"John\", age: \"30\"}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#optional-type",
    "href": "posts/typescript_basic_usage/index.html#optional-type",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string | undefined;\n  age?: number | string;\n};\n\nconst data1:User = {name: \"John\"}\nconst data2:User = {name: undefined}\nconst data3:User = {age: 1} // Type error, missing `name`"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#read-only-type",
    "href": "posts/typescript_basic_usage/index.html#read-only-type",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n}\n\nconst data1: User = {age: 22, name: \"Jenny\"}\ndata1.age = 23\n\nconst data2: Readonly&lt;User&gt; = {age: 22, name: \"Jenny\"}\ndata2.age = 23 // type error"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#limit-key-access-with-pick",
    "href": "posts/typescript_basic_usage/index.html#limit-key-access-with-pick",
    "title": "My TypeScript Journey",
    "section": "2.3 Limit key access with Pick",
    "text": "2.3 Limit key access with Pick\nconst person = {\n  name: \"John\",\n  age: 31,\n  sex: \"male\",\n};\n\ndeclare function pick&lt;T, K extends keyof T&gt; (obj: T, ...keys: K[]): Pick&lt;T, K&gt;;\n\nconst data = pick(person, \"name\", \"age\");\n// data can be only `data.name` or `data.age`"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#type-on-function",
    "href": "posts/typescript_basic_usage/index.html#type-on-function",
    "title": "TypeScript Journey",
    "section": "2.4 Type on function",
    "text": "2.4 Type on function"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#add-more-choice-on-type",
    "href": "posts/typescript_basic_usage/index.html#add-more-choice-on-type",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number | string;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User = {name: \"John\", age: \"30\"}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#exclude-some-types",
    "href": "posts/typescript_basic_usage/index.html#exclude-some-types",
    "title": "TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:Pick&lt;User, \"name\"&gt; = {name: \"John\"}\nconst data:Pick&lt;User, \"name\" | \"age\"&gt; = {name: \"John\", age: 30}"
  },
  {
    "objectID": "posts/cypher-query/index.html",
    "href": "posts/cypher-query/index.html",
    "title": "Cypher Query",
    "section": "",
    "text": "CALL db.schema.visualization()\n\n\n\nMATCH (n) RETURN n\n\n\n\nMATCH (n) \nWHERE NOT (n)-[]-()\nRETURN n\n\n\n\nMATCH (n)-[:HAS_INFO]-&gt;(m)-[:CITE]-&gt;(p) RETURN n, m, p\n\n\n\nMATCH (n:Herb:Drug) RETURN n\n\n\n\nMATCH (n)\nWHERE ID(n) = 1\nRETURN n\n\n\n\nMATCH (n:`Country`)-[r]-(m)\nWHERE n.text CONTAINS 'Thai'\nRETURN n, r, m\n\n\n\nMATCH (n:`Geographic Distribution`) RETURN n\n\n\n\nMATCH (n) RETURN COUNT(n) as count\n\n\n\nMATCH (n) DETACH DELETE n\n\n\n\nMATCH (n)\nREMOVE n.propertyName\n\n\n\nMATCH (n:Person)\nSET n.dummy = 0\n\n\n\nCREATE (:User {name: \"Charlie\", age: 35, gender: \"male\"});\nCREATE (:User {name: \"Dave\", age: 28, gender: \"male\"});\n\n\n\nMATCH (charlie:User {name: \"Charlie\"}), (dave:User {name: \"Dave\"})\nCREATE (charlie)&lt;-[:FRIENDS_WITH {date: \"Jan 19\"}]-(dave);"
  },
  {
    "objectID": "posts/cypher-query/index.html#firstly-create-simple-api-in-node.js",
    "href": "posts/cypher-query/index.html#firstly-create-simple-api-in-node.js",
    "title": "Cypher Query",
    "section": "",
    "text": "match (n) return n"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#property-key",
    "href": "posts/typescript_basic_usage/index.html#property-key",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type Prop = PropertyKey // string | number | symbol\n\nconst data:Prop = \"John\"\nconst data1:Prop = 21\nconst data2:Prop = false // type error"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#type-like-a-funtion",
    "href": "posts/typescript_basic_usage/index.html#type-like-a-funtion",
    "title": "My TypeScript Journey",
    "section": "2.1 Type like a funtion",
    "text": "2.1 Type like a funtion\ntype User = {\n  name: string;\n  age: number;\n  vegetarian: boolean;\n} \n\ntype Prop&lt;T, K extends keyof T&gt; = T[K]\n\ntype AA = Prop&lt;User, \"name\"&gt;\n//string\n\ntype BB = Prop&lt;User, \"name\" | \"age\"&gt;\n//string | number\n\ntype CC = Prop&lt;User, keyof User&gt;\n//string | number | boolean"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#promise-and-awaited-type",
    "href": "posts/typescript_basic_usage/index.html#promise-and-awaited-type",
    "title": "My TypeScript Journey",
    "section": "2.4 Promise and Awaited type",
    "text": "2.4 Promise and Awaited type\ntype NonPromiseType = number;\ntype PromiseType = Promise&lt;NonPromiseType&gt;;\ntype PPP = Awaited&lt;PromiseType&gt;"
  },
  {
    "objectID": "posts/cypher-query/index.html#get-started",
    "href": "posts/cypher-query/index.html#get-started",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) RETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#get-started-1",
    "href": "posts/cypher-query/index.html#get-started-1",
    "title": "Cypher Query",
    "section": "",
    "text": "match (n) return n"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-node-without-relationship",
    "href": "posts/cypher-query/index.html#match-node-without-relationship",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) \nWHERE NOT (n)-[]-()\nRETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#delete-all-nodes-and-edges",
    "href": "posts/cypher-query/index.html#delete-all-nodes-and-edges",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) DETACH DELETE n"
  },
  {
    "objectID": "posts/neo4j-in-nodejs/index.html",
    "href": "posts/neo4j-in-nodejs/index.html",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "",
    "text": "Terminal&gt;node\n\nconst neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"Ginger\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/neo4j-in-nodejs/index.html#firstly-create-simple-api-in-node.js",
    "href": "posts/neo4j-in-nodejs/index.html#firstly-create-simple-api-in-node.js",
    "title": "Connect to Neo4j Database in Node.js",
    "section": "",
    "text": "const neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"สมอ\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/neo4j-in-nodejs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "href": "posts/neo4j-in-nodejs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "2 Firstly, create simple API in pages/hello.ts",
    "text": "2 Firstly, create simple API in pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from 'next';\n\nexport default function handler(req: NextApiRequest, res: NextApiResponse) {\n\n  res.json({\n    message: 'Hello World!',\n  });\n}"
  },
  {
    "objectID": "posts/neo4j-in-nodejs/index.html#create-a-database-connection-pageshello.ts",
    "href": "posts/neo4j-in-nodejs/index.html#create-a-database-connection-pageshello.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "3 Create a database connection pages/hello.ts",
    "text": "3 Create a database connection pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from \"next\";\nimport neo4j, { Driver, Session, Result, Record } from \"neo4j-driver\";\nimport { log } from \"console\";\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\n// Define the type for properties\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\n// API route handler\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  const session: Session = driver.session();\n\n  try {\n    // Perform your Neo4j queries and logic here\n    const result = await session.run(\"MATCH (n) RETURN n LIMIT 5\");\n    \n    // const nodes: Record&lt;string, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n    const nodes: Dict&lt;PropertyKey, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n\n    console.log(nodes);\n\n    const jsonData = JSON.stringify(nodes);\n\n    res.status(200).json({ data: jsonData });\n\n  } catch (error) {\n    console.error(\"Error executing Neo4j query:\", error);\n    res.status(500).json({ error: \"Internal Server Error\" });\n  } finally {\n    session.close();\n  }\n}"
  },
  {
    "objectID": "posts/cypher-query/index.html#count-all-node",
    "href": "posts/cypher-query/index.html#count-all-node",
    "title": "Cypher Query",
    "section": "",
    "text": "match (n) return count(n) as count"
  },
  {
    "objectID": "posts/cypher-query/index.html#remove-a-property-from-all-nodes",
    "href": "posts/cypher-query/index.html#remove-a-property-from-all-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n)\nREMOVE n.propertyName"
  },
  {
    "objectID": "posts/cypher-query/index.html#remove-a-property-from-all-nodes-1",
    "href": "posts/cypher-query/index.html#remove-a-property-from-all-nodes-1",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n)\nWHERE ID(n) = 1\nRETURN n"
  },
  {
    "objectID": "posts/neo4j-in-nodejs/index.html#test-a-connection-in-nodejs",
    "href": "posts/neo4j-in-nodejs/index.html#test-a-connection-in-nodejs",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "",
    "text": "Terminal&gt;node\n\nconst neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"Ginger\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-all-nodes",
    "href": "posts/cypher-query/index.html#match-all-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) RETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-nodes-without-relationship",
    "href": "posts/cypher-query/index.html#match-nodes-without-relationship",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) \nWHERE NOT (n)-[]-()\nRETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#count-all-nodes",
    "href": "posts/cypher-query/index.html#count-all-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) RETURN COUNT(n) as count"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-node-with-an-id",
    "href": "posts/cypher-query/index.html#match-node-with-an-id",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n)\nWHERE ID(n) = 1\nRETURN n"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html",
    "href": "posts/neo4j-nextjs/index.html",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "",
    "text": "Terminal&gt;node\n\nconst neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"Ginger\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html#test-a-connection-in-nodejs",
    "href": "posts/neo4j-nextjs/index.html#test-a-connection-in-nodejs",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "",
    "text": "Terminal&gt;node\n\nconst neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"Ginger\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "href": "posts/neo4j-nextjs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "2 Firstly, create simple API in pages/hello.ts",
    "text": "2 Firstly, create simple API in pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from 'next';\n\nexport default function handler(req: NextApiRequest, res: NextApiResponse) {\n\n  res.json({\n    message: 'Hello World!',\n  });\n}"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html#create-a-database-connection-pageshello.ts",
    "href": "posts/neo4j-nextjs/index.html#create-a-database-connection-pageshello.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "3 Create a database connection pages/hello.ts",
    "text": "3 Create a database connection pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from \"next\";\nimport neo4j, { Driver, Session, Result, Record } from \"neo4j-driver\";\nimport { log } from \"console\";\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\n// Define the type for properties\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\n// API route handler\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  const session: Session = driver.session();\n\n  try {\n    // Perform your Neo4j queries and logic here\n    const result = await session.run(\"MATCH (n) RETURN n LIMIT 5\");\n    \n    // const nodes: Record&lt;string, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n    const nodes: Dict&lt;PropertyKey, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n\n    console.log(nodes);\n\n    const jsonData = JSON.stringify(nodes);\n\n    res.status(200).json({ data: jsonData });\n\n  } catch (error) {\n    console.error(\"Error executing Neo4j query:\", error);\n    res.status(500).json({ error: \"Internal Server Error\" });\n  } finally {\n    session.close();\n  }\n}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#assert-type-on-unknow-type",
    "href": "posts/typescript_basic_usage/index.html#assert-type-on-unknow-type",
    "title": "My TypeScript Journey",
    "section": "3.1 Assert type on unknow type",
    "text": "3.1 Assert type on unknow type\nWe use a type assertion (error as Error).message to assert that the error object has the type Error, which guarantees the presence of the message property. This allows TypeScript to recognize and accept the error.message access without any type error.\ntry {\n    const result = await dbManager.runQuery(query);\n    res.status(200).json(result.records);\n  } catch (error) {\n    // res.status(500).json({ error: error.message }); //type error\n    res.status(500).json({ error: (error as Error).message });\n  }"
  },
  {
    "objectID": "posts/using-python-environment/index.html",
    "href": "posts/using-python-environment/index.html",
    "title": "Using Python Environment",
    "section": "",
    "text": "Virtual environments help avoid conflicts with other Python packages.\n\n\npython -m venv keras-env\n\n\n\n.\\keras-env\\Scripts\\activate"
  },
  {
    "objectID": "posts/using-python-environment/index.html#set-up-a-virtual-environment",
    "href": "posts/using-python-environment/index.html#set-up-a-virtual-environment",
    "title": "Using Python Environment",
    "section": "",
    "text": "python -m venv keras-env"
  },
  {
    "objectID": "posts/using-python-environment/index.html#activate-the-virtual-environment-on-windows",
    "href": "posts/using-python-environment/index.html#activate-the-virtual-environment-on-windows",
    "title": "Using Python Environment",
    "section": "",
    "text": ".\\keras-env\\Scripts\\activate"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#nest-of-try-catch-block-to-resolve-error",
    "href": "posts/typescript_basic_usage/index.html#nest-of-try-catch-block-to-resolve-error",
    "title": "TypeScript Journey",
    "section": "3.1 Nest of try-catch block to resolve error",
    "text": "3.1 Nest of try-catch block to resolve error\ntry {\n    try {\n    \n  } catch (error) {\n    \n  }\n  }"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#using-symbol-in-object",
    "href": "posts/typescript_basic_usage/index.html#using-symbol-in-object",
    "title": "My TypeScript Journey",
    "section": "3.2 Using Symbol in Object",
    "text": "3.2 Using Symbol in Object\nChange the type constraint in the Dict definition to only allow string keys:\nexport type Dict&lt;T extends string, U&gt; = {\n  [K in T]: U;\n};\n\nconst user: Dict&lt;\"name\" | \"email\", string&gt; = {\n  name: \"John\",\n  email: \"john@gmail.com\",\n};"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#using-symbol-in-object-by-defining",
    "href": "posts/typescript_basic_usage/index.html#using-symbol-in-object-by-defining",
    "title": "My TypeScript Journey",
    "section": "3.3 Using Symbol in Object by defining",
    "text": "3.3 Using Symbol in Object by defining\nKeep the Dict type definition as it is, but use symbols as keys in the user object:\nexport type Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\nconst nameKey = Symbol(\"name\");\nconst emailKey = Symbol(\"email\");\n\nconst user: Dict&lt;symbol, string&gt; = {\n  [nameKey]: \"John\",\n  [emailKey]: \"john@gmail.com\",\n};"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#exclude-with-omit",
    "href": "posts/typescript_basic_usage/index.html#exclude-with-omit",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\";\n};\n\ntype User2 = Omit&lt;User, \"sex\"&gt;;\n\n// Usage example:\nconst user: User2 = {\n  name: \"John\",\n  age: 25,\n};"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#exclude-with-pick",
    "href": "posts/typescript_basic_usage/index.html#exclude-with-pick",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:Pick&lt;User, \"name\"&gt; = {name: \"John\"}\nconst data:Pick&lt;User, \"name\" | \"age\"&gt; = {name: \"John\", age: 30}"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html",
    "href": "posts/setup-hadoop-cluster/index.html",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "For example, I will use Ubuntu Server 20.04.6 in this setup. Install it in Virtual Box and don’t forget to install ssh I have set CPU 2 cors, Memory 4 GB and Space 25 GB. By default, the network is set as NAT, so that the VM can use internet.\n\n\n\nInstall java version 8 for Hadoop. This is the required java version.\nsudo apt update\nsudo apt install openjdk-8-jdk\nThe next is to set the variables to ~/.bashrc. To add a literal string to the ~/.bashrc file in Ubuntu, you can use the echo command with single quotes to prevent variable expansion.\necho 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' &gt;&gt; ~/.bashrc\necho 'export PATH=$JAVA_HOME/bin:$PATH' &gt;&gt; ~/.bashrc\n# reload\nsource ~/.bashrc\n# check\necho $JAVA_HOME\nUsing single quotes (') ensures that the string is treated as a literal and preserves the $PATH variable for future evaluation when the .bashrc file is sourced.\n\n\n\nLet’s visit released page and download hadoop-3.3.5.tar.gz or using the following command.\nwget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\nExtract the file with the command:\ntar -zxvf hadoop-3.3.5.tar.gz\n# remove a tar file\nrm hadoop-3.3.5.tar.gz\nIn order to run the Hadoop command without specifying the directory, let add the variables. Assume that the directory of hadoop is /home/ubuntu/hadoop-3.3.5, you can use:\necho 'export HADOOP_HOME=/home/ubuntu/hadoop-3.3.5' &gt;&gt; ~/.bashrc\necho 'export PATH=$PATH:$HADOOP_HOME/bin' &gt;&gt; ~/.bashrc\necho 'export PATH=$PATH:$HADOOP_HOME/sbin' &gt;&gt; ~/.bashrc\n# reload\nsource ~/.bashrc\n# check\necho $HADOOP_HOME\n\n\n\nLet’s add JAVA_HOME to flies located in ~/hadoop-3.3.5/etc/hadoop which are\n\n\n\nfilename\n\n\n\n\nmapred-env.sh\n\n\nhadoop-env.sh\n\n\nyarn-env.sh\n\n\n\ncd ~/hadoop-3.3.5/etc/hadoop\n\n# see file list in details\nls -lstr\nls -lstr *env*.sh\nOpen three files with the following commands\nvi ~/hadoop-3.3.5/etc/hadoop/mapred-env.sh\nvi ~/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\nvi ~/hadoop-3.3.5/etc/hadoop/yarn-env.sh\nand add JAVA_HOME below to all files.\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n\n\n\nIn the this configuration, you need to have the IP address of a VM. You can add the second network as host-only adapter in Virtual Box of the machine to obtain the IP address.\nvi ~/hadoop-3.3.5/etc/hadoop/core-site.xml\nLet the machine with a IP address 192.168.1.127 be the namenode. Paste the following code in &lt;configuration&gt;...&lt;/configuration&gt;\n&lt;property&gt;\n  &lt;name&gt;fs.defaultFS&lt;/name&gt;\n  &lt;value&gt;hdfs://192.168.1.127:9000&lt;/value&gt;\n&lt;/property&gt;\n\n\n\nvi ~/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\nBy default, data replication is 3, thus add the property dfs.replication to define more or less. Then, define the location to store hdfs files of namenode and datanode\n&lt;property&gt;\n  &lt;name&gt;dfs.replication&lt;/name&gt;\n  &lt;value&gt;2&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n  &lt;value&gt;/home/ubuntu/hadoop-data/namenode&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n  &lt;value&gt;/home/ubuntu/hadoop-data/datanode&lt;/value&gt;\n&lt;/property&gt;\n\n\n\nvi ~/hadoop-3.3.5/etc/hadoop/mapred-site.xml\nTell Hadoop to use yarn to manage mapreduce.\n&lt;property&gt;\n  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n  &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;\n\n\n\nvi ~/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; \n  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n  &lt;value&gt;192.168.1.127&lt;/value&gt;\n&lt;/property&gt;"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#create-virtual-machine-with-yaml",
    "href": "posts/setup-hadoop-cluster/index.html#create-virtual-machine-with-yaml",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Create Ubuntu Server image with yaml file. The word focal refers to Ubuntu 20.04 LTS.\n\n\n\nhostname\nduty\n\n\n\n\nhadoop0\nnamenode\n\n\nhadoop1\ndatanode\n\n\nhadoop2\ndatanode\n\n\n\npackage_update: true\npackage_upgrade: true\npackages:\n  - openjdk-8-jdk\n  - ssh\n\n\nWindow Terminal\n\nmultipass launch --name hadoop0 --network Wi-Fi --cloud-init myImage.yaml focal"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#install-packages",
    "href": "posts/setup-hadoop-cluster/index.html#install-packages",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "sudo apt-get install ssh\n\n# Only for master node\nsudo apt-get install pdsh"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#create-virtual-machine",
    "href": "posts/setup-hadoop-cluster/index.html#create-virtual-machine",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "Create Ubuntu Server image with yaml file.\n\n\n\nhostname\nrole\n\n\n\n\nnode0\nnamenode\n\n\nnode1\ndatanode\n\n\nnode2\ndatanode\n\n\n\n\n\nsetup-ubuntu.yaml\n\npackage_update: true\npackage_upgrade: true\npackages:\n  - openjdk-8-jdk\n  - ssh\n\n\n\nWindow Terminal\n\n# the default resources are 1 CPU cores, ~1 GB memory and ~5 GB disk space.\nmultipass launch --name node0 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml focal\n# or add more resources with the command\nmultipass launch -n hn1 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml -c 2 -d 20G -m 4G focal\nmultipass launch -n hn2 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml -c 2 -d 20G -m 4G focal\nmultipass launch -n hn3 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml -c 2 -d 20G -m 4G focal\n\nThe word focal refers to Ubuntu 20.04 LTS. The IP addresses are obtain as\n\n\n\nhostname\nrole\nIP address\n\n\n\n\nnode0\nnamenode\n192.168.1.127\n\n\nnode1\ndatanode\n192.168.1.128\n\n\nnode2\ndatanode\n192.168.1.129"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#install-pdsh-parallel-distributed-shell-in-hadoop0",
    "href": "posts/setup-hadoop-cluster/index.html#install-pdsh-parallel-distributed-shell-in-hadoop0",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "For executing shell commands in parallel across multiple remote machines, let’s install pdsh and it is required for Hadoop setup\nsudo apt-get install pdsh\n\n# Display output version information with\npdsh -V\nPlease change the type of shell protocol to be ssh, otherwise it’s cause an error for Hadoop.\nvi ~/.bashrc\n\n\n.bashrc\n\nexport PDSH_RCMD_TYPE=ssh\n\nBy testing with the following command, the hostname must be show up.\npdsh -w node[0-2] hostname\nor testing with the text file.\nvi hosts.txt\nSet all IP address of machine nodes\n\n\nhosts.txt\n\n192.168.1.120\n192.168.1.121\n192.168.1.123"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#prepare-the-file",
    "href": "posts/setup-hadoop-cluster/index.html#prepare-the-file",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "Window Terminal\n\nmultipass transfer C:\\Users\\Jorgnur\\Downloads\\hadoop-3.3.5.tar.gz node0:/home/ubuntu/\n\nIn Ubuntu, unpacking the software on all the machines in the cluster\ntar -zxvf ~/Downloads/hadoop-3.3.5.tar.gz"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#generate-rsa-key-pairs",
    "href": "posts/setup-hadoop-cluster/index.html#generate-rsa-key-pairs",
    "title": "Setup Hadoop Cluster",
    "section": "2.2 Generate rsa key pairs",
    "text": "2.2 Generate rsa key pairs\nGenerate ssh key pairs for each machine.\nssh-keygen\n# or\nssh-keygen -t rsa\ncd .ssh\nvi authorized_keys\nThis public key is of local host for connecting to each VM. It should also be added.\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHf4pMDm06FEWOXMmveyNTJw75+iVM30sz6iYQHLmD8DSGrrkfiKHsXxVGRJxB2A+V72AaoJFJ6CwSi4TVManNLHt8v1ropos45ST0JTTwrjlK6hc4Z68P5l0M4C6sZIEIYECWoA1VTuUzmH4d0szyNDCM+2qkwnWzYnfxbiESkExqgyNnfXw+YlvvPEioyNL8mxq4iPJCJ8HnPKVaAgQFwqJ4c4Zu6P4FZlFsuJHHkdcDdoDRSorqFr7LK7Ye0dfSrqcF39GQBvIZkcLhg1IUxLcPaaDvNXuU0r2N7j63/elVXpmYGolDgyz7HS0Ymr1fdr+bD3lO0fj+RMx9ANFn ubuntu@localhost\nThus, we need to add these public keys for each machine.\nssh-rsa AAAAB...NFn ubuntu@localhost\nssh-rsa AAAAB...Hns= ubuntu@node0\nssh-rsa AAAAB...1tE= ubuntu@node1\nssh-rsa AAAAB...AS0= ubuntu@node2"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#replace-ip-address-with-its-names",
    "href": "posts/setup-hadoop-cluster/index.html#replace-ip-address-with-its-names",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "In namenode, it might need declare hostname of workers (datanode).\nvi /etc/hosts\n192.168.1.128 node1\n192.168.1.129 node2"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#setup-.bashrc",
    "href": "posts/setup-hadoop-cluster/index.html#setup-.bashrc",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "Edit .bashrc with\nvi ~/.bashrc\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\nexport PATH=$PATH:$JAVA_HOME/bin\nexport HADOOP_HOME=~/hadoop-3.3.5\nexport PATH=$PATH:$HADOOP_HOME/bin\nexport PATH=$PATH:$HADOOP_HOME/sbin\nreload bash file\nsource ~/.bashrc\n# or\n. ~/.bashrc\n\n# check the result with\necho $PATH"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#hadoop-configuration",
    "href": "posts/setup-hadoop-cluster/index.html#hadoop-configuration",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "Let’s add JAVA_HOME to flies located in ~/hadoop-3.3.5/etc/hadoop which are\n\n\n\nfilename\n\n\n\n\nmapred-env.sh\n\n\nhadoop-env.sh\n\n\nyarn-env.sh\n\n\n\ncd ~/hadoop-3.3.5/etc/hadoop\n\n# see file list in details\nls -lstr\nls -lstr *env*.sh\nOpen three files with the following commands\nvi ~/hadoop-3.3.5/etc/hadoop/mapred-env.sh\nvi ~/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\nvi ~/hadoop-3.3.5/etc/hadoop/yarn-env.sh\nand add JAVA_HOME below to all files.\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configure-core-site.xml",
    "href": "posts/setup-hadoop-cluster/index.html#configure-core-site.xml",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "In the this configuration, you need to have the IP address of a VM. You can add the second network as host-only adapter in Virtual Box of the machine to obtain the IP address.\nvi ~/hadoop-3.3.5/etc/hadoop/core-site.xml\nLet the machine with a IP address 192.168.1.127 be the namenode. Paste the following code in &lt;configuration&gt;...&lt;/configuration&gt;\n&lt;property&gt;\n  &lt;name&gt;fs.defaultFS&lt;/name&gt;\n  &lt;value&gt;hdfs://192.168.1.127:9000&lt;/value&gt;\n&lt;/property&gt;"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configure-yarn-site.xml",
    "href": "posts/setup-hadoop-cluster/index.html#configure-yarn-site.xml",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "vi ~/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; \n  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n  &lt;value&gt;192.168.1.127&lt;/value&gt;\n&lt;/property&gt;"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configure-hdfs-site.xml",
    "href": "posts/setup-hadoop-cluster/index.html#configure-hdfs-site.xml",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "vi ~/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\nBy default, data replication is 3, thus add the property dfs.replication to define more or less. Then, define the location to store hdfs files of namenode and datanode\n&lt;property&gt;\n  &lt;name&gt;dfs.replication&lt;/name&gt;\n  &lt;value&gt;2&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n  &lt;value&gt;/home/ubuntu/hadoop-data/namenode&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n  &lt;value&gt;/home/ubuntu/hadoop-data/datanode&lt;/value&gt;\n&lt;/property&gt;"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configure-mapred-site.xml",
    "href": "posts/setup-hadoop-cluster/index.html#configure-mapred-site.xml",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "vi ~/hadoop-3.3.5/etc/hadoop/mapred-site.xml\nTell Hadoop to use yarn to manage mapreduce.\n&lt;property&gt;\n  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n  &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#set-workers-ip-address-only-in-master-node",
    "href": "posts/setup-hadoop-cluster/index.html#set-workers-ip-address-only-in-master-node",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "vi ~/hadoop-3.3.5/etc/hadoop/workers\n192.168.1.127\n192.168.1.128\n192.168.1.129"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#format",
    "href": "posts/setup-hadoop-cluster/index.html#format",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Let’s check the error here.\nbin/hadoop namenode -format"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#general-usage-command",
    "href": "posts/setup-hadoop-cluster/index.html#general-usage-command",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "cd ~/hadoop-3.3.5\nsbin/start-all.sh\n\n\n\ncd ~/hadoop-3.3.5\nsbin/stop-all.sh\n\n\n\njps"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#hadoop-ui",
    "href": "posts/setup-hadoop-cluster/index.html#hadoop-ui",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "Let’s go the browser http://localhost:9870/ of the Namenode, thus access with http://192.168.1.127:9870/ from outside a VM."
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#format-hdfs-namenode",
    "href": "posts/setup-hadoop-cluster/index.html#format-hdfs-namenode",
    "title": "Setup Hadoop Cluster",
    "section": "2.4 Format HDFS NameNode",
    "text": "2.4 Format HDFS NameNode\nIn namenode, let’ s initializes the directory structure and file system metadata required by the NameNode to start a fresh HDFS instance.\ncd ~/hadoop-3.3.5\nbin/hdfs namenode -format"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#create-a-demo-file",
    "href": "posts/setup-hadoop-cluster/index.html#create-a-demo-file",
    "title": "Setup Hadoop Cluster",
    "section": "2.7 Create a demo file",
    "text": "2.7 Create a demo file\nvi ~/demo\n\n\ndemo\n\nHello!\n\nhdfs dfs -put /home/ubuntu/demo /demo123"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#troubleshoots",
    "href": "posts/setup-hadoop-cluster/index.html#troubleshoots",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "When running the command\nsbin/start-all.sh \n, I got the message\nStopping namenodes on [master]\npdsh@hadoop0: master: rcmd: socket: Permission denied\nStopping datanodes\npdsh@hadoop0: 192.168.1.22: rcmd: socket: Permission denied\npdsh@hadoop0: 192.168.1.21: rcmd: socket: Permission denied\npdsh@hadoop0: 192.168.1.20: rcmd: socket: Permission denied\nyum -y install pdsh\nyum -y install epel-release\n\npdsh -v\npdsh-2.31 (+debug)\nrcmd modules: ssh,rsh,exec (default: rsh)\nmisc modules: genders\nexport PDSH_RCMD_TYPE=ssh\n{.bash {filename=.bashrc}} export PDSH_RCMD_TYPE=ssh\npdsh -w slave-[1-2] hostname\npdsh -w ^hosts.txt ls"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#troubleshooting",
    "href": "posts/setup-hadoop-cluster/index.html#troubleshooting",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "When running the command\nsbin/start-all.sh \n```,\nI got the message\n\n```bash\nStopping namenodes on [master]\npdsh@hadoop0: master: rcmd: socket: Permission denied\nStopping datanodes\npdsh@hadoop0: 192.168.1.23: rcmd: socket: Permission denied\npdsh@hadoop0: 192.168.1.21: rcmd: socket: Permission denied\npdsh@hadoop0: 192.168.1.20: rcmd: socket: Permission denied"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#namenode-web-interface",
    "href": "posts/setup-hadoop-cluster/index.html#namenode-web-interface",
    "title": "Setup Hadoop Cluster",
    "section": "3.3 NameNode Web Interface",
    "text": "3.3 NameNode Web Interface\nThe web interface for the NameNode is available at http://192.168.1.127:9870/ by default. Normally, the ufw is inactive after install Ubuntu image. you do not have to do here. If the ufw is enable for some cases, you should allow add ufw rule on port 9870\nsudo ufw allow 9870\nsudo ufw reload"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#permission-denied",
    "href": "posts/setup-hadoop-cluster/index.html#permission-denied",
    "title": "Setup Hadoop Cluster",
    "section": "3.2 Permission denied",
    "text": "3.2 Permission denied\nI got the message as follows:\nStopping namenodes on [master]\npdsh@node0: master: rcmd: socket: Permission denied\nStopping datanodes\npdsh@node0: 192.168.1.23: rcmd: socket: Permission denied\npdsh@node0: 192.168.1.21: rcmd: socket: Permission denied\npdsh@node0: 192.168.1.20: rcmd: socket: Permission denied\nPlease check in workers. The IP address was not set correctly.\nvi ~/hadoop-3.3.5/etc/hadoop/workers"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#navigate-ifle",
    "href": "posts/setup-hadoop-cluster/index.html#navigate-ifle",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "2.3 Navigate ifle",
    "text": "2.3 Navigate ifle\nI am currently working on Window system. Extraction with Winrar causeme some errors, thus I use Cygwin Linux Software to extract the file.\ncd /cygdrive/c/users/Jorgnur/Downloads/hadoop-3.3.5.tar.gz\n\ntar -zxvf hadoop-3.3.5.tar.gz"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#install-pdsh-parallel-distributed-shell-in-node0",
    "href": "posts/setup-hadoop-cluster/index.html#install-pdsh-parallel-distributed-shell-in-node0",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "For executing shell commands in parallel across multiple remote machines, let’s install pdsh and it is required for Hadoop setup\nsudo apt-get install pdsh\n\n# Display output version information with\npdsh -V\nPlease change the type of shell protocol to be ssh, otherwise it’s cause an error for Hadoop.\nvi ~/.bashrc\n\n\n.bashrc\n\nexport PDSH_RCMD_TYPE=ssh\n\nBy testing with the following command, the hostname must be show up.\npdsh -w node[0-2] hostname\nor testing with the text file.\nvi hosts.txt\nSet all IP address of machine nodes\n\n\nhosts.txt\n\n192.168.1.127\n192.168.1.128\n192.168.1.129\n\npdsh -w ^hosts.txt hostname"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#navigate-hadoop-files",
    "href": "posts/setup-hadoop-cluster/index.html#navigate-hadoop-files",
    "title": "Setup Hadoop Cluster",
    "section": "2.3 Navigate Hadoop Files",
    "text": "2.3 Navigate Hadoop Files\nI am currently working on Window system. Extraction with Winrar causes me some errors, thus I use Cygwin Linux Software to extract the file.\ncd /cygdrive/c/users/Jorgnur/Desktop/hadoop/ver/hadoop-3.3.5.tar.gz\ntar -zxvf hadoop-3.3.5.tar.gz\n# cygwin =&gt; node0\nrsync -avzphi /cygdrive/c/users/Jorgnur/Desktop/hadoop/ver/hadoop-3.3.5.tar.gz ubuntu@192.168.1.109:~/\nssh ubuntu@192.168.1.109\ntar -zxvf hadoop-3.3.5.tar.gz\nrm hadoop-3.3.5.tar.gz\nlogout\nrsync -avzphi /cygdrive/c/users/Jorgnur/Desktop/hadoop/ver/hadoop-3.3.5/etc/hadoop ubuntu@192.168.1.109:~/hadoop-3.3.5/etc/\n\nssh ubuntu@node0\n# node0 =&gt; node1\nrsync -avzphi ~/hadoop-3.3.5.tar.gz ubuntu@192.168.1.110:~/\nssh ubuntu@192.168.1.110\ntar -zxvf hadoop-3.3.5.tar.gz\nrm hadoop-3.3.5.tar.gz\nlogout\nrsync -avzphi ~/hadoop-3.3.5/etc/hadoop ubuntu@192.168.1.110:~/hadoop-3.3.5/etc/\n\n# node0 =&gt; node2\nrsync -avzphi ~/hadoop-3.3.5.tar.gz ubuntu@192.168.1.107:~/\nssh ubuntu@192.168.1.107\ntar -zxvf hadoop-3.3.5.tar.gz\nrm hadoop-3.3.5.tar.gz\nlogout\nrsync -avzphi ~/hadoop-3.3.5/etc/hadoop ubuntu@192.168.1.107:~/hadoop-3.3.5/etc/"
  },
  {
    "objectID": "posts/cypher-query/index.html#display-database-schema",
    "href": "posts/cypher-query/index.html#display-database-schema",
    "title": "Cypher Query",
    "section": "",
    "text": "CALL db.schema.visualization()"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-triple-nodes",
    "href": "posts/cypher-query/index.html#match-triple-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n)-[:HAS_INFO]-&gt;(m)-[:CITE]-&gt;(p) RETURN n, m, p"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-nodes-with-multiple-labels",
    "href": "posts/cypher-query/index.html#match-nodes-with-multiple-labels",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n:Herb:Drug) RETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-node-with-some-text",
    "href": "posts/cypher-query/index.html#match-node-with-some-text",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n:`Country`)-[r]-(m)\nWHERE n.text CONTAINS 'Thai'\nRETURN n, r, m"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-a-label-that-contains-space",
    "href": "posts/cypher-query/index.html#match-a-label-that-contains-space",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n:`Geographic Distribution`) RETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#set-a-new-property-to-all-nodes",
    "href": "posts/cypher-query/index.html#set-a-new-property-to-all-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n:Person)\nSET n.dummy = 0"
  },
  {
    "objectID": "posts/cypher-query/index.html#create-one-node",
    "href": "posts/cypher-query/index.html#create-one-node",
    "title": "Cypher Query",
    "section": "",
    "text": "CREATE (:User {name: \"Charlie\", age: 35, gender: \"male\"});\nCREATE (:User {name: \"Dave\", age: 28, gender: \"male\"});"
  },
  {
    "objectID": "posts/cypher-query/index.html#create-relationship-from-nodes",
    "href": "posts/cypher-query/index.html#create-relationship-from-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (charlie:User {name: \"Charlie\"}), (dave:User {name: \"Dave\"})\nCREATE (charlie)&lt;-[:FRIENDS_WITH {date: \"Jan 19\"}]-(dave);"
  },
  {
    "objectID": "posts/cypher-query/index.html#create-a-relationship-from-nodes",
    "href": "posts/cypher-query/index.html#create-a-relationship-from-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (charlie:User {name: \"Charlie\"}), (dave:User {name: \"Dave\"})\nCREATE (charlie)&lt;-[:FRIENDS_WITH {date: \"Jan 19\"}]-(dave);"
  },
  {
    "objectID": "posts/cypher-python/index.html",
    "href": "posts/cypher-python/index.html",
    "title": "Using Cypher in Python",
    "section": "",
    "text": "pip install py2neo"
  },
  {
    "objectID": "posts/cypher-python/index.html#firstly-create-simple-api-in-node.js",
    "href": "posts/cypher-python/index.html#firstly-create-simple-api-in-node.js",
    "title": "Using Cypher in Python",
    "section": "",
    "text": "match (n) return n\nSELECT * from mytable where id = 1\nSELECT * from mytable where id = 2\n\n\nterminal\n\nsudo ufw status\nsudo ufw reload\n\n\n\nmatplotlib.py\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n`{.python code-line-numbers=\"true\"} # highlight-style: arrow import matplotlib.pyplot as plt print(\"Hello\")"
  },
  {
    "objectID": "posts/cypher-python/index.html#install-a-py2neo-package",
    "href": "posts/cypher-python/index.html#install-a-py2neo-package",
    "title": "Using Cypher in Python",
    "section": "",
    "text": "pip install py2neo"
  },
  {
    "objectID": "posts/cypher-python/index.html#import-python-packages",
    "href": "posts/cypher-python/index.html#import-python-packages",
    "title": "Using Cypher in Python",
    "section": "2 Import Python packages",
    "text": "2 Import Python packages\nimport re #regular expression\nimport numpy as np\nimport pandas as pd\n\nfrom py2neo.bulk import merge_nodes\nfrom py2neo import Node, Graph, Relationship, NodeMatcher"
  },
  {
    "objectID": "posts/cypher-python/index.html#import-python-packages-1",
    "href": "posts/cypher-python/index.html#import-python-packages-1",
    "title": "Using Cypher in Python",
    "section": "3 Import Python packages",
    "text": "3 Import Python packages\nimport re #regular expression\nimport numpy as np\nimport pandas as pd\n\nfrom py2neo.bulk import merge_nodes\nfrom py2neo import Node, Graph, Relationship, NodeMatcher"
  },
  {
    "objectID": "posts/cypher-python/index.html#define-remove-special-characters",
    "href": "posts/cypher-python/index.html#define-remove-special-characters",
    "title": "Using Cypher in Python",
    "section": "3 Define remove special characters",
    "text": "3 Define remove special characters\ndef remove_special_characters(text):\n  regex = re.compile(r'[\\n\\r\\t]')\n  clean_text = regex.sub(\" \", text)  \n  return clean_text.strip()"
  },
  {
    "objectID": "posts/cypher-python/index.html#define-funtion-for-creating-tuple-list-of-data",
    "href": "posts/cypher-python/index.html#define-funtion-for-creating-tuple-list-of-data",
    "title": "Using Cypher in Python",
    "section": "4 Define funtion for creating tuple list of data",
    "text": "4 Define funtion for creating tuple list of data\ndef create_tuple_list(df):\n  # prepare tuple list\n  keys = df.columns.tolist()\n\n  df_selected_columns = pd.DataFrame(df, columns=keys)\n\n  records = df_selected_columns.to_records(index=False)\n  tuples = list(records)\n\n  return tuples"
  },
  {
    "objectID": "posts/cypher-python/index.html#create-connection-to-graph-database",
    "href": "posts/cypher-python/index.html#create-connection-to-graph-database",
    "title": "Using Cypher in Python",
    "section": "5 Create connection to graph database",
    "text": "5 Create connection to graph database\n# Connect database\nuri = 'bolt://localhost:7687'\nuser = 'neo4j'\npwd = 'my-password'\ngraph = Graph(uri, auth=(user, pwd))\nnodes_matcher = NodeMatcher(graph) # use in add_edges()"
  },
  {
    "objectID": "posts/cypher-python/index.html#find-node-numbers",
    "href": "posts/cypher-python/index.html#find-node-numbers",
    "title": "Using Cypher in Python",
    "section": "6 Find node numbers",
    "text": "6 Find node numbers\nprint('Number of nodes in graph: ', graph.nodes.match('Node').count())"
  },
  {
    "objectID": "posts/cypher-python/index.html#read-an-excel-file-using-pandas",
    "href": "posts/cypher-python/index.html#read-an-excel-file-using-pandas",
    "title": "Using Cypher in Python",
    "section": "7 Read an excel file using pandas",
    "text": "7 Read an excel file using pandas\nexcel_file = 'data.xlsx'\ndf0 = pd.read_excel(excel_file, sheet_name=0, skiprows=0, header=0, nrows=None)\ndf0 = df0.applymap(lambda x:remove_special_characters(x) if type(x) == str else x).fillna('')\nprint(len(df0))"
  },
  {
    "objectID": "posts/cypher-python/index.html#read-an-excel-file-using-pandas-1",
    "href": "posts/cypher-python/index.html#read-an-excel-file-using-pandas-1",
    "title": "Using Cypher in Python",
    "section": "8 Read an excel file using pandas",
    "text": "8 Read an excel file using pandas\nherb_name = \"Ginger\"\n[df_name, df_information] = df0\ndf_name.shape, df_information.shape"
  },
  {
    "objectID": "posts/cypher-python/index.html#read-an-excel-file-using-pandas-2",
    "href": "posts/cypher-python/index.html#read-an-excel-file-using-pandas-2",
    "title": "Using Cypher in Python",
    "section": "9 Read an excel file using pandas",
    "text": "9 Read an excel file using pandas\nnode_herb = Node(\"Herb\", name=herb_name)\n\ntuples = create_tuple_list(df_name)\nfor tuple in tuples:\n\n  for index, value in enumerate(tuple):\n\n    node_nomen_cat = Node(\"Nomenclature_Category\", name=tuple[ind_nomen-1])\n    node_ref = Node(\"Citation\", name=tuple[ind_ref], link=tuple[ind_ref+1])\n\n    if (index &gt;= ind_nomen and index &lt; ind_ref and value != ''):\n  \n      graph.merge(node_herb, \"Herb\", \"name\")\n      graph.merge(node_nomen_cat, \"Nomenclature_Category\", \"name\")\n      graph.merge(node_ref, \"Citation\", \"link\")\n\n      node_nomen = Node(\"Nomenclature\", name=tuple[index])\n\n      rel1 = Relationship(node_herb, \"IS_CALLED\", node_nomen)\n      rel2 = Relationship(node_nomen, \"CATEGORIZED\", node_nomen_cat)\n      rel3 = Relationship(node_nomen, \"CITE\", node_ref)\n\n      graph.create(rel1)\n      graph.create(rel2)\n      graph.create(rel3)"
  },
  {
    "objectID": "posts/cypher-python/index.html#check-data-in-the-database",
    "href": "posts/cypher-python/index.html#check-data-in-the-database",
    "title": "Using Cypher in Python",
    "section": "10 Check data in the database",
    "text": "10 Check data in the database\nquery = '''\nmatch (n:Herb {name: %s})-[:IS_CALLED]-(m)\nreturn m\n''' % herb_name\n\nresult = graph.run(query)\n\nlist(result)"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html#create-a-database-connection-pagesgetdata.ts",
    "href": "posts/neo4j-nextjs/index.html#create-a-database-connection-pagesgetdata.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "3 Create a database connection pages/getdata.ts",
    "text": "3 Create a database connection pages/getdata.ts\nimport { NextApiRequest, NextApiResponse } from \"next\";\nimport neo4j, { Driver, Session, Result, Record } from \"neo4j-driver\";\nimport { log } from \"console\";\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\n// Define the type for properties\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\n// API route handler\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  const session: Session = driver.session();\n\n  try {\n    // Perform your Neo4j queries and logic here\n    const result = await session.run(\"MATCH (n) RETURN n LIMIT 5\");\n    \n    // const nodes: Record&lt;string, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n    const nodes: Dict&lt;PropertyKey, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n\n    console.log(nodes);\n\n    const jsonData = JSON.stringify(nodes);\n\n    res.status(200).json({ data: jsonData });\n\n  } catch (error) {\n    console.error(\"Error executing Neo4j query:\", error);\n    res.status(500).json({ error: \"Internal Server Error\" });\n  } finally {\n    session.close();\n  }\n}"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html#create-a-database-connection-pagesgraph-data.ts",
    "href": "posts/neo4j-nextjs/index.html#create-a-database-connection-pagesgraph-data.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "3 Create a database connection pages/graph-data.ts",
    "text": "3 Create a database connection pages/graph-data.ts\nimport { NextApiRequest, NextApiResponse } from \"next\";\nimport neo4j, { Driver, Session, Result, Record } from \"neo4j-driver\";\nimport { log } from \"console\";\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\n// Define the type for properties\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\n// API route handler\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  const session: Session = driver.session();\n\n  try {\n    // Perform your Neo4j queries and logic here\n    const result = await session.run(\"MATCH (n) RETURN n LIMIT 5\");\n    \n    // const nodes: Record&lt;string, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n    const nodes: Dict&lt;PropertyKey, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n\n    console.log(nodes);\n\n    const jsonData = JSON.stringify(nodes);\n\n    res.status(200).json({ data: jsonData });\n\n  } catch (error) {\n    console.error(\"Error executing Neo4j query:\", error);\n    res.status(500).json({ error: \"Internal Server Error\" });\n  } finally {\n    session.close();\n  }\n}"
  },
  {
    "objectID": "posts/nextjs-redux/index.html",
    "href": "posts/nextjs-redux/index.html",
    "title": "Using Redux in Next.js",
    "section": "",
    "text": "Create the slices of states and hooks.\nimport { createSlice } from \"@reduxjs/toolkit\";\nimport type { PayloadAction } from \"@reduxjs/toolkit\";\n\nexport interface SearchState {\n  term: string;\n}\nconst initialState: SearchState = {\n  term: \"\",\n};\nconst searchSlice = createSlice({\n  name: \"search\",\n  initialState,\n  reducers: {\n    setTerm(state, action: PayloadAction&lt;string&gt;) {\n      state.term = action.payload.trim();\n    },\n  },\n});\n\nexport const {\n  setTerm,\n} = searchSlice.actions;\nexport default searchSlice.reducer;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-providers-for-a-redux-store",
    "href": "posts/nextjs-redux/index.html#create-providers-for-a-redux-store",
    "title": "Using Redux in Next.js",
    "section": "4 Create Providers for a Redux store",
    "text": "4 Create Providers for a Redux store\nThe Providers component used for wrapping components as a parent component that child components can import states and hooks just by importing (not as a component props).\n\"use client\";\nimport { Provider } from \"react-redux\";\nimport { store } from \"./store\";\n\nexport default function Providers({ children }: { children: React.ReactNode }) {\n  return &lt;Provider store={store}&gt;{children}&lt;/Provider&gt;;\n}"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-a-redux-store",
    "href": "posts/nextjs-redux/index.html#create-a-redux-store",
    "title": "Using Redux in Next.js",
    "section": "3 Create a Redux store",
    "text": "3 Create a Redux store\nCreate a Redux store for state and api slices.\nimport { configureStore } from \"@reduxjs/toolkit\"\nimport { useDispatch, useSelector } from \"react-redux\";\nimport type { TypedUseSelectorHook } from \"react-redux\";\n\nimport searchReducer from \"./sliceStateSearch\"\nimport { herbApi } from \"./sliceApiHerb\";\n\nexport const store = configureStore({\n  reducer: {\n    search: searchReducer,\n    [herbApi.reducerPath]: herbApi.reducer,\n  },\n  middleware(getDefaultMiddleware) {\n    return getDefaultMiddleware().concat(\n      herbApi.middleware\n    );\n  }\n  \n});\n\ntype AppDispatch = typeof store.dispatch;\ntype RootState = ReturnType&lt;typeof store.getState&gt;;\nexport const useAppDispatch = () =&gt; useDispatch&lt;AppDispatch&gt;();\nexport const useAppSelector: TypedUseSelectorHook&lt;RootState&gt; = useSelector;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-a-redux-store-1",
    "href": "posts/nextjs-redux/index.html#create-a-redux-store-1",
    "title": "Using Redux in Next.js",
    "section": "4 Create a Redux store",
    "text": "4 Create a Redux store\nCreate a Redux store for state and api slices.\nimport { configureStore } from \"@reduxjs/toolkit\"\nimport { useDispatch, useSelector } from \"react-redux\";\nimport type { TypedUseSelectorHook } from \"react-redux\";\n\nimport searchReducer from \"./sliceStateSearch\"\nimport { herbApi } from \"./sliceApiHerb\";\n\nexport const store = configureStore({\n  reducer: {\n    search: searchReducer,\n    [herbApi.reducerPath]: herbApi.reducer,\n  },\n  middleware(getDefaultMiddleware) {\n    return getDefaultMiddleware().concat(\n      herbApi.middleware\n    );\n  }\n  \n});\n\ntype AppDispatch = typeof store.dispatch;\ntype RootState = ReturnType&lt;typeof store.getState&gt;;\nexport const useAppDispatch = () =&gt; useDispatch&lt;AppDispatch&gt;();\nexport const useAppSelector: TypedUseSelectorHook&lt;RootState&gt; = useSelector;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-a-redux-store-2",
    "href": "posts/nextjs-redux/index.html#create-a-redux-store-2",
    "title": "Using Redux in Next.js",
    "section": "4 Create a Redux store",
    "text": "4 Create a Redux store\nCreate a Redux store for state and api slices.\nimport { configureStore } from \"@reduxjs/toolkit\"\nimport { useDispatch, useSelector } from \"react-redux\";\nimport type { TypedUseSelectorHook } from \"react-redux\";\n\nimport searchReducer from \"./sliceStateSearch\"\nimport { herbApi } from \"./sliceApiHerb\";\n\nexport const store = configureStore({\n  reducer: {\n    search: searchReducer,\n    [herbApi.reducerPath]: herbApi.reducer,\n  },\n  middleware(getDefaultMiddleware) {\n    return getDefaultMiddleware().concat(\n      herbApi.middleware\n    );\n  }\n  \n});\n\ntype AppDispatch = typeof store.dispatch;\ntype RootState = ReturnType&lt;typeof store.getState&gt;;\nexport const useAppDispatch = () =&gt; useDispatch&lt;AppDispatch&gt;();\nexport const useAppSelector: TypedUseSelectorHook&lt;RootState&gt; = useSelector;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-rtk-queries",
    "href": "posts/nextjs-redux/index.html#create-rtk-queries",
    "title": "Using Redux in Next.js",
    "section": "2 Create RTK queries",
    "text": "2 Create RTK queries\nThe api slices with endpoints for various use cases are managed by Redux for caching, validating, mutating, etc. .\nimport { DataProps } from \"@/interfaces/graph\";\nimport { createApi, fetchBaseQuery } from \"@reduxjs/toolkit/query/react\";\nimport { SearchNodesApiProps } from \"@/interfaces\";\n\nexport const herbApi = createApi({\n  reducerPath: \"herbApiSlice\",\n  // baseQuery: fetchBaseQuery({ baseUrl: \"/api\"}),\n  baseQuery: fetchBaseQuery({ baseUrl: \"/graph-db/api\"}), //check next.config \n  tagTypes: [\"herbs\"],\n  endpoints: (builder) =&gt; ({\n\n    herbs: builder.query&lt;DataProps, void&gt;({\n      query: () =&gt; \"\",\n      providesTags: (result, error, search) =&gt; [{ type: \"herbs\", search}]\n    }),\n\n    searchNode: builder.mutation&lt;DataProps, SearchNodesApiProps&gt;({\n      query: (body) =&gt; ({\n        url: \"/post-search-node\",\n        method: \"POST\",\n        body,\n      }),\n    }),\n\n  })\n})\n\nexport const { useHerbsQuery, useSearchNodeMutation } = herbApi;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-states-and-hooks",
    "href": "posts/nextjs-redux/index.html#create-states-and-hooks",
    "title": "Using Redux in Next.js",
    "section": "",
    "text": "Create the slices of states and hooks.\nimport { createSlice } from \"@reduxjs/toolkit\";\nimport type { PayloadAction } from \"@reduxjs/toolkit\";\n\nexport interface SearchState {\n  term: string;\n}\nconst initialState: SearchState = {\n  term: \"\",\n};\nconst searchSlice = createSlice({\n  name: \"search\",\n  initialState,\n  reducers: {\n    setTerm(state, action: PayloadAction&lt;string&gt;) {\n      state.term = action.payload.trim();\n    },\n  },\n});\n\nexport const {\n  setTerm,\n} = searchSlice.actions;\nexport default searchSlice.reducer;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#wrap-providers-in-the-app",
    "href": "posts/nextjs-redux/index.html#wrap-providers-in-the-app",
    "title": "Using Redux in Next.js",
    "section": "5 Wrap Providers in the app",
    "text": "5 Wrap Providers in the app\nimport Providers from \"@/context/Providers\";\n\nexport default function Home() {\n\n  return (\n    &lt;main&gt;\n      &lt;Providers&gt;\n        ...\n      &lt;/Providers&gt;\n    &lt;/main&gt;\n  );\n}"
  },
  {
    "objectID": "posts/using-rsync/index.html",
    "href": "posts/using-rsync/index.html",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync [options] source destination\n\n\n\nwhich rsync\n\n# If rsync is installed =&gt;\n# /usr/bin/rsync\n\n\n\nrsync -av ~/aaa ubuntu@slave-1:~/\n\n# Copy file `aaa` to remote machine\nrsync -avzphi ~/aaa ubuntu@slave-1:~/\n\n# Copy file from remote machine\nrsync -avzphi ubuntu@slave-1:~/aaa ~/\n\n\n\nman rsync\n\n\n\nparam\nduty\n\n\n\n\na\narchive\n\n\nv\nverbose\n\n\nz\ncompress\n\n\np\npermissions\n\n\nh\nhuman-readable\n\n\ni\nitemize-changes\n\n\n\n\n\n\n\n\nrsync -avzphi /cygdrive/c/users/Jorgnur/Desktop/xxx ubuntu@192.168.1.120:~/\n\n\n\nrsync -avzphi ubuntu@192.168.1.120:~/xxx /cygdrive/c/users/Jorgnur/Desktop/\n\n\n\npdsh -w ^hosts.txt rsync -avzphi ~/aaa ubuntu@%:~/\npdsh -H \"$(cat hosts.txt | tr '\\n' ',')\" \"rsync -avzphi ~/aaa ubuntu@%:~/\"\n# delete the files that contain `my-dir`\nfind . -name \"*my-dir*\" -exec rm {} \\;\nfind . -type f -name \"*my-dir*\" -delete"
  },
  {
    "objectID": "posts/using-rsync/index.html#display-database-schema",
    "href": "posts/using-rsync/index.html#display-database-schema",
    "title": "Cypher Query",
    "section": "",
    "text": "CALL db.schema.visualization()"
  },
  {
    "objectID": "posts/using-rsync/index.html#syntax",
    "href": "posts/using-rsync/index.html#syntax",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync [options] source destination"
  },
  {
    "objectID": "posts/using-rsync/index.html#check-if-rsync-is-installed-or-not",
    "href": "posts/using-rsync/index.html#check-if-rsync-is-installed-or-not",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "which rsync\n\n# If rsync is installed =&gt;\n# /usr/bin/rsync"
  },
  {
    "objectID": "posts/using-rsync/index.html#example",
    "href": "posts/using-rsync/index.html#example",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync -av ~/aaa ubuntu@slave-1:~/\n\n# Copy file `aaa` to remote machine\nrsync -avzphi ~/aaa ubuntu@slave-1:~/\n\n# Copy file from remote machine\nrsync -avzphi ubuntu@slave-1:~/aaa ~/"
  },
  {
    "objectID": "posts/using-rsync/index.html#get-help",
    "href": "posts/using-rsync/index.html#get-help",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "man rsync\n\n\n\nparam\nduty\n\n\n\n\na\narchive\n\n\nv\nverbose\n\n\nz\ncompress\n\n\np\npermissions\n\n\nh\nhuman-readable\n\n\ni\nitemize-changes"
  },
  {
    "objectID": "posts/using-rsync/index.html#example-for-cygwin",
    "href": "posts/using-rsync/index.html#example-for-cygwin",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync -avzphi /cygdrive/c/users/Jorgnur/Desktop/xxx ubuntu@192.168.1.120:~/\n\n\n\nrsync -avzphi ubuntu@192.168.1.120:~/xxx /cygdrive/c/users/Jorgnur/Desktop/"
  },
  {
    "objectID": "posts/using-multipass/index.html",
    "href": "posts/using-multipass/index.html",
    "title": "Using Multipass",
    "section": "",
    "text": "multipass shell &lt;image-name&gt;\n\n\n\nCreate Ubuntu Server image with yaml file. The word focal refers to Ubuntu 20.04 LTS.\nmultipass launch --name hadoop0 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\nmultipass launch --name hadoop1 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\nmultipass launch --name hadoop2 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\n\nmultipass launch --name node0 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml --disk 10G --mem 4G --cpus 2 focal\n\nmultipass launch --name node0 --network Wi-Fi --cloud-init setup-ubuntu.yaml --disk 10G --mem 4G --cpus 2 focal\npackage_update: true\npackage_upgrade: true\npackages:\n  - openjdk-8-jdk\n\n\n\nDelete created images\nmultipass delete &lt;image-name&gt;\n\n\n\nSuppose I has a text file (test.txt) and I want to transfer a file from a Window system into Ubuntu VM\nmultipass transfer test.txt hadoop0:/home/ubuntu/\n# or\nmultipass transfer C:\\Users\\Jorgnur\\Downloads\\hadoop-3.3.5.tar.gz hadoop0:/home/ubuntu/\nSuppose I has a folder (test) and I want to transfer a file from a Window system into Ubuntu VM\nmultipass transfer -r test hadoop0:/home/ubuntu/\n\n\n\nIf you do not set the password at first when creating VM, use the following command to set new password.\nsudo passwd ubuntu\n\n\n\nThe IP address assigned to instances is dynamic by default, to assign the IP address use this command\nmultipass set instance-name --address 192.168.1.22\n#\nmultipass set instance-name --address 192.168.1.22\n\n\n\nStop the instance you want to modify:\nmultipass stop hadoop0\nUpdate new CPUs, disk and memory to hadoop0\n$ multipass stop hadoop0\n$ multipass set local.hadoop0.cpus=4\n$ multipass set local.hadoop0.disk=10G\n$ multipass set local.hadoop0.memory=2G\nIncrease the amount of RAM:"
  },
  {
    "objectID": "posts/using-multipass/index.html#start-shell-image",
    "href": "posts/using-multipass/index.html#start-shell-image",
    "title": "Using Multipass",
    "section": "",
    "text": "multipass shell &lt;image-name&gt;"
  },
  {
    "objectID": "posts/using-multipass/index.html#create-virtual-machine-with-yaml",
    "href": "posts/using-multipass/index.html#create-virtual-machine-with-yaml",
    "title": "Using Multipass",
    "section": "",
    "text": "Create Ubuntu Server image with yaml file. The word focal refers to Ubuntu 20.04 LTS.\nmultipass launch --name hadoop0 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\nmultipass launch --name hadoop1 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\nmultipass launch --name hadoop2 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\n\nmultipass launch --name node0 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml --disk 10G --mem 4G --cpus 2 focal\n\nmultipass launch --name node0 --network Wi-Fi --cloud-init setup-ubuntu.yaml --disk 10G --mem 4G --cpus 2 focal\npackage_update: true\npackage_upgrade: true\npackages:\n  - openjdk-8-jdk"
  },
  {
    "objectID": "posts/using-multipass/index.html#delete-images",
    "href": "posts/using-multipass/index.html#delete-images",
    "title": "Using Multipass",
    "section": "",
    "text": "Delete created images\nmultipass delete &lt;image-name&gt;"
  },
  {
    "objectID": "posts/using-multipass/index.html#transfer-a-file",
    "href": "posts/using-multipass/index.html#transfer-a-file",
    "title": "Using Multipass",
    "section": "",
    "text": "Suppose I has a text file (test.txt) and I want to transfer a file from a Window system into Ubuntu VM\nmultipass transfer test.txt hadoop0:/home/ubuntu/\n# or\nmultipass transfer C:\\Users\\Jorgnur\\Downloads\\hadoop-3.3.5.tar.gz hadoop0:/home/ubuntu/\nSuppose I has a folder (test) and I want to transfer a file from a Window system into Ubuntu VM\nmultipass transfer -r test hadoop0:/home/ubuntu/"
  },
  {
    "objectID": "posts/using-multipass/index.html#set-user-password",
    "href": "posts/using-multipass/index.html#set-user-password",
    "title": "Using Multipass",
    "section": "",
    "text": "If you do not set the password at first when creating VM, use the following command to set new password.\nsudo passwd ubuntu"
  },
  {
    "objectID": "posts/using-multipass/index.html#set-a-fixed-ip-address-for-your-instances",
    "href": "posts/using-multipass/index.html#set-a-fixed-ip-address-for-your-instances",
    "title": "Using Multipass",
    "section": "",
    "text": "The IP address assigned to instances is dynamic by default, to assign the IP address use this command\nmultipass set instance-name --address 192.168.1.22\n#\nmultipass set instance-name --address 192.168.1.22"
  },
  {
    "objectID": "posts/using-multipass/index.html#add-more-resource",
    "href": "posts/using-multipass/index.html#add-more-resource",
    "title": "Using Multipass",
    "section": "",
    "text": "Stop the instance you want to modify:\nmultipass stop hadoop0\nUpdate new CPUs, disk and memory to hadoop0\n$ multipass stop hadoop0\n$ multipass set local.hadoop0.cpus=4\n$ multipass set local.hadoop0.disk=10G\n$ multipass set local.hadoop0.memory=2G\nIncrease the amount of RAM:"
  },
  {
    "objectID": "posts/using-rsync/index.html#examples",
    "href": "posts/using-rsync/index.html#examples",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync -av ~/aaa ubuntu@slave-1:~/\n\n# Copy file `aaa` to remote machine\nrsync -avzphi ~/aaa ubuntu@slave-1:~/\n\n# Copy file from remote machine\nrsync -avzphi ubuntu@slave-1:~/aaa ~/"
  },
  {
    "objectID": "posts/using-rsync/index.html#examples-for-cygwin",
    "href": "posts/using-rsync/index.html#examples-for-cygwin",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync -avzphi /cygdrive/c/users/Jorgnur/Desktop/xxx ubuntu@192.168.1.120:~/\n\n\n\nrsync -avzphi ubuntu@192.168.1.120:~/xxx /cygdrive/c/users/Jorgnur/Desktop/\n\n\n\npdsh -w ^hosts.txt rsync -avzphi ~/aaa ubuntu@%:~/\npdsh -H \"$(cat hosts.txt | tr '\\n' ',')\" \"rsync -avzphi ~/aaa ubuntu@%:~/\"\n# delete the files that contain `my-dir`\nfind . -name \"*my-dir*\" -exec rm {} \\;\nfind . -type f -name \"*my-dir*\" -delete"
  },
  {
    "objectID": "posts/using-hadoop/index.html",
    "href": "posts/using-hadoop/index.html",
    "title": "Using Hadoop",
    "section": "",
    "text": "# check JAVA is running\njps\n\nhdfs namenode -format\nstart-dfs.sh\njps\n\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/john\nhdfs dfs -mkdir /input\nhdfs dfs -put ~/hadoop-3.3.5/etc/hadoop/*.xml /input\nhdfs dfs -ls /input\n\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep /input /output 'dfs[a-z.]+'\nhdfs dfs -ls /output\nhdfs dfs -cat /output/*\n\n# can also copy the output directory to `~/output`\nhdfs dfs -get /output ~/output\ncat ~/output/*\n\nstop-dfs.sh\n\n\n\nstart-yarn.sh\nResourceManager and NodeManager will show up\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ jps\n14357 SecondaryNameNode\n13943 NameNode\n14135 DataNode\n14569 Jps\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ start-yarn.sh\nStarting resourcemanager\nStarting nodemanagers\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ jps\n14833 NodeManager\n14357 SecondaryNameNode\n13943 NameNode\n14135 DataNode\n15145 Jps\n14652 ResourceManager\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$\nBrowse the web interface for the ResourceManager\nhttp://localhost:8088\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep input output 'dfs[a-z.]+'"
  },
  {
    "objectID": "posts/using-hadoop/index.html#run-hadoop-examples",
    "href": "posts/using-hadoop/index.html#run-hadoop-examples",
    "title": "Using Hadoop",
    "section": "",
    "text": "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep input output 'dfs[a-z.]+'"
  },
  {
    "objectID": "posts/using-hadoop/index.html#create-dir-in-hdfs",
    "href": "posts/using-hadoop/index.html#create-dir-in-hdfs",
    "title": "Using Hadoop",
    "section": "",
    "text": "Create dir in HDFS\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/john\nhdfs dfs -mkdir /input"
  },
  {
    "objectID": "posts/using-hadoop/index.html#verify-hdfs-file-system",
    "href": "posts/using-hadoop/index.html#verify-hdfs-file-system",
    "title": "Using Hadoop",
    "section": "",
    "text": "Verify HDFS file system\nstop-dfs.sh\n\nhdfs namenode -format\nstart-dfs.sh\njps\n\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/john\nhdfs dfs -mkdir /input\nhdfs dfs -put etc/hadoop/*.xml /input\nhdfs dfs -ls /input\n\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep /input /output 'dfs[a-z.]+'"
  },
  {
    "objectID": "posts/using-hadoop/index.html#run-mapreduce-examples",
    "href": "posts/using-hadoop/index.html#run-mapreduce-examples",
    "title": "Using Hadoop",
    "section": "",
    "text": "# check JAVA is running\njps\n\nhdfs namenode -format\nstart-dfs.sh\njps\n\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/john\nhdfs dfs -mkdir /input\nhdfs dfs -put ~/hadoop-3.3.5/etc/hadoop/*.xml /input\nhdfs dfs -ls /input\n\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep /input /output 'dfs[a-z.]+'\nhdfs dfs -ls /output\nhdfs dfs -cat /output/*\n\n# can also copy the output directory to `~/output`\nhdfs dfs -get /output ~/output\ncat ~/output/*\n\nstop-dfs.sh"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#obtain-public-key-of-each",
    "href": "posts/setup-hadoop-cluster/index.html#obtain-public-key-of-each",
    "title": "Setup Hadoop Cluster",
    "section": "3.4 Obtain public key of each",
    "text": "3.4 Obtain public key of each\nUsing pdsh and regular expression to retreive the rsa public key of each machine\npdsh -w node[1-2] cat ~/.ssh/id_rsa.pub | sed 's/[^:]*: //' &gt;&gt; text.txt"
  },
  {
    "objectID": "posts/using-hadoop/index.html#solving-between-conflict-between-namenode-clusterid-and-datanode-clusterid",
    "href": "posts/using-hadoop/index.html#solving-between-conflict-between-namenode-clusterid-and-datanode-clusterid",
    "title": "Using Hadoop",
    "section": "",
    "text": "Solving between conflict between Namenode clusterID and Datanode clusterID, the problem is shown in log file as\n\n\nlogs/hadoop-ubuntu-datanode-node0.log\n\n...\n2023-06-22 06:23:24,449 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data\njava.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-ec7e5f04-e16f-4a33-afeb-ce45032aa09c; datanode clusterID = CID-8056152a-0184-4cfe-8fbe-b16cf655140e\n2023-06-22 06:23:24,460 ERROR\n\nvi /tmp/hadoop-ubuntu/dfs/data/current/VERSION\n\n\nVERSION\n\n#Wed Jun 21 22:47:29 ICT 2023\nstorageID=DS-87747f6d-f615-4b10-97ec-019c72544fe8\nclusterID=CID-ec7e5f04-e16f-4a33-afeb-ce45032aa09c\ncTime=0\ndatanodeUuid=3a51fe58-e17d-4ce9-8598-e93378acad0c\nstorageType=DATA_NODE\nlayoutVersion=-57"
  },
  {
    "objectID": "posts/using-hadoop/index.html#conflict-in-clusterids",
    "href": "posts/using-hadoop/index.html#conflict-in-clusterids",
    "title": "Using Hadoop",
    "section": "2.1 Conflict in clusterIDs",
    "text": "2.1 Conflict in clusterIDs\nIf you reinstall or recreate the Hadoop cluster on Multipass Ubuntu, the clusterID might change. The Datanode can not be started. Solving between conflict between Namenode clusterID and Datanode clusterID,\nchecking log with the command\ncat ~/hadoop-3.3.5/logs/hadoop-ubuntu-datanode-node0.log\nthe problem is shown in log file as\n\n\nlogs/hadoop-ubuntu-datanode-node0.log\n\n...\n2023-06-22 06:23:24,449 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data\njava.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-ec7e5f04-e16f-4a33-afeb-ce45032aa09c; datanode clusterID = CID-8056152a-0184-4cfe-8fbe-b16cf655140e\n2023-06-22 06:23:24,460 ERROR\n\nEdit the VERSION file with\nvi /tmp/hadoop-ubuntu/dfs/data/current/VERSION\nChange the clusterID in file as in the log as clusterID = CID-ec7e5f04-e16f-4a33-afeb-ce45032aa09c.\n\n\nVERSION\n\n#Wed Jun 21 22:47:29 ICT 2023\nstorageID=DS-87747f6d-f615-4b10-97ec-019c72544fe8\nclusterID=CID-ec7e5f04-e16f-4a33-afeb-ce45032aa09c\ncTime=0\ndatanodeUuid=3a51fe58-e17d-4ce9-8598-e93378acad0c\nstorageType=DATA_NODE\nlayoutVersion=-57"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configuration-files",
    "href": "posts/setup-hadoop-cluster/index.html#configuration-files",
    "title": "Setup Hadoop Cluster",
    "section": "3.1 Configuration files",
    "text": "3.1 Configuration files\nBy default, Hadoop configurations lie on the following files. In order to set other properties, please check in these files.\nhadoop-3.3.5/share/doc/hadoop/hadoop-project-dist/hadoop-common/core-default.xml\nhadoop-3.3.5/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\nhadoop-3.3.5/share/doc/hadoop/hadoop-yarn/hadoop-yarn-common/yarn-default.xml\nhadoop-3.3.5/share/doc/hadoop/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml\nThen, set into these site-specific configuration for replacement.\nhadoop-3.3.5/etc/hadoop/core-site.xml\nhadoop-3.3.5/etc/hadoop/hdfs-site.xml\nhadoop-3.3.5/etc/hadoop/yarn-site.xml\nhadoop-3.3.5/etc/hadoop/mapred-site.xml"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configuring-environment-of-hadoop-daemons",
    "href": "posts/setup-hadoop-cluster/index.html#configuring-environment-of-hadoop-daemons",
    "title": "Setup Hadoop Cluster",
    "section": "2.6 Configuring Environment of Hadoop Daemons",
    "text": "2.6 Configuring Environment of Hadoop Daemons\nAdministrators should use the\netc/hadoop/hadoop-env.sh\netc/hadoop/mapred-env.sh\netc/hadoop/yarn-env.sh\nto do site-specific customization of the Hadoop daemons’ process environment.\nDo I need the specify the JAVA_HOME to all these etc/hadoop/hadoop-env.sh etc/hadoop/mapred-env.sh etc/hadoop/yarn-env.sh in Hadoop 3.3.5 ?"
  },
  {
    "objectID": "posts/using-hadoop/index.html#incompatibility-issue",
    "href": "posts/using-hadoop/index.html#incompatibility-issue",
    "title": "Using Hadoop",
    "section": "2.2 Incompatibility issue",
    "text": "2.2 Incompatibility issue\nThe error message you encountered in Hadoop indicates that the cluster ID of the NameNode does not match the cluster ID of the DataNode. This mismatch prevents the DataNode from successfully communicating with the NameNode.\nTo resolve this issue, you can follow these steps:\n\nStop all Hadoop services, including the NameNode and DataNode.\nDelete the contents of the directories specified in the dfs.namenode.name.dir and dfs.datanode.data.dir properties. In this case, you should delete the contents of /home/ubuntu/namenode and /home/ubuntu/datanode.\nAfter removing the contents of the directories specified, you need to ensure that the dfs.namenode.name.dir and dfs.datanode.data.dir properties have the same values across all nodes in your Hadoop cluster. Verify that these properties are identical on both the NameNode and DataNode.\nStart the Hadoop services, beginning with the NameNode and then the DataNode.\n\nThe cluster ID in Hadoop is a unique identifier assigned to each Hadoop cluster. It is generated when the NameNode is formatted for the first time. The cluster ID is stored in the VERSION file within the NameNode’s metadata directory.\nWhen a DataNode starts up, it reads the cluster ID from the VERSION file of the NameNode it connects to and compares it with its own cluster ID. If the cluster IDs do not match, it indicates that the DataNode belongs to a different Hadoop cluster. In such a case, the DataNode refuses to join the cluster and throws the “Incompatible clusterIDs” error."
  },
  {
    "objectID": "posts/using-hadoop/index.html#matching-a-word-in-files",
    "href": "posts/using-hadoop/index.html#matching-a-word-in-files",
    "title": "Using Hadoop",
    "section": "",
    "text": "# check JAVA is running\njps\n\nhdfs namenode -format\nstart-dfs.sh\njps\n\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/john\nhdfs dfs -mkdir /input\nhdfs dfs -put ~/hadoop-3.3.5/etc/hadoop/*.xml /input\nhdfs dfs -ls /input\n\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep /input /output 'dfs[a-z.]+'\nhdfs dfs -ls /output\nhdfs dfs -cat /output/*\n\n# can also copy the output directory to `~/output`\nhdfs dfs -get /output ~/output\ncat ~/output/*\n\nstop-dfs.sh"
  },
  {
    "objectID": "posts/using-hadoop/index.html#starting-yarn",
    "href": "posts/using-hadoop/index.html#starting-yarn",
    "title": "Using Hadoop",
    "section": "",
    "text": "start-yarn.sh\nResourceManager and NodeManager will show up\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ jps\n14357 SecondaryNameNode\n13943 NameNode\n14135 DataNode\n14569 Jps\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ start-yarn.sh\nStarting resourcemanager\nStarting nodemanagers\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ jps\n14833 NodeManager\n14357 SecondaryNameNode\n13943 NameNode\n14135 DataNode\n15145 Jps\n14652 ResourceManager\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$\nBrowse the web interface for the ResourceManager\nhttp://localhost:8088\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep input output 'dfs[a-z.]+'"
  },
  {
    "objectID": "posts/hadoop-java-debugging/index.html",
    "href": "posts/hadoop-java-debugging/index.html",
    "title": "Debugging a Hadoop Java Code",
    "section": "",
    "text": "Count all words in files\n\n\nAssume that you have already installed these softwares:\n\nJava (on the Server)\nHadoop (on the Server installed in Standalone Operation)\nCgywin (Optional for the Clients)\n\nThe overall files in this example are shown here. \n\n\n\n\n\nWordCount.java\n\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.*;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCount {\n\n  public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {\n    private final static IntWritable one = new IntWritable(1);\n    private Text word = new Text();\n\n    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n      String line = value.toString();\n      String[] words = line.split(\" \");\n\n      for (String word : words) {\n        this.word.set(word);\n        context.write(this.word, one);\n      }\n    }\n  }\n\n  public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {\n      int sum = 0;\n      for (IntWritable value : values) {\n        sum += value.get();\n      }\n      context.write(key, new IntWritable(sum));\n    }\n  }\n\n  public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    Job job = Job.getInstance(conf, \"wordcount\");\n\n    job.setJarByClass(WordCount.class);\n    job.setMapperClass(Map.class);\n    job.setReducerClass(Reduce.class);\n\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(IntWritable.class);\n\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n\n\n\n\n\n\ncompile-java.sh\n\n#!/bin/bash\necho \"==============================================\"\n\nHADOOP_HOME=\"/home/ubuntu/hadoop-3.3.5\"\n\n# Compile the Java program using the javac command:\njavac -cp \"$HADOOP_HOME/share/hadoop/common/hadoop-common-3.3.5.jar:\\\n$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.5.jar\" \\\nWordCount.java\n\n# Create a JAR file using the jar command:\njar cf WordCount.jar WordCount*.class\n\nls *.jar\n\n\n\n\n\n\nhadoop-process.sh\n\n#!/bin/bash\necho \"==============================================\"\n\nHADOOP_HOME=\"/home/ubuntu/hadoop-3.3.5\"\n\nif [ -d \"output\" ]; then\n    rm -r output\nfi\n\n$HADOOP_HOME/bin/hadoop jar WordCount.jar WordCount input output\n\necho \"==============================================\"\nls -l output\necho \"==============================================\"\ncat output/*\n\n\n\n\nCreate input folder that consists of\n\n\ntest.txt\n\nfish dog fish\n\nand\n\n\ntest2.txt\n\nfish dog fish\n\n\n\n\nRun all scripts\n\n\nrun.sh\n\n#!/bin/bash\n\nremote_ip=\"192.168.1.109\"\nfile_dir=\"example1\"\nfile_path=\"/cygdrive/c/users/Jorgnur/Desktop/hadoop/ver/$file_dir\"\ndest_path=\"~/\"\n\nrsync -avzphi \"$file_path\" \"ubuntu@$remote_ip:$dest_path\"\n\nssh ubuntu@$remote_ip \" \\\n    cd $dest_path$file_dir && \\\n    ./compile-java.sh && \\\n    ./hadoop-process.sh\n\"\n\nWe can run the script by typing ./run.sh\n\n\n\npic.2 Running the script\n\n\nThen, we can obtain 2 words of dogs and 4 words of fish.\n\n\n\npic.3 Resuts"
  },
  {
    "objectID": "posts/hadoop-java-debugging/index.html#word-count-in-java-code",
    "href": "posts/hadoop-java-debugging/index.html#word-count-in-java-code",
    "title": "Debugging a Hadoop Java Code",
    "section": "",
    "text": "WordCount.java\n\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.*;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCount {\n\n  public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {\n    private final static IntWritable one = new IntWritable(1);\n    private Text word = new Text();\n\n    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n      String line = value.toString();\n      String[] words = line.split(\" \");\n\n      for (String word : words) {\n        this.word.set(word);\n        context.write(this.word, one);\n      }\n    }\n  }\n\n  public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {\n      int sum = 0;\n      for (IntWritable value : values) {\n        sum += value.get();\n      }\n      context.write(key, new IntWritable(sum));\n    }\n  }\n\n  public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    Job job = Job.getInstance(conf, \"wordcount\");\n\n    job.setJarByClass(WordCount.class);\n    job.setMapperClass(Map.class);\n    job.setReducerClass(Reduce.class);\n\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(IntWritable.class);\n\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}"
  },
  {
    "objectID": "posts/hadoop-java-debugging/index.html#compile-java-into-jar",
    "href": "posts/hadoop-java-debugging/index.html#compile-java-into-jar",
    "title": "Debugging a Hadoop Java Code",
    "section": "",
    "text": "compile-java.sh\n\n#!/bin/bash\necho \"==============================================\"\n\nHADOOP_HOME=\"/home/ubuntu/hadoop-3.3.5\"\n\n# Compile the Java program using the javac command:\njavac -cp \"$HADOOP_HOME/share/hadoop/common/hadoop-common-3.3.5.jar:\\\n$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.5.jar\" \\\nWordCount.java\n\n# Create a JAR file using the jar command:\njar cf WordCount.jar WordCount*.class\n\nls *.jar"
  },
  {
    "objectID": "posts/hadoop-java-debugging/index.html#process-a-jar-file-in-hadoop",
    "href": "posts/hadoop-java-debugging/index.html#process-a-jar-file-in-hadoop",
    "title": "Debugging a Hadoop Java Code",
    "section": "",
    "text": "hadoop-process.sh\n\n#!/bin/bash\necho \"==============================================\"\n\nHADOOP_HOME=\"/home/ubuntu/hadoop-3.3.5\"\n\nif [ -d \"output\" ]; then\n    rm -r output\nfi\n\n$HADOOP_HOME/bin/hadoop jar WordCount.jar WordCount input output\n\necho \"==============================================\"\nls -l output\necho \"==============================================\"\ncat output/*"
  },
  {
    "objectID": "posts/hadoop-java-debugging/index.html#create-input-files",
    "href": "posts/hadoop-java-debugging/index.html#create-input-files",
    "title": "Debugging a Hadoop Java Code",
    "section": "",
    "text": "Create input folder that consists of\n\n\ntest.txt\n\nfish dog fish\n\nand\n\n\ntest2.txt\n\nfish dog fish"
  },
  {
    "objectID": "posts/hadoop-java-debugging/index.html#run-all-scripts",
    "href": "posts/hadoop-java-debugging/index.html#run-all-scripts",
    "title": "Debugging a Hadoop Java Code",
    "section": "",
    "text": "Run all scripts\n\n\nrun.sh\n\n#!/bin/bash\n\nremote_ip=\"192.168.1.109\"\nfile_dir=\"example1\"\nfile_path=\"/cygdrive/c/users/Jorgnur/Desktop/hadoop/ver/$file_dir\"\ndest_path=\"~/\"\n\nrsync -avzphi \"$file_path\" \"ubuntu@$remote_ip:$dest_path\"\n\nssh ubuntu@$remote_ip \" \\\n    cd $dest_path$file_dir && \\\n    ./compile-java.sh && \\\n    ./hadoop-process.sh\n\"\n\nWe can run the script by typing ./run.sh\n\n\n\npic.2 Running the script\n\n\nThen, we can obtain 2 words of dogs and 4 words of fish.\n\n\n\npic.3 Resuts"
  },
  {
    "objectID": "posts/hadoop-java-debugging/index.html#prerequisites",
    "href": "posts/hadoop-java-debugging/index.html#prerequisites",
    "title": "Debugging a Hadoop Java Code",
    "section": "",
    "text": "Assume that you have already installed these softwares:\n\nJava (on the Server)\nHadoop (on the Server installed in Standalone Operation)\nCgywin (Optional for the Clients)\n\nThe overall files in this example are shown here."
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#install-ubuntu-server",
    "href": "posts/setup-hadoop-cluster/index.html#install-ubuntu-server",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "For example, I will use Ubuntu Server 20.04.6 in this setup. Install it in virtual box and don’t forget to install ssh I have set CPU 2 cors, Memory 4 GB, Space 25 GB By default, the network is set as NAT, so that the VM can use internet.\nInstall java version 8 for Hadoop.\nsudo apt update\nsudo apt install openjdk-8-jdk"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#install-virtual-machine",
    "href": "posts/setup-hadoop-cluster/index.html#install-virtual-machine",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "For example, I will use Ubuntu Server 20.04.6 in this setup. Install it in Virtual Box and don’t forget to install ssh I have set CPU 2 cors, Memory 4 GB and Space 25 GB. By default, the network is set as NAT, so that the VM can use internet."
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#install-java-version-8-for-hadoop.",
    "href": "posts/setup-hadoop-cluster/index.html#install-java-version-8-for-hadoop.",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "Install java version 8 for Hadoop. This is the required java version.\nsudo apt update\nsudo apt install openjdk-8-jdk\nThe next is to set the variables to ~/.bashrc. To add a literal string to the ~/.bashrc file in Ubuntu, you can use the echo command with single quotes to prevent variable expansion.\necho 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' &gt;&gt; ~/.bashrc\necho 'export PATH=$JAVA_HOME/bin:$PATH' &gt;&gt; ~/.bashrc\n# reload\nsource ~/.bashrc\n# check\necho $JAVA_HOME\nUsing single quotes (') ensures that the string is treated as a literal and preserves the $PATH variable for future evaluation when the .bashrc file is sourced.\nLet’s visit released page and download hadoop-3.3.5.tar.gz or using the following command.\nwget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\nExtract the file with the command:\ntar -zxvf hadoop-3.3.5.tar.gz\n# remove a tar file\nrm hadoop-3.3.5.tar.gz\necho 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' &gt;&gt; ~/.bashrc\necho 'export PATH=$JAVA_HOME/bin:$PATH' &gt;&gt; ~/.bashrc\n# reload\nsource ~/.bashrc\n# check\necho $JAVA_HOME"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#install-java",
    "href": "posts/setup-hadoop-cluster/index.html#install-java",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "Install java version 8 for Hadoop. This is the required java version.\nsudo apt update\nsudo apt install openjdk-8-jdk\nThe next is to set the variables to ~/.bashrc. To add a literal string to the ~/.bashrc file in Ubuntu, you can use the echo command with single quotes to prevent variable expansion.\necho 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' &gt;&gt; ~/.bashrc\necho 'export PATH=$JAVA_HOME/bin:$PATH' &gt;&gt; ~/.bashrc\n# reload\nsource ~/.bashrc\n# check\necho $JAVA_HOME\nUsing single quotes (') ensures that the string is treated as a literal and preserves the $PATH variable for future evaluation when the .bashrc file is sourced."
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#download-hadoop",
    "href": "posts/setup-hadoop-cluster/index.html#download-hadoop",
    "title": "Setup Hadoop Cluster",
    "section": "",
    "text": "Let’s visit released page and download hadoop-3.3.5.tar.gz or using the following command.\nwget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\nExtract the file with the command:\ntar -zxvf hadoop-3.3.5.tar.gz\n# remove a tar file\nrm hadoop-3.3.5.tar.gz\nIn order to run the Hadoop command without specifying the directory, let add the variables. Assume that the directory of hadoop is /home/ubuntu/hadoop-3.3.5, you can use:\necho 'export HADOOP_HOME=/home/ubuntu/hadoop-3.3.5' &gt;&gt; ~/.bashrc\necho 'export PATH=$PATH:$HADOOP_HOME/bin' &gt;&gt; ~/.bashrc\necho 'export PATH=$PATH:$HADOOP_HOME/sbin' &gt;&gt; ~/.bashrc\n# reload\nsource ~/.bashrc\n# check\necho $HADOOP_HOME"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#clone-virtual-machine",
    "href": "posts/setup-hadoop-cluster/index.html#clone-virtual-machine",
    "title": "Setup Hadoop Cluster",
    "section": "2.1 Clone virtual machine",
    "text": "2.1 Clone virtual machine\nClone virtual machine and change the files /etc/hostname and /etc/hosts to replace the word of the original VM, then reboot the system.\n\n\n\nhostname\nassigned role\nIP address\n\n\n\n\nnode0\nnamenode\n192.168.1.127\n\n\nnode1\ndatanode\n192.168.1.128\n\n\nnode2\ndatanode\n192.168.1.129\n\n\n\nIf the clone VM have the same IP address as a original one, see Section 3.5."
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#assign-datanode-in-namenode",
    "href": "posts/setup-hadoop-cluster/index.html#assign-datanode-in-namenode",
    "title": "Setup Hadoop Cluster",
    "section": "2.3 Assign datanode in namenode",
    "text": "2.3 Assign datanode in namenode\nIn namenode, it might need declare hostname of workers (datanode).\nvi /etc/hosts\n192.168.1.128 node1\n192.168.1.129 node2\nSet workers IP address only in namenode.\nvi ~/hadoop-3.3.5/etc/hadoop/workers\n192.168.1.128\n192.168.1.129"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#start-the-cluster",
    "href": "posts/setup-hadoop-cluster/index.html#start-the-cluster",
    "title": "Setup Hadoop Cluster",
    "section": "2.5 Start the cluster",
    "text": "2.5 Start the cluster\n\nStart processes\nstart-dfs.sh\nstart-yarn.sh\n\n\nStop processes\nstop-dfs.sh\nstop-yarn.sh\n\n\nCheck running Java nodes\njps"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#hadoop-web-ui",
    "href": "posts/setup-hadoop-cluster/index.html#hadoop-web-ui",
    "title": "Setup Hadoop Cluster",
    "section": "2.6 Hadoop Web UI",
    "text": "2.6 Hadoop Web UI\nLet’s go the browser HDFS http://localhost:9870/ and ResourceManager - http://localhost:8088/ of the Namenode, thus access with http://192.168.1.127:9870/ from outside a VM."
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#hadoop-daemons",
    "href": "posts/setup-hadoop-cluster/index.html#hadoop-daemons",
    "title": "Setup Hadoop Cluster",
    "section": "3.4 Hadoop Daemons",
    "text": "3.4 Hadoop Daemons\nHadoop Daemons can be checked via the command:\njps\nHDFS daemons will be listed when the command start-dfs.sh is used.\nNameNode\nSecondaryNameNode\nDataNode\nYARN daemons will be listed when the command start-yarn.sh is used.\nResourceManager\nNodeManager\nWebAppProxy"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#sec-get_different_IP",
    "href": "posts/setup-hadoop-cluster/index.html#sec-get_different_IP",
    "title": "Setup Hadoop Cluster",
    "section": "3.5 How to get different IP on cloned machines ?",
    "text": "3.5 How to get different IP on cloned machines ?\nI have created Ubuntu Server VM, then clone it again. By checking\nip a\nThe ip address also the same as the previous VM.\nI figure out to use NetworkManager to handle IP address in User-host Adapter Network Settings. You can check the status of the NetworkManager service by running the following command:\nsystemctl status NetworkManager\nIf it is not available, install with commands\nsudo apt update\nsudo apt install network-manager\n# check it again\nsystemctl status NetworkManager\n# or start it manually\nsudo systemctl start NetworkManager\nThen, change network plan\ncd /etc/netplan\nls\nOpen the yaml file cat 00-installer-config.yaml\n# This is the network config written by 'subiquity'\nnetwork:\n  ethernets:\n    enp0s3:\n      dhcp4: true\n  version: 2\nand chang it into\n# Let NetworkManager manage all devices on this system\nnetwork:\n  version: 2\n  renderer: NetworkManager\nsudo netplan try\nsudo netplan apply\n# check the IP\nip a"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#sec-get-different-IP",
    "href": "posts/setup-hadoop-cluster/index.html#sec-get-different-IP",
    "title": "Setup Hadoop Cluster",
    "section": "3.5 How to get different IP on cloned machines ?",
    "text": "3.5 How to get different IP on cloned machines ?\nFirstly, check what the ip address on a VM.\nip a\nI figure out how to use NetworkManager to handle IP address in User-host Adapter Network Settings. You can check the status of the NetworkManager service by running the following command:\nsystemctl status NetworkManager\nIf it is not available, install with commands\nsudo apt update\nsudo apt install network-manager\n# check it again\nsystemctl status NetworkManager\n# or start it manually\nsudo systemctl start NetworkManager\nThen, change yaml in /etc/netplan\ncd /etc/netplan\nls\nOpen the yaml file, e.g., cat 00-installer-config.yaml\n# This is the network config written by 'subiquity'\nnetwork:\n  ethernets:\n    enp0s3:\n      dhcp4: true\n  version: 2\nand chang it into\n# Let NetworkManager manage all devices on this system\nnetwork:\n  version: 2\n  renderer: NetworkManager\nsudo netplan try\nsudo netplan apply\n# check the IP\nip a"
  },
  {
    "objectID": "posts/using-apache-spark/index.html",
    "href": "posts/using-apache-spark/index.html",
    "title": "Using Apache Spark",
    "section": "",
    "text": "Apache Spark can be download here Download Apache Spark\n\n\n\ntar -zxvf spark-3.4.1-bin-hadoop3.tgz\nmv spark-3.4.1-bin-hadoop3 spark\nrm spark-3.4.1-bin-hadoop3.tgz\n\n\n\necho 'export SPARK_HOME=/home/ubuntu/spark' &gt;&gt; ~/.bashrc\necho 'export PATH=$PATH:$SPARK_HOME/bin' &gt;&gt; ~/.bashrc\n# reload\nsource ~/.bashrc\n# check\necho $SPARK_HOME\n\n\n\necho 'export SPARK_HOME=/home/ubuntu/spark' &gt;&gt; ~/.bashrc\necho 'export PATH=$PATH:$SPARK_HOME/bin' &gt;&gt; ~/.bashrc\n# reload\nsource ~/.bashrc\n# check\necho $SPARK_HOME\n\n\n\ncp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh\nvi $SPARK_HOME/conf/spark-env.sh\n\ncp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf\nvi $SPARK_HOME/conf/spark-defaults.conf\n\n\n\nspark-shell\n\n\n\nrun-example SparkPi 10"
  },
  {
    "objectID": "posts/using-apache-spark/index.html#download-apache-spark",
    "href": "posts/using-apache-spark/index.html#download-apache-spark",
    "title": "Using Apache Spark",
    "section": "",
    "text": "Apache Spark can be download here Download Apache Spark"
  },
  {
    "objectID": "posts/using-apache-spark/index.html#download-apache-spark-1",
    "href": "posts/using-apache-spark/index.html#download-apache-spark-1",
    "title": "Using Apache Spark",
    "section": "",
    "text": "tar -zxvf spark-3.4.1-bin-hadoop3.tgz\nmv spark-3.4.1-bin-hadoop3 spark\nrm spark-3.4.1-bin-hadoop3.tgz"
  },
  {
    "objectID": "posts/using-apache-spark/index.html#set-environment-variables",
    "href": "posts/using-apache-spark/index.html#set-environment-variables",
    "title": "Using Apache Spark",
    "section": "",
    "text": "echo 'export SPARK_HOME=/home/ubuntu/spark' &gt;&gt; ~/.bashrc\necho 'export PATH=$PATH:$SPARK_HOME/bin' &gt;&gt; ~/.bashrc\n# reload\nsource ~/.bashrc\n# check\necho $SPARK_HOME"
  },
  {
    "objectID": "posts/using-apache-spark/index.html#set-environment-variables-1",
    "href": "posts/using-apache-spark/index.html#set-environment-variables-1",
    "title": "Using Apache Spark",
    "section": "",
    "text": "echo 'export SPARK_HOME=/home/ubuntu/spark' &gt;&gt; ~/.bashrc\necho 'export PATH=$PATH:$SPARK_HOME/bin' &gt;&gt; ~/.bashrc\n# reload\nsource ~/.bashrc\n# check\necho $SPARK_HOME"
  },
  {
    "objectID": "posts/using-apache-spark/index.html#update-configuration",
    "href": "posts/using-apache-spark/index.html#update-configuration",
    "title": "Using Apache Spark",
    "section": "",
    "text": "cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh\nvi $SPARK_HOME/conf/spark-env.sh\n\ncp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf\nvi $SPARK_HOME/conf/spark-defaults.conf"
  },
  {
    "objectID": "posts/using-apache-spark/index.html#using-spark-shell",
    "href": "posts/using-apache-spark/index.html#using-spark-shell",
    "title": "Using Apache Spark",
    "section": "",
    "text": "spark-shell"
  },
  {
    "objectID": "posts/using-apache-spark/index.html#run-spark-examples",
    "href": "posts/using-apache-spark/index.html#run-spark-examples",
    "title": "Using Apache Spark",
    "section": "",
    "text": "run-example SparkPi 10"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html",
    "href": "posts/kafka-stream-wordcount/index.html",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "This code sets up a Kafka Streams application that reads sentences from the “sentences” topic, performs a word count analysis, and writes the result to the “word-count” topic using the Kafka Streams DSL. The configuration properties define the necessary Kafka and Kafka Streams settings for the application to connect to Kafka brokers and process the data.\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.Materialized;\n\nimport java.util.Arrays;\nimport java.util.Properties;\n\npublic class WordCountApp {\n\n    private static Properties getConfig() {\n        Properties properties = new Properties();\n        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, \"word-count-app\");\n        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, \"0\");\n        return properties;\n    }\n\n    public static void main(String[] args) {\n        Properties props = getConfig();\n\n        StreamsBuilder streamsBuilder = new StreamsBuilder();\n        \n        streamsBuilder.&lt;String, String&gt;stream(\"sentences\")\n                .flatMapValues((key, value) -&gt;\n                        Arrays.asList(value.toLowerCase()\n                            .split(\" \")))\n                .groupBy((key, value) -&gt; value)\n                .count(Materialized.with(Serdes.String(), Serdes.Long()))\n                .toStream()\n                .to(\"word-count\", Produced.with(Serdes.String(), Serdes.Long()));\n        \n        KafkaStreams kafkaStreams = new KafkaStreams(streamsBuilder.build(), props);\n\n        kafkaStreams.start();\n\n        Runtime.getRuntime().addShutdownHook(new Thread(kafkaStreams::close));\n    }\n}\nThe provided code is a simple Apache Kafka Streams application written in Java for performing a word count analysis on input sentences. Let’s break down the code and explain its functionality:\n\n\nThe necessary import statements are included to import required classes and interfaces from the Apache Kafka Streams library and other Kafka-related dependencies.\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.Materialized;\n\nimport java.util.Arrays;\nimport java.util.Properties;\n\n...\n\n\n\nThis class contains the main method, which serves as the entry point for the application.\nimport ...\n\npublic class WordCountApp {\n    ...\n}\n\n\n\nThis method is responsible for configuring and returning a Properties object that holds various Kafka Streams configuration settings. These settings include the application ID, bootstrap servers, default key and value serde classes, consumer auto-offset reset, and caching configuration. The broker server localhost:9092 can be setup as in quick start.\npublic class WordCountApp {\n    private static Properties getConfig() {\n        Properties properties = new Properties();\n        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, \"word-count-app\");\n        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, \"0\");\n        return properties;\n    }\n    ...\n}\n\n\n\nThe main method is where the application logic is implemented.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n    }\n}\n\n\n\nThe getConfig() method is called to obtain the Kafka Streams configuration properties.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        Properties props = getConfig();\n        ...\n    }\n}\n\n\n\nAn instance of StreamsBuilder is created, which serves as the high-level Kafka Streams DSL for building the processing topology.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n        StreamsBuilder streamsBuilder = new StreamsBuilder();\n        ...\n    }\n}\n\n\n\nThe processing topology is defined using the methods and operations provided by the Kafka Streams DSL. In this case, the following operations are chained together: - stream(): Reads input records from the “sentences” Kafka topic. - flatMapValues(): Tokenizes each sentence into individual words and converts them to lowercase. - groupBy(): Groups the words by their value. - count(): Performs a count aggregation on the grouped words. - toStream(): Converts the result into a KStream of key-value pairs. - to(): Writes the result to the “word-count” Kafka topic.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n        streamsBuilder.&lt;String, String&gt;stream(\"sentences\")\n                .flatMapValues((key, value) -&gt;\n                        Arrays.asList(value.toLowerCase()\n                            .split(\" \")))\n                .groupBy((key, value) -&gt; value)\n                .count(Materialized.with(Serdes.String(), Serdes.Long()))\n                .toStream()\n                .to(\"word-count\", Produced.with(Serdes.String(), Serdes.Long()));\n        ...                \n    }\n}\n\n\n\nAn instance of KafkaStreams is created, which represents the Kafka Streams client that runs the processing topology.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n        KafkaStreams kafkaStreams = new KafkaStreams(streamsBuilder.build(), props);\n        ...\n    }\n}\n\n\n\nThe Kafka Streams client is started, and the processing of input records begins.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n        kafkaStreams.start();\n        ...\n    }\n}\n\n\n\nA shutdown hook is registered to gracefully close the Kafka Streams client when the application is terminated.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n        Runtime.getRuntime().addShutdownHook(new Thread(kafkaStreams::close));\n    }\n}\n\n\n\nThe results should work as followed: \n\n\n\npic.2 Kafka Consumer Console"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#java-code",
    "href": "posts/kafka-stream-wordcount/index.html#java-code",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "Overall, this code sets up a Kafka Streams application that reads sentences from the “sentences” topic, performs a word count analysis, and writes the result to the “word-count” topic using the Kafka Streams DSL. The configuration properties define the necessary Kafka and Kafka Streams settings for the application to connect to Kafka brokers and process the data.\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.Materialized;\n\nimport java.util.Arrays;\nimport java.util.Properties;\n\npublic class WordCountApp {\n\n    private static Properties getConfig() {\n        Properties properties = new Properties();\n        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, \"word-count-app\");\n        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:29092\");\n        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, \"0\");\n        return properties;\n    }\n\n    public static void main(String[] args) {\n        Properties props = getConfig();\n\n        StreamsBuilder streamsBuilder = new StreamsBuilder();\n        \n        streamsBuilder.&lt;String, String&gt;stream(\"sentences\")\n                .flatMapValues((key, value) -&gt;\n                        Arrays.asList(value.toLowerCase()\n                            .split(\" \")))\n                .groupBy((key, value) -&gt; value)\n                .count(Materialized.with(Serdes.String(), Serdes.Long()))\n                .toStream()\n                .to(\"word-count\", Produced.with(Serdes.String(), Serdes.Long()));\n        \n        KafkaStreams kafkaStreams = new KafkaStreams(streamsBuilder.build(), props);\n\n        kafkaStreams.start();\n\n        Runtime.getRuntime().addShutdownHook(new Thread(kafkaStreams::close));\n    }\n}\nThe provided code is a simple Apache Kafka Streams application written in Java for performing a word count analysis on input sentences. Let’s go through the code and explain its functionality:"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#import-statements",
    "href": "posts/kafka-stream-wordcount/index.html#import-statements",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "The necessary import statements are included to import required classes and interfaces from the Apache Kafka Streams library and other Kafka-related dependencies.\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.Materialized;\n\nimport java.util.Arrays;\nimport java.util.Properties;\n\n..."
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#wordcountapp-class",
    "href": "posts/kafka-stream-wordcount/index.html#wordcountapp-class",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "This class contains the main method, which serves as the entry point for the application.\nimport ...\n\npublic class WordCountApp {\n    ...\n}"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#getconfig-method",
    "href": "posts/kafka-stream-wordcount/index.html#getconfig-method",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "This method is responsible for configuring and returning a Properties object that holds various Kafka Streams configuration settings. These settings include the application ID, bootstrap servers, default key and value serde classes, consumer auto-offset reset, and caching configuration. The broker server localhost:9092 can be setup as in quick start.\npublic class WordCountApp {\n    private static Properties getConfig() {\n        Properties properties = new Properties();\n        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, \"word-count-app\");\n        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, \"0\");\n        return properties;\n    }\n    ...\n}"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#main-method",
    "href": "posts/kafka-stream-wordcount/index.html#main-method",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "The main method is where the application logic is implemented.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#properties-initialization",
    "href": "posts/kafka-stream-wordcount/index.html#properties-initialization",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "The getConfig() method is called to obtain the Kafka Streams configuration properties.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        Properties props = getConfig();\n        ...\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#streamsbuilder",
    "href": "posts/kafka-stream-wordcount/index.html#streamsbuilder",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "An instance of StreamsBuilder is created, which serves as the high-level Kafka Streams DSL for building the processing topology.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n        StreamsBuilder streamsBuilder = new StreamsBuilder();\n        ...\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#stream-processing-topology",
    "href": "posts/kafka-stream-wordcount/index.html#stream-processing-topology",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "The processing topology is defined using the methods and operations provided by the Kafka Streams DSL. In this case, the following operations are chained together: - stream(): Reads input records from the “sentences” Kafka topic. - flatMapValues(): Tokenizes each sentence into individual words and converts them to lowercase. - groupBy(): Groups the words by their value. - count(): Performs a count aggregation on the grouped words. - toStream(): Converts the result into a KStream of key-value pairs. - to(): Writes the result to the “word-count” Kafka topic.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n        streamsBuilder.&lt;String, String&gt;stream(\"sentences\")\n                .flatMapValues((key, value) -&gt;\n                        Arrays.asList(value.toLowerCase()\n                            .split(\" \")))\n                .groupBy((key, value) -&gt; value)\n                .count(Materialized.with(Serdes.String(), Serdes.Long()))\n                .toStream()\n                .to(\"word-count\", Produced.with(Serdes.String(), Serdes.Long()));\n        ...                \n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#kafkastreams",
    "href": "posts/kafka-stream-wordcount/index.html#kafkastreams",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "An instance of KafkaStreams is created, which represents the Kafka Streams client that runs the processing topology.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n        KafkaStreams kafkaStreams = new KafkaStreams(streamsBuilder.build(), props);\n        ...\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#kafkastreams.start",
    "href": "posts/kafka-stream-wordcount/index.html#kafkastreams.start",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "The Kafka Streams client is started, and the processing of input records begins.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n        kafkaStreams.start();\n        ...\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#shutdown-hook",
    "href": "posts/kafka-stream-wordcount/index.html#shutdown-hook",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "A shutdown hook is registered to gracefully close the Kafka Streams client when the application is terminated.\nimport ...\n\npublic class WordCountApp {\n    ...\n    public static void main(String[] args) {\n        ...\n        Runtime.getRuntime().addShutdownHook(new Thread(kafkaStreams::close));\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-wordcount/index.html#results",
    "href": "posts/kafka-stream-wordcount/index.html#results",
    "title": "WordCount using Kafka Streams",
    "section": "",
    "text": "The results should work as followed: \n\n\n\npic.2 Kafka Consumer Console"
  },
  {
    "objectID": "posts/kafka-stream-transaction/index.html",
    "href": "posts/kafka-stream-transaction/index.html",
    "title": "Bank Transactions using Kafka Streams",
    "section": "",
    "text": "&gt; tree streams.examples\n    streams-quickstart\n    |-- pom.xml\n    |-- src\n        |-- main\n            |-- java\n            |   |-- myapps\n            |       |-- config\n            |           |-- Pipe.java\n            |       |-- model\n            |           |-- BankBalance.java\n            |           |-- BankTransaction.java\n            |           |-- JsonSerde.java\n            |       |-- topology\n            |           |-- BankBalanceTopology.java\n            |       |-- BankBalanceApp.java            \n            |-- resources\n                |-- log4j.properties\n\n\npackage myapps.config;\n\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.StreamsConfig;\n\nimport java.util.Properties;\n\npublic class StreamConfiguration {\n    public static Properties getConfiguration() {\n        Properties properties = new Properties();\n        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, \"bank-balance\");\n        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:29092\");\n        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, \"0\");\n        return properties;\n    }\n}\n\n\n\npackage myapps.model;\n\nimport lombok.AllArgsConstructor;\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\n\nimport java.math.BigDecimal;\nimport java.util.Date;\n\n@Data\n@NoArgsConstructor\n@AllArgsConstructor\npublic class BankBalance {\n\n    private Long id;\n    private BigDecimal amount = BigDecimal.ZERO;\n    private Date lastUpdate;\n    private BankTransaction latestTransaction;\n\n    public BankBalance process(BankTransaction bankTransaction) {\n        this.id = bankTransaction.getBalanceId();\n        this.latestTransaction = bankTransaction;\n\n        if (this.amount.add(bankTransaction.getAmount()).compareTo(BigDecimal.ZERO) &gt;= 0) {\n            this.latestTransaction.setState(BankTransaction.BankTransactionState.APPROVED);\n            this.amount = this.amount.add(bankTransaction.getAmount());\n        } else {\n            this.latestTransaction.setState(BankTransaction.BankTransactionState.REJECTED);\n        }\n\n        this.lastUpdate = bankTransaction.getTime();\n        return this;\n    }\n}\n\n\n\npackage myapps.model;\n\nimport com.fasterxml.jackson.annotation.JsonFormat;\nimport lombok.AllArgsConstructor;\nimport lombok.Builder;\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\n\nimport java.math.BigDecimal;\nimport java.util.Date;\n\n@Data\n@NoArgsConstructor\n@AllArgsConstructor\n@Builder\npublic class BankTransaction {\n\n    private Long id;\n    private Long balanceId;\n    private BigDecimal amount;\n    @JsonFormat(shape = JsonFormat.Shape.STRING,\n                pattern = \"dd-MM-yyyy hh:mm:ss\")\n    public Date time;\n    @Builder.Default\n    public BankTransactionState state = BankTransactionState.CREATED;\n\n    public static enum BankTransactionState {\n        CREATED, APPROVED, REJECTED\n    }\n}\n\n\n\npackage myapps.model;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport lombok.SneakyThrows;\nimport org.apache.kafka.common.serialization.Deserializer;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.serialization.Serializer;\n\npublic class JsonSerde&lt;T&gt; implements Serde&lt;T&gt; {\n\n    public static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();\n    private final Class&lt;T&gt; type;\n\n    public JsonSerde(Class&lt;T&gt; type) {\n        this.type = type;\n    }\n\n    @Override\n    public Serializer&lt;T&gt; serializer() {\n        return (topic, data) -&gt; serialize(data);\n    }\n\n    @SneakyThrows\n    private byte[] serialize(T data) {\n        return OBJECT_MAPPER.writeValueAsBytes(data);\n    }\n\n    @Override\n    public Deserializer&lt;T&gt; deserializer() {\n        return (topic, bytes) -&gt; deserialize(bytes);\n    }\n\n    @SneakyThrows\n    private T deserialize(byte[] bytes) {\n        return OBJECT_MAPPER.readValue(bytes, type);\n    }\n}\n\n\n\npackage myapps.topology;\n\nimport myapps.model.BankBalance;\nimport myapps.model.BankTransaction;\nimport myapps.model.JsonSerde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.kstream.Produced;\n\npublic class BankBalanceTopology {\n\n    public static final String BANK_TRANSACTIONS = \"bank-transactions\";\n    public static final String BANK_BALANCES = \"bank-balances\";\n    public static final String REJECTED_TRANSACTIONS = \"rejected-transactions\";\n\n    public static Topology buildTopology() {\n        StreamsBuilder streamsBuilder = new StreamsBuilder();\n        var bankTransactionJsonSerde = new JsonSerde&lt;BankTransaction&gt;(BankTransaction.class);\n        var bankBalanceJsonSerde = new JsonSerde&lt;&gt;(BankBalance.class);\n\n        var bankBalanceKStream = streamsBuilder.stream(BANK_TRANSACTIONS,\n                        Consumed.with(Serdes.Long(), bankTransactionJsonSerde))\n                .groupByKey()\n                .aggregate(BankBalance::new,\n                        (key, value, aggregate) -&gt; aggregate.process(value),\n                        Materialized.with(Serdes.Long(), bankBalanceJsonSerde))\n                .toStream();\n\n        bankBalanceKStream.to(BANK_BALANCES,\n                Produced.with(Serdes.Long(), bankBalanceJsonSerde));\n\n        bankBalanceKStream.mapValues((readOnlyKey, value) -&gt; value.getLatestTransaction())\n                .filter((key, value) -&gt; value.getState().equals(BankTransaction.BankTransactionState.REJECTED))\n                .to(REJECTED_TRANSACTIONS, Produced.with(Serdes.Long(), bankTransactionJsonSerde));\n\n        return streamsBuilder.build();\n    }\n}\n\n\n\npackage myapps;\n\nimport myapps.config.StreamConfiguration;\nimport myapps.topology.BankBalanceTopology;\nimport org.apache.kafka.streams.KafkaStreams;\n\npublic class BankBalanceApp {\n\n    public static void main(String[] args) {\n        var configuration = StreamConfiguration.getConfiguration();\n        var topology = BankBalanceTopology.buildTopology();\n        var kafkaStreams = new KafkaStreams(topology, configuration);\n\n        kafkaStreams.start();\n\n        Runtime.getRuntime().addShutdownHook(new Thread(kafkaStreams::close));\n    }\n}\n\n\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;myapps&lt;/groupId&gt;\n    &lt;artifactId&gt;bank-transactions&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n\n    &lt;dependencies&gt;\n        &lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-streams --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n            &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;\n            &lt;version&gt;2.8.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;\n            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;\n            &lt;version&gt;1.7.30&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;\n            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;\n            &lt;version&gt;1.7.30&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;\n            &lt;artifactId&gt;lombok&lt;/artifactId&gt;\n            &lt;version&gt;RELEASE&lt;/version&gt;\n            &lt;scope&gt;compile&lt;/scope&gt;\n        &lt;/dependency&gt;\n\n        &lt;!-- TEST --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;\n            &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt;\n            &lt;version&gt;RELEASE&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n            &lt;artifactId&gt;kafka-streams-test-utils&lt;/artifactId&gt;\n            &lt;version&gt;2.8.0&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.mockito&lt;/groupId&gt;\n            &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;\n            &lt;version&gt;3.11.2&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.mockito&lt;/groupId&gt;\n            &lt;artifactId&gt;mockito-junit-jupiter&lt;/artifactId&gt;\n            &lt;version&gt;2.23.0&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n\n    &lt;/dependencies&gt;\n\n\n    &lt;properties&gt;\n        &lt;maven.compiler.source&gt;15&lt;/maven.compiler.source&gt;\n        &lt;maven.compiler.target&gt;15&lt;/maven.compiler.target&gt;\n    &lt;/properties&gt;\n&lt;/project&gt;"
  },
  {
    "objectID": "posts/kafka-stream-transaction/index.html#kafka-streams-configuration",
    "href": "posts/kafka-stream-transaction/index.html#kafka-streams-configuration",
    "title": "Bank Transactions using Kafka Streams",
    "section": "",
    "text": "package myapps.config;\n\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.StreamsConfig;\n\nimport java.util.Properties;\n\npublic class StreamConfiguration {\n    public static Properties getConfiguration() {\n        Properties properties = new Properties();\n        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, \"bank-balance\");\n        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:29092\");\n        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, \"0\");\n        return properties;\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-transaction/index.html#bankbalance.java",
    "href": "posts/kafka-stream-transaction/index.html#bankbalance.java",
    "title": "Bank Transactions using Kafka Streams",
    "section": "",
    "text": "package myapps.model;\n\nimport lombok.AllArgsConstructor;\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\n\nimport java.math.BigDecimal;\nimport java.util.Date;\n\n@Data\n@NoArgsConstructor\n@AllArgsConstructor\npublic class BankBalance {\n\n    private Long id;\n    private BigDecimal amount = BigDecimal.ZERO;\n    private Date lastUpdate;\n    private BankTransaction latestTransaction;\n\n    public BankBalance process(BankTransaction bankTransaction) {\n        this.id = bankTransaction.getBalanceId();\n        this.latestTransaction = bankTransaction;\n\n        if (this.amount.add(bankTransaction.getAmount()).compareTo(BigDecimal.ZERO) &gt;= 0) {\n            this.latestTransaction.setState(BankTransaction.BankTransactionState.APPROVED);\n            this.amount = this.amount.add(bankTransaction.getAmount());\n        } else {\n            this.latestTransaction.setState(BankTransaction.BankTransactionState.REJECTED);\n        }\n\n        this.lastUpdate = bankTransaction.getTime();\n        return this;\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-transaction/index.html#banktransaction.java",
    "href": "posts/kafka-stream-transaction/index.html#banktransaction.java",
    "title": "Bank Transactions using Kafka Streams",
    "section": "",
    "text": "package myapps.model;\n\nimport com.fasterxml.jackson.annotation.JsonFormat;\nimport lombok.AllArgsConstructor;\nimport lombok.Builder;\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\n\nimport java.math.BigDecimal;\nimport java.util.Date;\n\n@Data\n@NoArgsConstructor\n@AllArgsConstructor\n@Builder\npublic class BankTransaction {\n\n    private Long id;\n    private Long balanceId;\n    private BigDecimal amount;\n    @JsonFormat(shape = JsonFormat.Shape.STRING,\n                pattern = \"dd-MM-yyyy hh:mm:ss\")\n    public Date time;\n    @Builder.Default\n    public BankTransactionState state = BankTransactionState.CREATED;\n\n    public static enum BankTransactionState {\n        CREATED, APPROVED, REJECTED\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-transaction/index.html#jsonserde.java",
    "href": "posts/kafka-stream-transaction/index.html#jsonserde.java",
    "title": "Bank Transactions using Kafka Streams",
    "section": "",
    "text": "package myapps.model;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport lombok.SneakyThrows;\nimport org.apache.kafka.common.serialization.Deserializer;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.serialization.Serializer;\n\npublic class JsonSerde&lt;T&gt; implements Serde&lt;T&gt; {\n\n    public static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();\n    private final Class&lt;T&gt; type;\n\n    public JsonSerde(Class&lt;T&gt; type) {\n        this.type = type;\n    }\n\n    @Override\n    public Serializer&lt;T&gt; serializer() {\n        return (topic, data) -&gt; serialize(data);\n    }\n\n    @SneakyThrows\n    private byte[] serialize(T data) {\n        return OBJECT_MAPPER.writeValueAsBytes(data);\n    }\n\n    @Override\n    public Deserializer&lt;T&gt; deserializer() {\n        return (topic, bytes) -&gt; deserialize(bytes);\n    }\n\n    @SneakyThrows\n    private T deserialize(byte[] bytes) {\n        return OBJECT_MAPPER.readValue(bytes, type);\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-transaction/index.html#bankbalancetopology.java",
    "href": "posts/kafka-stream-transaction/index.html#bankbalancetopology.java",
    "title": "Bank Transactions using Kafka Streams",
    "section": "",
    "text": "package myapps.topology;\n\nimport myapps.model.BankBalance;\nimport myapps.model.BankTransaction;\nimport myapps.model.JsonSerde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.kstream.Produced;\n\npublic class BankBalanceTopology {\n\n    public static final String BANK_TRANSACTIONS = \"bank-transactions\";\n    public static final String BANK_BALANCES = \"bank-balances\";\n    public static final String REJECTED_TRANSACTIONS = \"rejected-transactions\";\n\n    public static Topology buildTopology() {\n        StreamsBuilder streamsBuilder = new StreamsBuilder();\n        var bankTransactionJsonSerde = new JsonSerde&lt;BankTransaction&gt;(BankTransaction.class);\n        var bankBalanceJsonSerde = new JsonSerde&lt;&gt;(BankBalance.class);\n\n        var bankBalanceKStream = streamsBuilder.stream(BANK_TRANSACTIONS,\n                        Consumed.with(Serdes.Long(), bankTransactionJsonSerde))\n                .groupByKey()\n                .aggregate(BankBalance::new,\n                        (key, value, aggregate) -&gt; aggregate.process(value),\n                        Materialized.with(Serdes.Long(), bankBalanceJsonSerde))\n                .toStream();\n\n        bankBalanceKStream.to(BANK_BALANCES,\n                Produced.with(Serdes.Long(), bankBalanceJsonSerde));\n\n        bankBalanceKStream.mapValues((readOnlyKey, value) -&gt; value.getLatestTransaction())\n                .filter((key, value) -&gt; value.getState().equals(BankTransaction.BankTransactionState.REJECTED))\n                .to(REJECTED_TRANSACTIONS, Produced.with(Serdes.Long(), bankTransactionJsonSerde));\n\n        return streamsBuilder.build();\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-transaction/index.html#bankbalanceapp.java",
    "href": "posts/kafka-stream-transaction/index.html#bankbalanceapp.java",
    "title": "Bank Transactions using Kafka Streams",
    "section": "",
    "text": "package myapps;\n\nimport myapps.config.StreamConfiguration;\nimport myapps.topology.BankBalanceTopology;\nimport org.apache.kafka.streams.KafkaStreams;\n\npublic class BankBalanceApp {\n\n    public static void main(String[] args) {\n        var configuration = StreamConfiguration.getConfiguration();\n        var topology = BankBalanceTopology.buildTopology();\n        var kafkaStreams = new KafkaStreams(topology, configuration);\n\n        kafkaStreams.start();\n\n        Runtime.getRuntime().addShutdownHook(new Thread(kafkaStreams::close));\n    }\n}"
  },
  {
    "objectID": "posts/kafka-stream-transaction/index.html#pom.xml",
    "href": "posts/kafka-stream-transaction/index.html#pom.xml",
    "title": "Bank Transactions using Kafka Streams",
    "section": "",
    "text": "&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;myapps&lt;/groupId&gt;\n    &lt;artifactId&gt;bank-transactions&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n\n    &lt;dependencies&gt;\n        &lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-streams --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n            &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;\n            &lt;version&gt;2.8.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;\n            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;\n            &lt;version&gt;1.7.30&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;\n            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;\n            &lt;version&gt;1.7.30&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;\n            &lt;artifactId&gt;lombok&lt;/artifactId&gt;\n            &lt;version&gt;RELEASE&lt;/version&gt;\n            &lt;scope&gt;compile&lt;/scope&gt;\n        &lt;/dependency&gt;\n\n        &lt;!-- TEST --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;\n            &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt;\n            &lt;version&gt;RELEASE&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n            &lt;artifactId&gt;kafka-streams-test-utils&lt;/artifactId&gt;\n            &lt;version&gt;2.8.0&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.mockito&lt;/groupId&gt;\n            &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;\n            &lt;version&gt;3.11.2&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.mockito&lt;/groupId&gt;\n            &lt;artifactId&gt;mockito-junit-jupiter&lt;/artifactId&gt;\n            &lt;version&gt;2.23.0&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n\n    &lt;/dependencies&gt;\n\n\n    &lt;properties&gt;\n        &lt;maven.compiler.source&gt;15&lt;/maven.compiler.source&gt;\n        &lt;maven.compiler.target&gt;15&lt;/maven.compiler.target&gt;\n    &lt;/properties&gt;\n&lt;/project&gt;"
  }
]