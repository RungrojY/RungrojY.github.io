[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "ðŸ™‚ðŸ‘‰Click here to download the PDF"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Runroj Yadbantungâ€™s memos",
    "section": "",
    "text": "Using Hadoop\n\n\n\n\n\n\n\nbash\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nSetup Hadoop Cluster in Ubuntu Multipass\n\n\n\n\n\n\n\nbash\n\n\nxml\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nUsing Redux in Next.js\n\n\n\n\n\n\n\ntypescript\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nCreate an API of Neo4j Database in Next.js\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nUsing Cypher in Python\n\n\n\n\n\n\n\ncypher\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nCypher Query\n\n\n\n\n\n\n\ncypher\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nMy TypeScript Journey\n\n\n\n\n\n\n\ntypescript\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nUsing Multipass\n\n\n\n\n\n\n\nbash\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\n  \n\n\n\n\nUsing rsync in Linux\n\n\n\n\n\n\n\nbash\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nRungroj Yadbantung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/neo4j-in-nextjs/index.html",
    "href": "posts/neo4j-in-nextjs/index.html",
    "title": "Connect to Neo4j Database in Node.js",
    "section": "",
    "text": "const neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"à¸ªà¸¡à¸­\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/neo4j-in-nextjs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "href": "posts/neo4j-in-nextjs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "title": "Connect to Neo4j Database in Node.js",
    "section": "2 Firstly, create simple API in pages/hello.ts",
    "text": "2 Firstly, create simple API in pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from 'next';\n\nexport default function handler(req: NextApiRequest, res: NextApiResponse) {\n\n  res.json({\n    message: 'Hello World!',\n  });\n}"
  },
  {
    "objectID": "posts/neo4j-in-nextjs/index.html#create-a-database-connection-pageshello.ts",
    "href": "posts/neo4j-in-nextjs/index.html#create-a-database-connection-pageshello.ts",
    "title": "Connect to Neo4j Database in Node.js",
    "section": "3 Create a database connection pages/hello.ts",
    "text": "3 Create a database connection pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from \"next\";\nimport neo4j, { Driver, Session, Result, Record } from \"neo4j-driver\";\nimport { log } from \"console\";\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\n// Define the type for properties\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\n// API route handler\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  const session: Session = driver.session();\n\n  try {\n    // Perform your Neo4j queries and logic here\n    const result = await session.run(\"MATCH (n) RETURN n LIMIT 5\");\n    \n    // const nodes: Record&lt;string, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n    const nodes: Dict&lt;PropertyKey, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n\n    console.log(nodes);\n\n    const jsonData = JSON.stringify(nodes);\n\n    res.status(200).json({ data: jsonData });\n\n  } catch (error) {\n    console.error(\"Error executing Neo4j query:\", error);\n    res.status(500).json({ error: \"Internal Server Error\" });\n  } finally {\n    session.close();\n  }\n}"
  },
  {
    "objectID": "posts/neo4j-in-nextjs/index.html#firstly-create-simple-api-in-node.js",
    "href": "posts/neo4j-in-nextjs/index.html#firstly-create-simple-api-in-node.js",
    "title": "Connect to Neo4j Database in Node.js",
    "section": "",
    "text": "const neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"à¸ªà¸¡à¸­\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html",
    "href": "posts/typescript_basic_usage/index.html",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User = {name: \"John\", age: \"30\", sex: \"male\"}\n\n\n\nconst data = {name: \"John\", age:30}\ntype User = typeof data;\n\nconst data2:User = {name: \"John\", age:30}\nconst data3:typeof data = {name: \"John\", age:30}\n\n\n\ntype User = {\n  name: string;\n  age: number | string;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User = {name: \"John\", age: \"30\"}\n\n\n\ntype User = {\n  name: string | undefined;\n  age?: number | string;\n};\n\nconst data1:User = {name: \"John\"}\nconst data2:User = {name: undefined}\nconst data3:User = {age: 1} // Type error, missing `name`\n\n\n\ntype User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User[\"name\"] = \"John\"\n\n\n\ntype User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\";\n};\n\ntype User2 = Omit&lt;User, \"sex\"&gt;;\n\n// Usage example:\nconst user: User2 = {\n  name: \"John\",\n  age: 25,\n};\n\n\n\ntype User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:Pick&lt;User, \"name\"&gt; = {name: \"John\"}\nconst data:Pick&lt;User, \"name\" | \"age\"&gt; = {name: \"John\", age: 30}\n\n\n\ntype User = {\n  name: string;\n  age: number;\n}\n\nconst data1: User = {age: 22, name: \"Jenny\"}\ndata1.age = 23\n\nconst data2: Readonly&lt;User&gt; = {age: 22, name: \"Jenny\"}\ndata2.age = 23 // type error\n\n\n\ntype Prop = PropertyKey // string | number | symbol\n\nconst data:Prop = \"John\"\nconst data1:Prop = 21\nconst data2:Prop = false // type error"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#define-type",
    "href": "posts/typescript_basic_usage/index.html#define-type",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User = {name: \"John\", age: \"30\", sex: \"male\"}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#define-type-1",
    "href": "posts/typescript_basic_usage/index.html#define-type-1",
    "title": "My TypeScript Journey",
    "section": "2.2 Define type",
    "text": "2.2 Define type\ntype Props&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\ntype Props2 = Props&lt;number, string&gt;\n// {\n//   [x: number]: string;\n// }\n\nconst data:Props2 = {\n  0: \"apple\",\n  1: \"orange\"\n}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#define-type-2",
    "href": "posts/typescript_basic_usage/index.html#define-type-2",
    "title": "TypeScript Journey",
    "section": "2.1 Define type",
    "text": "2.1 Define type\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};"
  },
  {
    "objectID": "posts/cypher-import-export/index.html",
    "href": "posts/cypher-import-export/index.html",
    "title": "Create Cypher for JSON",
    "section": "",
    "text": "match (n) return n\nSELECT * from mytable where id = 1\nSELECT * from mytable where id = 2\n\n\nterminal\n\nsudo ufw status\nsudo ufw reload\n\n\n\nmatplotlib.py\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n# highlight-style: arrow\nimport matplotlib.pyplot as plt\nprint(\"Hello\")\n```{python}\n1 + 1\n```\n\nlibrary(ggplot2)\ndat &lt;- data.frame(cond = rep(c(\"A\", \"B\"), each=10),\n                  xvar = 1:20 + rnorm(20,sd=3),\n                  yvar = 1:20 + rnorm(20,sd=3))\n\nggplot(dat, aes(x=xvar, y=yvar)) +\n  geom_point(shape=1) + \n  geom_smooth() \n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\nFigureÂ 1: Temperature and ozone level."
  },
  {
    "objectID": "posts/cypher-import-export/index.html#firstly-create-simple-api-in-node.js",
    "href": "posts/cypher-import-export/index.html#firstly-create-simple-api-in-node.js",
    "title": "Create Cypher for JSON",
    "section": "",
    "text": "match (n) return n\nSELECT * from mytable where id = 1\nSELECT * from mytable where id = 2\n\n\nterminal\n\nsudo ufw status\nsudo ufw reload\n\n\n\nmatplotlib.py\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n# highlight-style: arrow\nimport matplotlib.pyplot as plt\nprint(\"Hello\")\n```{python}\n1 + 1\n```\n\nlibrary(ggplot2)\ndat &lt;- data.frame(cond = rep(c(\"A\", \"B\"), each=10),\n                  xvar = 1:20 + rnorm(20,sd=3),\n                  yvar = 1:20 + rnorm(20,sd=3))\n\nggplot(dat, aes(x=xvar, y=yvar)) +\n  geom_point(shape=1) + \n  geom_smooth() \n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\nFigureÂ 1: Temperature and ozone level."
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#get-specific-type",
    "href": "posts/typescript_basic_usage/index.html#get-specific-type",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User[\"name\"] = \"John\""
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#get-specific-type-1",
    "href": "posts/typescript_basic_usage/index.html#get-specific-type-1",
    "title": "TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:Pick&lt;User, \"name\"&gt; = {name: \"John\"}\nconst data:Pick&lt;User, \"name\" | \"age\"&gt; = {name: \"John\", age: 30}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#limit-keys-object-with-pick",
    "href": "posts/typescript_basic_usage/index.html#limit-keys-object-with-pick",
    "title": "TypeScript Journey",
    "section": "2.2 Limit keys object with Pick",
    "text": "2.2 Limit keys object with Pick\nconst person = {\n  name: \"John\",\n  age: 31,\n  sex: \"male\",\n};\n\ndeclare function pick&lt;T, K extends keyof T&gt; (obj: T, ...keys: K[]): Pick&lt;T, K&gt;;\n\nconst data = pick(person, \"name\", \"age\");\n// data can be only `data.name` or `data.age`"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#get-type-from-data",
    "href": "posts/typescript_basic_usage/index.html#get-type-from-data",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "const data = {name: \"John\", age:30}\ntype User = typeof data;\n\nconst data2:User = {name: \"John\", age:30}\nconst data3:typeof data = {name: \"John\", age:30}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#add-more-choices",
    "href": "posts/typescript_basic_usage/index.html#add-more-choices",
    "title": "TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number | string;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User = {name: \"John\", age: \"30\"}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#optional-type",
    "href": "posts/typescript_basic_usage/index.html#optional-type",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string | undefined;\n  age?: number | string;\n};\n\nconst data1:User = {name: \"John\"}\nconst data2:User = {name: undefined}\nconst data3:User = {age: 1} // Type error, missing `name`"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#read-only-type",
    "href": "posts/typescript_basic_usage/index.html#read-only-type",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n}\n\nconst data1: User = {age: 22, name: \"Jenny\"}\ndata1.age = 23\n\nconst data2: Readonly&lt;User&gt; = {age: 22, name: \"Jenny\"}\ndata2.age = 23 // type error"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#limit-key-access-with-pick",
    "href": "posts/typescript_basic_usage/index.html#limit-key-access-with-pick",
    "title": "My TypeScript Journey",
    "section": "2.3 Limit key access with Pick",
    "text": "2.3 Limit key access with Pick\nconst person = {\n  name: \"John\",\n  age: 31,\n  sex: \"male\",\n};\n\ndeclare function pick&lt;T, K extends keyof T&gt; (obj: T, ...keys: K[]): Pick&lt;T, K&gt;;\n\nconst data = pick(person, \"name\", \"age\");\n// data can be only `data.name` or `data.age`"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#type-on-function",
    "href": "posts/typescript_basic_usage/index.html#type-on-function",
    "title": "TypeScript Journey",
    "section": "2.4 Type on function",
    "text": "2.4 Type on function"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#add-more-choice-on-type",
    "href": "posts/typescript_basic_usage/index.html#add-more-choice-on-type",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number | string;\n  sex: \"male\" | \"female\"\n};\n\nconst data:User = {name: \"John\", age: \"30\"}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#exclude-some-types",
    "href": "posts/typescript_basic_usage/index.html#exclude-some-types",
    "title": "TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:Pick&lt;User, \"name\"&gt; = {name: \"John\"}\nconst data:Pick&lt;User, \"name\" | \"age\"&gt; = {name: \"John\", age: 30}"
  },
  {
    "objectID": "posts/cypher-query/index.html",
    "href": "posts/cypher-query/index.html",
    "title": "Cypher Query",
    "section": "",
    "text": "CALL db.schema.visualization()\n\n\n\nMATCH (n) RETURN n\n\n\n\nMATCH (n) \nWHERE NOT (n)-[]-()\nRETURN n\n\n\n\nMATCH (n)-[:HAS_INFO]-&gt;(m)-[:CITE]-&gt;(p) RETURN n, m, p\n\n\n\nMATCH (n:Herb:Drug) RETURN n\n\n\n\nMATCH (n)\nWHERE ID(n) = 1\nRETURN n\n\n\n\nMATCH (n:`Country`)-[r]-(m)\nWHERE n.text CONTAINS 'Thai'\nRETURN n, r, m\n\n\n\nMATCH (n:`Geographic Distribution`) RETURN n\n\n\n\nMATCH (n) RETURN COUNT(n) as count\n\n\n\nMATCH (n) DETACH DELETE n\n\n\n\nMATCH (n)\nREMOVE n.propertyName\n\n\n\nMATCH (n:Person)\nSET n.dummy = 0\n\n\n\nCREATE (:User {name: \"Charlie\", age: 35, gender: \"male\"});\nCREATE (:User {name: \"Dave\", age: 28, gender: \"male\"});\n\n\n\nMATCH (charlie:User {name: \"Charlie\"}), (dave:User {name: \"Dave\"})\nCREATE (charlie)&lt;-[:FRIENDS_WITH {date: \"Jan 19\"}]-(dave);"
  },
  {
    "objectID": "posts/cypher-query/index.html#firstly-create-simple-api-in-node.js",
    "href": "posts/cypher-query/index.html#firstly-create-simple-api-in-node.js",
    "title": "Cypher Query",
    "section": "",
    "text": "match (n) return n"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#property-key",
    "href": "posts/typescript_basic_usage/index.html#property-key",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type Prop = PropertyKey // string | number | symbol\n\nconst data:Prop = \"John\"\nconst data1:Prop = 21\nconst data2:Prop = false // type error"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#type-like-a-funtion",
    "href": "posts/typescript_basic_usage/index.html#type-like-a-funtion",
    "title": "My TypeScript Journey",
    "section": "2.1 Type like a funtion",
    "text": "2.1 Type like a funtion\ntype User = {\n  name: string;\n  age: number;\n  vegetarian: boolean;\n} \n\ntype Prop&lt;T, K extends keyof T&gt; = T[K]\n\ntype AA = Prop&lt;User, \"name\"&gt;\n//string\n\ntype BB = Prop&lt;User, \"name\" | \"age\"&gt;\n//string | number\n\ntype CC = Prop&lt;User, keyof User&gt;\n//string | number | boolean"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#promise-and-awaited-type",
    "href": "posts/typescript_basic_usage/index.html#promise-and-awaited-type",
    "title": "My TypeScript Journey",
    "section": "2.4 Promise and Awaited type",
    "text": "2.4 Promise and Awaited type\ntype NonPromiseType = number;\ntype PromiseType = Promise&lt;NonPromiseType&gt;;\ntype PPP = Awaited&lt;PromiseType&gt;"
  },
  {
    "objectID": "posts/cypher-query/index.html#get-started",
    "href": "posts/cypher-query/index.html#get-started",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) RETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#get-started-1",
    "href": "posts/cypher-query/index.html#get-started-1",
    "title": "Cypher Query",
    "section": "",
    "text": "match (n) return n"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-node-without-relationship",
    "href": "posts/cypher-query/index.html#match-node-without-relationship",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) \nWHERE NOT (n)-[]-()\nRETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#delete-all-nodes-and-edges",
    "href": "posts/cypher-query/index.html#delete-all-nodes-and-edges",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) DETACH DELETE n"
  },
  {
    "objectID": "posts/neo4j-in-nodejs/index.html",
    "href": "posts/neo4j-in-nodejs/index.html",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "",
    "text": "Terminal&gt;node\n\nconst neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"Ginger\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/neo4j-in-nodejs/index.html#firstly-create-simple-api-in-node.js",
    "href": "posts/neo4j-in-nodejs/index.html#firstly-create-simple-api-in-node.js",
    "title": "Connect to Neo4j Database in Node.js",
    "section": "",
    "text": "const neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"à¸ªà¸¡à¸­\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/neo4j-in-nodejs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "href": "posts/neo4j-in-nodejs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "2 Firstly, create simple API in pages/hello.ts",
    "text": "2 Firstly, create simple API in pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from 'next';\n\nexport default function handler(req: NextApiRequest, res: NextApiResponse) {\n\n  res.json({\n    message: 'Hello World!',\n  });\n}"
  },
  {
    "objectID": "posts/neo4j-in-nodejs/index.html#create-a-database-connection-pageshello.ts",
    "href": "posts/neo4j-in-nodejs/index.html#create-a-database-connection-pageshello.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "3 Create a database connection pages/hello.ts",
    "text": "3 Create a database connection pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from \"next\";\nimport neo4j, { Driver, Session, Result, Record } from \"neo4j-driver\";\nimport { log } from \"console\";\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\n// Define the type for properties\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\n// API route handler\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  const session: Session = driver.session();\n\n  try {\n    // Perform your Neo4j queries and logic here\n    const result = await session.run(\"MATCH (n) RETURN n LIMIT 5\");\n    \n    // const nodes: Record&lt;string, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n    const nodes: Dict&lt;PropertyKey, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n\n    console.log(nodes);\n\n    const jsonData = JSON.stringify(nodes);\n\n    res.status(200).json({ data: jsonData });\n\n  } catch (error) {\n    console.error(\"Error executing Neo4j query:\", error);\n    res.status(500).json({ error: \"Internal Server Error\" });\n  } finally {\n    session.close();\n  }\n}"
  },
  {
    "objectID": "posts/cypher-query/index.html#count-all-node",
    "href": "posts/cypher-query/index.html#count-all-node",
    "title": "Cypher Query",
    "section": "",
    "text": "match (n) return count(n) as count"
  },
  {
    "objectID": "posts/cypher-query/index.html#remove-a-property-from-all-nodes",
    "href": "posts/cypher-query/index.html#remove-a-property-from-all-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n)\nREMOVE n.propertyName"
  },
  {
    "objectID": "posts/cypher-query/index.html#remove-a-property-from-all-nodes-1",
    "href": "posts/cypher-query/index.html#remove-a-property-from-all-nodes-1",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n)\nWHERE ID(n) = 1\nRETURN n"
  },
  {
    "objectID": "posts/neo4j-in-nodejs/index.html#test-a-connection-in-nodejs",
    "href": "posts/neo4j-in-nodejs/index.html#test-a-connection-in-nodejs",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "",
    "text": "Terminal&gt;node\n\nconst neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"Ginger\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-all-nodes",
    "href": "posts/cypher-query/index.html#match-all-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) RETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-nodes-without-relationship",
    "href": "posts/cypher-query/index.html#match-nodes-without-relationship",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) \nWHERE NOT (n)-[]-()\nRETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#count-all-nodes",
    "href": "posts/cypher-query/index.html#count-all-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n) RETURN COUNT(n) as count"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-node-with-an-id",
    "href": "posts/cypher-query/index.html#match-node-with-an-id",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n)\nWHERE ID(n) = 1\nRETURN n"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html",
    "href": "posts/neo4j-nextjs/index.html",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "",
    "text": "Terminal&gt;node\n\nconst neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"Ginger\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html#test-a-connection-in-nodejs",
    "href": "posts/neo4j-nextjs/index.html#test-a-connection-in-nodejs",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "",
    "text": "Terminal&gt;node\n\nconst neo4j = require('neo4j-driver');\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\nlet query = `\n  MATCH (s:Herb)-[v:HAS_COMPOUND]-(o)\n  WHERE toLower(s.name) CONTAINS toLower(\"Ginger\")\n  return s,v,o\n  limit 5\n  `;\n\nconst session = driver.session();\n\nlet result = await session.run(query);\n\n\nresult.records[0].get('s').properties\nresult.records[0].get('s').elementId\nresult.records[0].get('s').labels\n\nresult.records[0].get('v').properties\nresult.records[0].get('v').type\nresult.records[0].get('v').startNodeElementId\nresult.records[0].get('v').endNodeElementId\n\nresult.records[0].get('o').properties\nresult.records[0].get('o').labels\nresult.records[0].get('o').elementId"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "href": "posts/neo4j-nextjs/index.html#firstly-create-simple-api-in-pageshello.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "2 Firstly, create simple API in pages/hello.ts",
    "text": "2 Firstly, create simple API in pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from 'next';\n\nexport default function handler(req: NextApiRequest, res: NextApiResponse) {\n\n  res.json({\n    message: 'Hello World!',\n  });\n}"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html#create-a-database-connection-pageshello.ts",
    "href": "posts/neo4j-nextjs/index.html#create-a-database-connection-pageshello.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "3 Create a database connection pages/hello.ts",
    "text": "3 Create a database connection pages/hello.ts\nimport { NextApiRequest, NextApiResponse } from \"next\";\nimport neo4j, { Driver, Session, Result, Record } from \"neo4j-driver\";\nimport { log } from \"console\";\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\n// Define the type for properties\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\n// API route handler\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  const session: Session = driver.session();\n\n  try {\n    // Perform your Neo4j queries and logic here\n    const result = await session.run(\"MATCH (n) RETURN n LIMIT 5\");\n    \n    // const nodes: Record&lt;string, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n    const nodes: Dict&lt;PropertyKey, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n\n    console.log(nodes);\n\n    const jsonData = JSON.stringify(nodes);\n\n    res.status(200).json({ data: jsonData });\n\n  } catch (error) {\n    console.error(\"Error executing Neo4j query:\", error);\n    res.status(500).json({ error: \"Internal Server Error\" });\n  } finally {\n    session.close();\n  }\n}"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#assert-type-on-unknow-type",
    "href": "posts/typescript_basic_usage/index.html#assert-type-on-unknow-type",
    "title": "My TypeScript Journey",
    "section": "3.1 Assert type on unknow type",
    "text": "3.1 Assert type on unknow type\nWe use a type assertion (error as Error).message to assert that the error object has the type Error, which guarantees the presence of the message property. This allows TypeScript to recognize and accept the error.message access without any type error.\ntry {\n    const result = await dbManager.runQuery(query);\n    res.status(200).json(result.records);\n  } catch (error) {\n    // res.status(500).json({ error: error.message }); //type error\n    res.status(500).json({ error: (error as Error).message });\n  }"
  },
  {
    "objectID": "posts/using-python-environment/index.html",
    "href": "posts/using-python-environment/index.html",
    "title": "Using Python Environment",
    "section": "",
    "text": "Virtual environments help avoid conflicts with other Python packages.\n\n\npython -m venv keras-env\n\n\n\n.\\keras-env\\Scripts\\activate"
  },
  {
    "objectID": "posts/using-python-environment/index.html#set-up-a-virtual-environment",
    "href": "posts/using-python-environment/index.html#set-up-a-virtual-environment",
    "title": "Using Python Environment",
    "section": "",
    "text": "python -m venv keras-env"
  },
  {
    "objectID": "posts/using-python-environment/index.html#activate-the-virtual-environment-on-windows",
    "href": "posts/using-python-environment/index.html#activate-the-virtual-environment-on-windows",
    "title": "Using Python Environment",
    "section": "",
    "text": ".\\keras-env\\Scripts\\activate"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#nest-of-try-catch-block-to-resolve-error",
    "href": "posts/typescript_basic_usage/index.html#nest-of-try-catch-block-to-resolve-error",
    "title": "TypeScript Journey",
    "section": "3.1 Nest of try-catch block to resolve error",
    "text": "3.1 Nest of try-catch block to resolve error\ntry {\n    try {\n    \n  } catch (error) {\n    \n  }\n  }"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#using-symbol-in-object",
    "href": "posts/typescript_basic_usage/index.html#using-symbol-in-object",
    "title": "My TypeScript Journey",
    "section": "3.2 Using Symbol in Object",
    "text": "3.2 Using Symbol in Object\nChange the type constraint in the Dict definition to only allow string keys:\nexport type Dict&lt;T extends string, U&gt; = {\n  [K in T]: U;\n};\n\nconst user: Dict&lt;\"name\" | \"email\", string&gt; = {\n  name: \"John\",\n  email: \"john@gmail.com\",\n};"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#using-symbol-in-object-by-defining",
    "href": "posts/typescript_basic_usage/index.html#using-symbol-in-object-by-defining",
    "title": "My TypeScript Journey",
    "section": "3.3 Using Symbol in Object by defining",
    "text": "3.3 Using Symbol in Object by defining\nKeep the Dict type definition as it is, but use symbols as keys in the user object:\nexport type Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\nconst nameKey = Symbol(\"name\");\nconst emailKey = Symbol(\"email\");\n\nconst user: Dict&lt;symbol, string&gt; = {\n  [nameKey]: \"John\",\n  [emailKey]: \"john@gmail.com\",\n};"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#exclude-with-omit",
    "href": "posts/typescript_basic_usage/index.html#exclude-with-omit",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\";\n};\n\ntype User2 = Omit&lt;User, \"sex\"&gt;;\n\n// Usage example:\nconst user: User2 = {\n  name: \"John\",\n  age: 25,\n};"
  },
  {
    "objectID": "posts/typescript_basic_usage/index.html#exclude-with-pick",
    "href": "posts/typescript_basic_usage/index.html#exclude-with-pick",
    "title": "My TypeScript Journey",
    "section": "",
    "text": "type User = {\n  name: string;\n  age: number;\n  sex: \"male\" | \"female\"\n};\n\nconst data:Pick&lt;User, \"name\"&gt; = {name: \"John\"}\nconst data:Pick&lt;User, \"name\" | \"age\"&gt; = {name: \"John\", age: 30}"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html",
    "href": "posts/setup-hadoop-cluster/index.html",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Create Ubuntu Server image with yaml file.\n\n\n\nhostname\nrole\n\n\n\n\nnode0\nnamenode\n\n\nnode1\ndatanode\n\n\nnode2\ndatanode\n\n\n\n\n\nsetup-ubuntu.yaml\n\npackage_update: true\npackage_upgrade: true\npackages:\n  - openjdk-8-jdk\n  - ssh\n\n\n\nWindow Terminal\n\n# the default resources are 1 CPU cores, ~1 GB memory and ~5 GB disk space.\nmultipass launch --name node0 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml focal\n# or add more resources with the command\nmultipass launch -n node0 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml -c 1 -d 10G -m 4G focal\nmultipass launch -n node1 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml -c 1 -d 10G -m 2G focal\nmultipass launch -n node2 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml -c 1 -d 10G -m 2G focal\n\nThe word focal refers to Ubuntu 20.04 LTS. The IP addresses are obtain as\n\n\n\nhostname\nrole\nIP address\n\n\n\n\nnode0\nnamenode\n192.168.1.127\n\n\nnode1\ndatanode\n192.168.1.128\n\n\nnode2\ndatanode\n192.168.1.129\n\n\n\n\n\n\nGenerate ssh key pairs for each machine.\nssh-keygen\n# or\nssh-keygen -t rsa\ncd .ssh\nvi authorized_keys\nThe original public key should also be added for multipass shell command.\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHf4pMDm06FEWOXMmveyNTJw75+iVM30sz6iYQHLmD8DSGrrkfiKHsXxVGRJxB2A+V72AaoJFJ6CwSi4TVManNLHt8v1ropos45ST0JTTwrjlK6hc4Z68P5l0M4C6sZIEIYECWoA1VTuUzmH4d0szyNDCM+2qkwnWzYnfxbiESkExqgyNnfXw+YlvvPEioyNL8mxq4iPJCJ8HnPKVaAgQFwqJ4c4Zu6P4FZlFsuJHHkdcDdoDRSorqFr7LK7Ye0dfSrqcF39GQBvIZkcLhg1IUxLcPaaDvNXuU0r2N7j63/elVXpmYGolDgyz7HS0Ymr1fdr+bD3lO0fj+RMx9ANFn ubuntu@localhost\nThus, we need to add these public keys for each machine.\nssh-rsa AAAAB...NFn ubuntu@localhost\nssh-rsa AAAAB...Hns= ubuntu@node0\nssh-rsa AAAAB...1tE= ubuntu@node1\nssh-rsa AAAAB...AS0= ubuntu@node2\n\n\n\nIn order to call the IP address, we can replace the IP addresses as their short name.\nvi /etc/hosts\n192.168.1.127 node0\n192.168.1.128 node1\n192.168.1.129 node2\n\n\n\nFor executing shell commands in parallel across multiple remote machines, letâ€™s install pdsh and it is required for Hadoop setup\nsudo apt-get install pdsh\n\n# Display output version information with\npdsh -V\nPlease change the type of shell protocol to be ssh, otherwise itâ€™s cause an error for Hadoop.\nvi ~/.bashrc\n\n\n.bashrc\n\nexport PDSH_RCMD_TYPE=ssh\n\nBy testing with the following command, the hostname must be show up.\npdsh -w node[0-2] hostname\nor testing with the text file.\nvi hosts.txt\nSet all IP address of machine nodes\n\n\nhosts.txt\n\n192.168.1.127\n192.168.1.128\n192.168.1.129\n\npdsh -w ^hosts.txt hostname\n\n\n\nLetâ€™s visit released page and download hadoop-3.3.5.tar.gz in Window\n\n\nWindow Terminal\n\nmultipass transfer C:\\Users\\Jorgnur\\Downloads\\hadoop-3.3.5.tar.gz node0:/home/ubuntu/\n\nIn Ubuntu, unpacking the software on all the machines in the cluster\ntar -zxvf ~/Downloads/hadoop-3.3.5.tar.gz \n\n\n\nEdit .bashrc with\nvi ~/.bashrc\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\nexport PATH=$PATH:$JAVA_HOME/bin\nexport HADOOP_HOME=~/hadoop-3.3.5\nexport PATH=$PATH:$HADOOP_HOME/bin\nexport PATH=$PATH:$HADOOP_HOME/sbin\nreload bash file\nsource ~/.bashrc\n# or\n. ~/.bashrc\n\n# check the result with\necho $PATH\n\n\n\nLetâ€™s add JAVA_HOME to flies located in ~/hadoop-3.3.5/etc/hadoop which are\n\n\n\nfilename\n\n\n\n\nmapred-env.sh\n\n\nhadoop-env.sh\n\n\nyarn-env.sh\n\n\n\ncd ~/hadoop-3.3.5/etc/hadoop\n\n# see file list in details\nls -lstr\nls -lstr *env*.sh\nOpen three files with the following commands\nvi ~/hadoop-3.3.5/etc/hadoop/mapred-env.sh\nvi ~/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\nvi ~/hadoop-3.3.5/etc/hadoop/yarn-env.sh\nand add JAVA_HOME below to all files.\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n\n\n\nvi ~/hadoop-3.3.5/etc/hadoop/core-site.xml\n&lt;!-- Set master node --&gt;\n&lt;property&gt;\n  &lt;name&gt;fs.default.name&lt;/name&gt;\n  &lt;value&gt;hdfs://192.168.1.127:50000&lt;/value&gt;\n&lt;/property&gt;\n\n\n\nvi ~/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; \n  &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;description&gt;The hostname of the RM.&lt;/description&gt;\n  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n  &lt;value&gt;192.168.1.127&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;description&gt;The address of the applications manager interface in the RM.&lt;/description&gt;\n  &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;\n  &lt;value&gt;192.168.1.127:8032&lt;/value&gt;\n&lt;/property&gt;\n\n\n\nvi ~/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n  &lt;value&gt;/home/ubuntu/hadoop-data/namenode-dir&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n  &lt;value&gt;/home/ubuntu/hadoop-data/datanode-dir&lt;/value&gt;\n&lt;/property&gt;\n\n\n\nvi ~/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n&lt;property&gt;\n  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n  &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;\n\n\n\nvi ~/hadoop-3.3.5/etc/hadoop/workers\n192.168.1.127\n192.168.1.128\n192.168.1.129\n\n\n\nLetâ€™ s initializes the directory structure and file system metadata required by the NameNode to start a fresh HDFS instance.\ncd ~/hadoop-3.3.5\nbin/hdfs namenode -format\n# or (deprecated)\nbin/hadoop namenode -format\n\n\n\n\n\ncd ~/hadoop-3.3.5\nsbin/start-all.sh\n\n\n\ncd ~/hadoop-3.3.5\nsbin/stop-all.sh\n\n\n\njps\n\n\n\n\nLetâ€™s go the browser http://localhost:9870/ of the Namenode, thus it is 192.168.1.120:9870 for in this case.\n\n\n\nvi ~/demo\n\n\ndemo\n\nHello!\n\ncd ~/hadoop-3.3.5\nbin/hadoop dfs -put /home/ubuntu/demo /demo123\n# or\nbin/hdfs dfs -put /home/ubuntu/demo /demo123"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#create-virtual-machine-with-yaml",
    "href": "posts/setup-hadoop-cluster/index.html#create-virtual-machine-with-yaml",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Create Ubuntu Server image with yaml file. The word focal refers to Ubuntu 20.04 LTS.\n\n\n\nhostname\nduty\n\n\n\n\nhadoop0\nnamenode\n\n\nhadoop1\ndatanode\n\n\nhadoop2\ndatanode\n\n\n\npackage_update: true\npackage_upgrade: true\npackages:\n  - openjdk-8-jdk\n  - ssh\n\n\nWindow Terminal\n\nmultipass launch --name hadoop0 --network Wi-Fi --cloud-init myImage.yaml focal"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#install-packages",
    "href": "posts/setup-hadoop-cluster/index.html#install-packages",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "sudo apt-get install ssh\n\n# Only for master node\nsudo apt-get install pdsh"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#create-virtual-machine",
    "href": "posts/setup-hadoop-cluster/index.html#create-virtual-machine",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Create Ubuntu Server image with yaml file.\n\n\n\nhostname\nrole\n\n\n\n\nnode0\nnamenode\n\n\nnode1\ndatanode\n\n\nnode2\ndatanode\n\n\n\n\n\nsetup-ubuntu.yaml\n\npackage_update: true\npackage_upgrade: true\npackages:\n  - openjdk-8-jdk\n  - ssh\n\n\n\nWindow Terminal\n\n# the default resources are 1 CPU cores, ~1 GB memory and ~5 GB disk space.\nmultipass launch --name node0 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml focal\n# or add more resources with the command\nmultipass launch -n node0 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml -c 1 -d 10G -m 4G focal\nmultipass launch -n node1 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml -c 1 -d 10G -m 2G focal\nmultipass launch -n node2 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml -c 1 -d 10G -m 2G focal\n\nThe word focal refers to Ubuntu 20.04 LTS. The IP addresses are obtain as\n\n\n\nhostname\nrole\nIP address\n\n\n\n\nnode0\nnamenode\n192.168.1.127\n\n\nnode1\ndatanode\n192.168.1.128\n\n\nnode2\ndatanode\n192.168.1.129"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#install-pdsh-parallel-distributed-shell-in-hadoop0",
    "href": "posts/setup-hadoop-cluster/index.html#install-pdsh-parallel-distributed-shell-in-hadoop0",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "For executing shell commands in parallel across multiple remote machines, letâ€™s install pdsh and it is required for Hadoop setup\nsudo apt-get install pdsh\n\n# Display output version information with\npdsh -V\nPlease change the type of shell protocol to be ssh, otherwise itâ€™s cause an error for Hadoop.\nvi ~/.bashrc\n\n\n.bashrc\n\nexport PDSH_RCMD_TYPE=ssh\n\nBy testing with the following command, the hostname must be show up.\npdsh -w node[0-2] hostname\nor testing with the text file.\nvi hosts.txt\nSet all IP address of machine nodes\n\n\nhosts.txt\n\n192.168.1.120\n192.168.1.121\n192.168.1.123"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#prepare-the-file",
    "href": "posts/setup-hadoop-cluster/index.html#prepare-the-file",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Letâ€™s visit released page and download hadoop-3.3.5.tar.gz in Window\n\n\nWindow Terminal\n\nmultipass transfer C:\\Users\\Jorgnur\\Downloads\\hadoop-3.3.5.tar.gz node0:/home/ubuntu/\n\nIn Ubuntu, unpacking the software on all the machines in the cluster\ntar -zxvf ~/Downloads/hadoop-3.3.5.tar.gz"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#generate-rsa-key-pairs",
    "href": "posts/setup-hadoop-cluster/index.html#generate-rsa-key-pairs",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Generate ssh key pairs for each machine.\nssh-keygen\n# or\nssh-keygen -t rsa\ncd .ssh\nvi authorized_keys\nThe original public key should also be added for multipass shell command.\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHf4pMDm06FEWOXMmveyNTJw75+iVM30sz6iYQHLmD8DSGrrkfiKHsXxVGRJxB2A+V72AaoJFJ6CwSi4TVManNLHt8v1ropos45ST0JTTwrjlK6hc4Z68P5l0M4C6sZIEIYECWoA1VTuUzmH4d0szyNDCM+2qkwnWzYnfxbiESkExqgyNnfXw+YlvvPEioyNL8mxq4iPJCJ8HnPKVaAgQFwqJ4c4Zu6P4FZlFsuJHHkdcDdoDRSorqFr7LK7Ye0dfSrqcF39GQBvIZkcLhg1IUxLcPaaDvNXuU0r2N7j63/elVXpmYGolDgyz7HS0Ymr1fdr+bD3lO0fj+RMx9ANFn ubuntu@localhost\nThus, we need to add these public keys for each machine.\nssh-rsa AAAAB...NFn ubuntu@localhost\nssh-rsa AAAAB...Hns= ubuntu@node0\nssh-rsa AAAAB...1tE= ubuntu@node1\nssh-rsa AAAAB...AS0= ubuntu@node2"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#replace-ip-address-with-its-names",
    "href": "posts/setup-hadoop-cluster/index.html#replace-ip-address-with-its-names",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "In order to call the IP address, we can replace the IP addresses as their short name.\nvi /etc/hosts\n192.168.1.127 node0\n192.168.1.128 node1\n192.168.1.129 node2"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#setup-.bashrc",
    "href": "posts/setup-hadoop-cluster/index.html#setup-.bashrc",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Edit .bashrc with\nvi ~/.bashrc\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\nexport PATH=$PATH:$JAVA_HOME/bin\nexport HADOOP_HOME=~/hadoop-3.3.5\nexport PATH=$PATH:$HADOOP_HOME/bin\nexport PATH=$PATH:$HADOOP_HOME/sbin\nreload bash file\nsource ~/.bashrc\n# or\n. ~/.bashrc\n\n# check the result with\necho $PATH"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#hadoop-configuration",
    "href": "posts/setup-hadoop-cluster/index.html#hadoop-configuration",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Letâ€™s add JAVA_HOME to flies located in ~/hadoop-3.3.5/etc/hadoop which are\n\n\n\nfilename\n\n\n\n\nmapred-env.sh\n\n\nhadoop-env.sh\n\n\nyarn-env.sh\n\n\n\ncd ~/hadoop-3.3.5/etc/hadoop\n\n# see file list in details\nls -lstr\nls -lstr *env*.sh\nOpen three files with the following commands\nvi ~/hadoop-3.3.5/etc/hadoop/mapred-env.sh\nvi ~/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\nvi ~/hadoop-3.3.5/etc/hadoop/yarn-env.sh\nand add JAVA_HOME below to all files.\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configure-core-site.xml",
    "href": "posts/setup-hadoop-cluster/index.html#configure-core-site.xml",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "vi ~/hadoop-3.3.5/etc/hadoop/core-site.xml\n&lt;!-- Set master node --&gt;\n&lt;property&gt;\n  &lt;name&gt;fs.default.name&lt;/name&gt;\n  &lt;value&gt;hdfs://192.168.1.127:50000&lt;/value&gt;\n&lt;/property&gt;"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configure-yarn-site.xml",
    "href": "posts/setup-hadoop-cluster/index.html#configure-yarn-site.xml",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "vi ~/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; \n  &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;description&gt;The hostname of the RM.&lt;/description&gt;\n  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n  &lt;value&gt;192.168.1.127&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;description&gt;The address of the applications manager interface in the RM.&lt;/description&gt;\n  &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;\n  &lt;value&gt;192.168.1.127:8032&lt;/value&gt;\n&lt;/property&gt;"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configure-hdfs-site.xml",
    "href": "posts/setup-hadoop-cluster/index.html#configure-hdfs-site.xml",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "vi ~/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n  &lt;value&gt;/home/ubuntu/hadoop-data/namenode-dir&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n  &lt;value&gt;/home/ubuntu/hadoop-data/datanode-dir&lt;/value&gt;\n&lt;/property&gt;"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configure-mapred-site.xml",
    "href": "posts/setup-hadoop-cluster/index.html#configure-mapred-site.xml",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "vi ~/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n&lt;property&gt;\n  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n  &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#set-workers-ip-address-only-in-master-node",
    "href": "posts/setup-hadoop-cluster/index.html#set-workers-ip-address-only-in-master-node",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "vi ~/hadoop-3.3.5/etc/hadoop/workers\n192.168.1.127\n192.168.1.128\n192.168.1.129"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#format",
    "href": "posts/setup-hadoop-cluster/index.html#format",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Letâ€™s check the error here.\nbin/hadoop namenode -format"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#general-usage-command",
    "href": "posts/setup-hadoop-cluster/index.html#general-usage-command",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "cd ~/hadoop-3.3.5\nsbin/start-all.sh\n\n\n\ncd ~/hadoop-3.3.5\nsbin/stop-all.sh\n\n\n\njps"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#hadoop-ui",
    "href": "posts/setup-hadoop-cluster/index.html#hadoop-ui",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Letâ€™s go the browser http://localhost:9870/ of the Namenode, thus it is 192.168.1.120:9870 for in this case."
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#format-hdfs-namenode",
    "href": "posts/setup-hadoop-cluster/index.html#format-hdfs-namenode",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "Letâ€™ s initializes the directory structure and file system metadata required by the NameNode to start a fresh HDFS instance.\ncd ~/hadoop-3.3.5\nbin/hdfs namenode -format\n# or (deprecated)\nbin/hadoop namenode -format"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#create-a-demo-file",
    "href": "posts/setup-hadoop-cluster/index.html#create-a-demo-file",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "vi ~/demo\n\n\ndemo\n\nHello!\n\ncd ~/hadoop-3.3.5\nbin/hadoop dfs -put /home/ubuntu/demo /demo123\n# or\nbin/hdfs dfs -put /home/ubuntu/demo /demo123"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#troubleshoots",
    "href": "posts/setup-hadoop-cluster/index.html#troubleshoots",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "When running the command\nsbin/start-all.sh \n, I got the message\nStopping namenodes on [master]\npdsh@hadoop0: master: rcmd: socket: Permission denied\nStopping datanodes\npdsh@hadoop0: 192.168.1.22: rcmd: socket: Permission denied\npdsh@hadoop0: 192.168.1.21: rcmd: socket: Permission denied\npdsh@hadoop0: 192.168.1.20: rcmd: socket: Permission denied\nyum -y install pdsh\nyum -y install epel-release\n\npdsh -v\npdsh-2.31 (+debug)\nrcmd modules: ssh,rsh,exec (default: rsh)\nmisc modules: genders\nexport PDSH_RCMD_TYPE=ssh\n{.bash {filename=.bashrc}} export PDSH_RCMD_TYPE=ssh\npdsh -w slave-[1-2] hostname\npdsh -w ^hosts.txt ls"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#troubleshooting",
    "href": "posts/setup-hadoop-cluster/index.html#troubleshooting",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "When running the command\nsbin/start-all.sh \n```,\nI got the message\n\n```bash\nStopping namenodes on [master]\npdsh@hadoop0: master: rcmd: socket: Permission denied\nStopping datanodes\npdsh@hadoop0: 192.168.1.23: rcmd: socket: Permission denied\npdsh@hadoop0: 192.168.1.21: rcmd: socket: Permission denied\npdsh@hadoop0: 192.168.1.20: rcmd: socket: Permission denied"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#namenode-web-interface",
    "href": "posts/setup-hadoop-cluster/index.html#namenode-web-interface",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "2.2 NameNode Web Interface",
    "text": "2.2 NameNode Web Interface\nThe web interface for the NameNode is available at http://localhost:9870/ by default. If the ufw is enable, you should allow add ufw rule on port 9870\nsudo ufw allow 9870\nsudo ufw reload"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#permission-denied",
    "href": "posts/setup-hadoop-cluster/index.html#permission-denied",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "2.1 Permission denied",
    "text": "2.1 Permission denied\nI got the message as follows:\nStopping namenodes on [master]\npdsh@node0: master: rcmd: socket: Permission denied\nStopping datanodes\npdsh@node0: 192.168.1.23: rcmd: socket: Permission denied\npdsh@node0: 192.168.1.21: rcmd: socket: Permission denied\npdsh@node0: 192.168.1.20: rcmd: socket: Permission denied\nPlease check in workers. The IP address was not set correctly.\nvi ~/hadoop-3.3.5/etc/hadoop/workers"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#navigate-ifle",
    "href": "posts/setup-hadoop-cluster/index.html#navigate-ifle",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "2.3 Navigate ifle",
    "text": "2.3 Navigate ifle\nI am currently working on Window system. Extraction with Winrar causeme some errors, thus I use Cygwin Linux Software to extract the file.\ncd /cygdrive/c/users/Jorgnur/Downloads/hadoop-3.3.5.tar.gz\n\ntar -zxvf hadoop-3.3.5.tar.gz"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#install-pdsh-parallel-distributed-shell-in-node0",
    "href": "posts/setup-hadoop-cluster/index.html#install-pdsh-parallel-distributed-shell-in-node0",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "",
    "text": "For executing shell commands in parallel across multiple remote machines, letâ€™s install pdsh and it is required for Hadoop setup\nsudo apt-get install pdsh\n\n# Display output version information with\npdsh -V\nPlease change the type of shell protocol to be ssh, otherwise itâ€™s cause an error for Hadoop.\nvi ~/.bashrc\n\n\n.bashrc\n\nexport PDSH_RCMD_TYPE=ssh\n\nBy testing with the following command, the hostname must be show up.\npdsh -w node[0-2] hostname\nor testing with the text file.\nvi hosts.txt\nSet all IP address of machine nodes\n\n\nhosts.txt\n\n192.168.1.127\n192.168.1.128\n192.168.1.129\n\npdsh -w ^hosts.txt hostname"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#navigate-hadoop-files",
    "href": "posts/setup-hadoop-cluster/index.html#navigate-hadoop-files",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "2.3 Navigate Hadoop Files",
    "text": "2.3 Navigate Hadoop Files\nI am currently working on Window system. Extraction with Winrar causes me some errors, thus I use Cygwin Linux Software to extract the file.\ncd /cygdrive/c/users/Jorgnur/Desktop/hadoop/ver/hadoop-3.3.5.tar.gz\ntar -zxvf hadoop-3.3.5.tar.gz\n# cygwin =&gt; node0\nrsync -avzphi /cygdrive/c/users/Jorgnur/Desktop/hadoop/ver/hadoop-3.3.5.tar.gz ubuntu@192.168.1.109:~/\nssh ubuntu@192.168.1.109\ntar -zxvf hadoop-3.3.5.tar.gz\nrm hadoop-3.3.5.tar.gz\nlogout\nrsync -avzphi /cygdrive/c/users/Jorgnur/Desktop/hadoop/ver/hadoop-3.3.5/etc/hadoop ubuntu@192.168.1.109:~/hadoop-3.3.5/etc/\n\nssh ubuntu@node0\n# node0 =&gt; node1\nrsync -avzphi ~/hadoop-3.3.5.tar.gz ubuntu@192.168.1.110:~/\nssh ubuntu@192.168.1.110\ntar -zxvf hadoop-3.3.5.tar.gz\nrm hadoop-3.3.5.tar.gz\nlogout\nrsync -avzphi ~/hadoop-3.3.5/etc/hadoop ubuntu@192.168.1.110:~/hadoop-3.3.5/etc/\n\n# node0 =&gt; node2\nrsync -avzphi ~/hadoop-3.3.5.tar.gz ubuntu@192.168.1.107:~/\nssh ubuntu@192.168.1.107\ntar -zxvf hadoop-3.3.5.tar.gz\nrm hadoop-3.3.5.tar.gz\nlogout\nrsync -avzphi ~/hadoop-3.3.5/etc/hadoop ubuntu@192.168.1.107:~/hadoop-3.3.5/etc/"
  },
  {
    "objectID": "posts/cypher-query/index.html#display-database-schema",
    "href": "posts/cypher-query/index.html#display-database-schema",
    "title": "Cypher Query",
    "section": "",
    "text": "CALL db.schema.visualization()"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-triple-nodes",
    "href": "posts/cypher-query/index.html#match-triple-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n)-[:HAS_INFO]-&gt;(m)-[:CITE]-&gt;(p) RETURN n, m, p"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-nodes-with-multiple-labels",
    "href": "posts/cypher-query/index.html#match-nodes-with-multiple-labels",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n:Herb:Drug) RETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-node-with-some-text",
    "href": "posts/cypher-query/index.html#match-node-with-some-text",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n:`Country`)-[r]-(m)\nWHERE n.text CONTAINS 'Thai'\nRETURN n, r, m"
  },
  {
    "objectID": "posts/cypher-query/index.html#match-a-label-that-contains-space",
    "href": "posts/cypher-query/index.html#match-a-label-that-contains-space",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n:`Geographic Distribution`) RETURN n"
  },
  {
    "objectID": "posts/cypher-query/index.html#set-a-new-property-to-all-nodes",
    "href": "posts/cypher-query/index.html#set-a-new-property-to-all-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (n:Person)\nSET n.dummy = 0"
  },
  {
    "objectID": "posts/cypher-query/index.html#create-one-node",
    "href": "posts/cypher-query/index.html#create-one-node",
    "title": "Cypher Query",
    "section": "",
    "text": "CREATE (:User {name: \"Charlie\", age: 35, gender: \"male\"});\nCREATE (:User {name: \"Dave\", age: 28, gender: \"male\"});"
  },
  {
    "objectID": "posts/cypher-query/index.html#create-relationship-from-nodes",
    "href": "posts/cypher-query/index.html#create-relationship-from-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (charlie:User {name: \"Charlie\"}), (dave:User {name: \"Dave\"})\nCREATE (charlie)&lt;-[:FRIENDS_WITH {date: \"Jan 19\"}]-(dave);"
  },
  {
    "objectID": "posts/cypher-query/index.html#create-a-relationship-from-nodes",
    "href": "posts/cypher-query/index.html#create-a-relationship-from-nodes",
    "title": "Cypher Query",
    "section": "",
    "text": "MATCH (charlie:User {name: \"Charlie\"}), (dave:User {name: \"Dave\"})\nCREATE (charlie)&lt;-[:FRIENDS_WITH {date: \"Jan 19\"}]-(dave);"
  },
  {
    "objectID": "posts/cypher-python/index.html",
    "href": "posts/cypher-python/index.html",
    "title": "Using Cypher in Python",
    "section": "",
    "text": "pip install py2neo"
  },
  {
    "objectID": "posts/cypher-python/index.html#firstly-create-simple-api-in-node.js",
    "href": "posts/cypher-python/index.html#firstly-create-simple-api-in-node.js",
    "title": "Using Cypher in Python",
    "section": "",
    "text": "match (n) return n\nSELECT * from mytable where id = 1\nSELECT * from mytable where id = 2\n\n\nterminal\n\nsudo ufw status\nsudo ufw reload\n\n\n\nmatplotlib.py\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n`{.python code-line-numbers=\"true\"} # highlight-style: arrow import matplotlib.pyplot as plt print(\"Hello\")"
  },
  {
    "objectID": "posts/cypher-python/index.html#install-a-py2neo-package",
    "href": "posts/cypher-python/index.html#install-a-py2neo-package",
    "title": "Using Cypher in Python",
    "section": "",
    "text": "pip install py2neo"
  },
  {
    "objectID": "posts/cypher-python/index.html#import-python-packages",
    "href": "posts/cypher-python/index.html#import-python-packages",
    "title": "Using Cypher in Python",
    "section": "2 Import Python packages",
    "text": "2 Import Python packages\nimport re #regular expression\nimport numpy as np\nimport pandas as pd\n\nfrom py2neo.bulk import merge_nodes\nfrom py2neo import Node, Graph, Relationship, NodeMatcher"
  },
  {
    "objectID": "posts/cypher-python/index.html#import-python-packages-1",
    "href": "posts/cypher-python/index.html#import-python-packages-1",
    "title": "Using Cypher in Python",
    "section": "3 Import Python packages",
    "text": "3 Import Python packages\nimport re #regular expression\nimport numpy as np\nimport pandas as pd\n\nfrom py2neo.bulk import merge_nodes\nfrom py2neo import Node, Graph, Relationship, NodeMatcher"
  },
  {
    "objectID": "posts/cypher-python/index.html#define-remove-special-characters",
    "href": "posts/cypher-python/index.html#define-remove-special-characters",
    "title": "Using Cypher in Python",
    "section": "3 Define remove special characters",
    "text": "3 Define remove special characters\ndef remove_special_characters(text):\n  regex = re.compile(r'[\\n\\r\\t]')\n  clean_text = regex.sub(\" \", text)  \n  return clean_text.strip()"
  },
  {
    "objectID": "posts/cypher-python/index.html#define-funtion-for-creating-tuple-list-of-data",
    "href": "posts/cypher-python/index.html#define-funtion-for-creating-tuple-list-of-data",
    "title": "Using Cypher in Python",
    "section": "4 Define funtion for creating tuple list of data",
    "text": "4 Define funtion for creating tuple list of data\ndef create_tuple_list(df):\n  # prepare tuple list\n  keys = df.columns.tolist()\n\n  df_selected_columns = pd.DataFrame(df, columns=keys)\n\n  records = df_selected_columns.to_records(index=False)\n  tuples = list(records)\n\n  return tuples"
  },
  {
    "objectID": "posts/cypher-python/index.html#create-connection-to-graph-database",
    "href": "posts/cypher-python/index.html#create-connection-to-graph-database",
    "title": "Using Cypher in Python",
    "section": "5 Create connection to graph database",
    "text": "5 Create connection to graph database\n# Connect database\nuri = 'bolt://localhost:7687'\nuser = 'neo4j'\npwd = 'my-password'\ngraph = Graph(uri, auth=(user, pwd))\nnodes_matcher = NodeMatcher(graph) # use in add_edges()"
  },
  {
    "objectID": "posts/cypher-python/index.html#find-node-numbers",
    "href": "posts/cypher-python/index.html#find-node-numbers",
    "title": "Using Cypher in Python",
    "section": "6 Find node numbers",
    "text": "6 Find node numbers\nprint('Number of nodes in graph: ', graph.nodes.match('Node').count())"
  },
  {
    "objectID": "posts/cypher-python/index.html#read-an-excel-file-using-pandas",
    "href": "posts/cypher-python/index.html#read-an-excel-file-using-pandas",
    "title": "Using Cypher in Python",
    "section": "7 Read an excel file using pandas",
    "text": "7 Read an excel file using pandas\nexcel_file = 'data.xlsx'\ndf0 = pd.read_excel(excel_file, sheet_name=0, skiprows=0, header=0, nrows=None)\ndf0 = df0.applymap(lambda x:remove_special_characters(x) if type(x) == str else x).fillna('')\nprint(len(df0))"
  },
  {
    "objectID": "posts/cypher-python/index.html#read-an-excel-file-using-pandas-1",
    "href": "posts/cypher-python/index.html#read-an-excel-file-using-pandas-1",
    "title": "Using Cypher in Python",
    "section": "8 Read an excel file using pandas",
    "text": "8 Read an excel file using pandas\nherb_name = \"Ginger\"\n[df_name, df_information] = df0\ndf_name.shape, df_information.shape"
  },
  {
    "objectID": "posts/cypher-python/index.html#read-an-excel-file-using-pandas-2",
    "href": "posts/cypher-python/index.html#read-an-excel-file-using-pandas-2",
    "title": "Using Cypher in Python",
    "section": "9 Read an excel file using pandas",
    "text": "9 Read an excel file using pandas\nnode_herb = Node(\"Herb\", name=herb_name)\n\ntuples = create_tuple_list(df_name)\nfor tuple in tuples:\n\n  for index, value in enumerate(tuple):\n\n    node_nomen_cat = Node(\"Nomenclature_Category\", name=tuple[ind_nomen-1])\n    node_ref = Node(\"Citation\", name=tuple[ind_ref], link=tuple[ind_ref+1])\n\n    if (index &gt;= ind_nomen and index &lt; ind_ref and value != ''):\n  \n      graph.merge(node_herb, \"Herb\", \"name\")\n      graph.merge(node_nomen_cat, \"Nomenclature_Category\", \"name\")\n      graph.merge(node_ref, \"Citation\", \"link\")\n\n      node_nomen = Node(\"Nomenclature\", name=tuple[index])\n\n      rel1 = Relationship(node_herb, \"IS_CALLED\", node_nomen)\n      rel2 = Relationship(node_nomen, \"CATEGORIZED\", node_nomen_cat)\n      rel3 = Relationship(node_nomen, \"CITE\", node_ref)\n\n      graph.create(rel1)\n      graph.create(rel2)\n      graph.create(rel3)"
  },
  {
    "objectID": "posts/cypher-python/index.html#check-data-in-the-database",
    "href": "posts/cypher-python/index.html#check-data-in-the-database",
    "title": "Using Cypher in Python",
    "section": "10 Check data in the database",
    "text": "10 Check data in the database\nquery = '''\nmatch (n:Herb {name: %s})-[:IS_CALLED]-(m)\nreturn m\n''' % herb_name\n\nresult = graph.run(query)\n\nlist(result)"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html#create-a-database-connection-pagesgetdata.ts",
    "href": "posts/neo4j-nextjs/index.html#create-a-database-connection-pagesgetdata.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "3 Create a database connection pages/getdata.ts",
    "text": "3 Create a database connection pages/getdata.ts\nimport { NextApiRequest, NextApiResponse } from \"next\";\nimport neo4j, { Driver, Session, Result, Record } from \"neo4j-driver\";\nimport { log } from \"console\";\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\n// Define the type for properties\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\n// API route handler\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  const session: Session = driver.session();\n\n  try {\n    // Perform your Neo4j queries and logic here\n    const result = await session.run(\"MATCH (n) RETURN n LIMIT 5\");\n    \n    // const nodes: Record&lt;string, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n    const nodes: Dict&lt;PropertyKey, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n\n    console.log(nodes);\n\n    const jsonData = JSON.stringify(nodes);\n\n    res.status(200).json({ data: jsonData });\n\n  } catch (error) {\n    console.error(\"Error executing Neo4j query:\", error);\n    res.status(500).json({ error: \"Internal Server Error\" });\n  } finally {\n    session.close();\n  }\n}"
  },
  {
    "objectID": "posts/neo4j-nextjs/index.html#create-a-database-connection-pagesgraph-data.ts",
    "href": "posts/neo4j-nextjs/index.html#create-a-database-connection-pagesgraph-data.ts",
    "title": "Create an API of Neo4j Database in Next.js",
    "section": "3 Create a database connection pages/graph-data.ts",
    "text": "3 Create a database connection pages/graph-data.ts\nimport { NextApiRequest, NextApiResponse } from \"next\";\nimport neo4j, { Driver, Session, Result, Record } from \"neo4j-driver\";\nimport { log } from \"console\";\n\nconst driver = neo4j.driver(\n  \"bolt://44.201.240.207:7687\",\n  neo4j.auth.basic(\"neo4j\", \"job-returns-machines\")\n);\n\n// Define the type for properties\ntype Dict&lt;T extends PropertyKey, U&gt; = {\n  [K in T]: U;\n};\n\n// API route handler\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  const session: Session = driver.session();\n\n  try {\n    // Perform your Neo4j queries and logic here\n    const result = await session.run(\"MATCH (n) RETURN n LIMIT 5\");\n    \n    // const nodes: Record&lt;string, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n    const nodes: Dict&lt;PropertyKey, any&gt;[] = result.records.map(record =&gt; record.get('n').properties);\n\n    console.log(nodes);\n\n    const jsonData = JSON.stringify(nodes);\n\n    res.status(200).json({ data: jsonData });\n\n  } catch (error) {\n    console.error(\"Error executing Neo4j query:\", error);\n    res.status(500).json({ error: \"Internal Server Error\" });\n  } finally {\n    session.close();\n  }\n}"
  },
  {
    "objectID": "posts/nextjs-redux/index.html",
    "href": "posts/nextjs-redux/index.html",
    "title": "Using Redux in Next.js",
    "section": "",
    "text": "Create the slices of states and hooks.\nimport { createSlice } from \"@reduxjs/toolkit\";\nimport type { PayloadAction } from \"@reduxjs/toolkit\";\n\nexport interface SearchState {\n  term: string;\n}\nconst initialState: SearchState = {\n  term: \"\",\n};\nconst searchSlice = createSlice({\n  name: \"search\",\n  initialState,\n  reducers: {\n    setTerm(state, action: PayloadAction&lt;string&gt;) {\n      state.term = action.payload.trim();\n    },\n  },\n});\n\nexport const {\n  setTerm,\n} = searchSlice.actions;\nexport default searchSlice.reducer;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-providers-for-a-redux-store",
    "href": "posts/nextjs-redux/index.html#create-providers-for-a-redux-store",
    "title": "Using Redux in Next.js",
    "section": "4 Create Providers for a Redux store",
    "text": "4 Create Providers for a Redux store\nThe Providers component used for wrapping components as a parent component that child components can import states and hooks just by importing (not as a component props).\n\"use client\";\nimport { Provider } from \"react-redux\";\nimport { store } from \"./store\";\n\nexport default function Providers({ children }: { children: React.ReactNode }) {\n  return &lt;Provider store={store}&gt;{children}&lt;/Provider&gt;;\n}"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-a-redux-store",
    "href": "posts/nextjs-redux/index.html#create-a-redux-store",
    "title": "Using Redux in Next.js",
    "section": "3 Create a Redux store",
    "text": "3 Create a Redux store\nCreate a Redux store for state and api slices.\nimport { configureStore } from \"@reduxjs/toolkit\"\nimport { useDispatch, useSelector } from \"react-redux\";\nimport type { TypedUseSelectorHook } from \"react-redux\";\n\nimport searchReducer from \"./sliceStateSearch\"\nimport { herbApi } from \"./sliceApiHerb\";\n\nexport const store = configureStore({\n  reducer: {\n    search: searchReducer,\n    [herbApi.reducerPath]: herbApi.reducer,\n  },\n  middleware(getDefaultMiddleware) {\n    return getDefaultMiddleware().concat(\n      herbApi.middleware\n    );\n  }\n  \n});\n\ntype AppDispatch = typeof store.dispatch;\ntype RootState = ReturnType&lt;typeof store.getState&gt;;\nexport const useAppDispatch = () =&gt; useDispatch&lt;AppDispatch&gt;();\nexport const useAppSelector: TypedUseSelectorHook&lt;RootState&gt; = useSelector;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-a-redux-store-1",
    "href": "posts/nextjs-redux/index.html#create-a-redux-store-1",
    "title": "Using Redux in Next.js",
    "section": "4 Create a Redux store",
    "text": "4 Create a Redux store\nCreate a Redux store for state and api slices.\nimport { configureStore } from \"@reduxjs/toolkit\"\nimport { useDispatch, useSelector } from \"react-redux\";\nimport type { TypedUseSelectorHook } from \"react-redux\";\n\nimport searchReducer from \"./sliceStateSearch\"\nimport { herbApi } from \"./sliceApiHerb\";\n\nexport const store = configureStore({\n  reducer: {\n    search: searchReducer,\n    [herbApi.reducerPath]: herbApi.reducer,\n  },\n  middleware(getDefaultMiddleware) {\n    return getDefaultMiddleware().concat(\n      herbApi.middleware\n    );\n  }\n  \n});\n\ntype AppDispatch = typeof store.dispatch;\ntype RootState = ReturnType&lt;typeof store.getState&gt;;\nexport const useAppDispatch = () =&gt; useDispatch&lt;AppDispatch&gt;();\nexport const useAppSelector: TypedUseSelectorHook&lt;RootState&gt; = useSelector;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-a-redux-store-2",
    "href": "posts/nextjs-redux/index.html#create-a-redux-store-2",
    "title": "Using Redux in Next.js",
    "section": "4 Create a Redux store",
    "text": "4 Create a Redux store\nCreate a Redux store for state and api slices.\nimport { configureStore } from \"@reduxjs/toolkit\"\nimport { useDispatch, useSelector } from \"react-redux\";\nimport type { TypedUseSelectorHook } from \"react-redux\";\n\nimport searchReducer from \"./sliceStateSearch\"\nimport { herbApi } from \"./sliceApiHerb\";\n\nexport const store = configureStore({\n  reducer: {\n    search: searchReducer,\n    [herbApi.reducerPath]: herbApi.reducer,\n  },\n  middleware(getDefaultMiddleware) {\n    return getDefaultMiddleware().concat(\n      herbApi.middleware\n    );\n  }\n  \n});\n\ntype AppDispatch = typeof store.dispatch;\ntype RootState = ReturnType&lt;typeof store.getState&gt;;\nexport const useAppDispatch = () =&gt; useDispatch&lt;AppDispatch&gt;();\nexport const useAppSelector: TypedUseSelectorHook&lt;RootState&gt; = useSelector;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-rtk-queries",
    "href": "posts/nextjs-redux/index.html#create-rtk-queries",
    "title": "Using Redux in Next.js",
    "section": "2 Create RTK queries",
    "text": "2 Create RTK queries\nThe api slices with endpoints for various use cases are managed by Redux for caching, validating, mutating, etc. .\nimport { DataProps } from \"@/interfaces/graph\";\nimport { createApi, fetchBaseQuery } from \"@reduxjs/toolkit/query/react\";\nimport { SearchNodesApiProps } from \"@/interfaces\";\n\nexport const herbApi = createApi({\n  reducerPath: \"herbApiSlice\",\n  // baseQuery: fetchBaseQuery({ baseUrl: \"/api\"}),\n  baseQuery: fetchBaseQuery({ baseUrl: \"/graph-db/api\"}), //check next.config \n  tagTypes: [\"herbs\"],\n  endpoints: (builder) =&gt; ({\n\n    herbs: builder.query&lt;DataProps, void&gt;({\n      query: () =&gt; \"\",\n      providesTags: (result, error, search) =&gt; [{ type: \"herbs\", search}]\n    }),\n\n    searchNode: builder.mutation&lt;DataProps, SearchNodesApiProps&gt;({\n      query: (body) =&gt; ({\n        url: \"/post-search-node\",\n        method: \"POST\",\n        body,\n      }),\n    }),\n\n  })\n})\n\nexport const { useHerbsQuery, useSearchNodeMutation } = herbApi;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#create-states-and-hooks",
    "href": "posts/nextjs-redux/index.html#create-states-and-hooks",
    "title": "Using Redux in Next.js",
    "section": "",
    "text": "Create the slices of states and hooks.\nimport { createSlice } from \"@reduxjs/toolkit\";\nimport type { PayloadAction } from \"@reduxjs/toolkit\";\n\nexport interface SearchState {\n  term: string;\n}\nconst initialState: SearchState = {\n  term: \"\",\n};\nconst searchSlice = createSlice({\n  name: \"search\",\n  initialState,\n  reducers: {\n    setTerm(state, action: PayloadAction&lt;string&gt;) {\n      state.term = action.payload.trim();\n    },\n  },\n});\n\nexport const {\n  setTerm,\n} = searchSlice.actions;\nexport default searchSlice.reducer;"
  },
  {
    "objectID": "posts/nextjs-redux/index.html#wrap-providers-in-the-app",
    "href": "posts/nextjs-redux/index.html#wrap-providers-in-the-app",
    "title": "Using Redux in Next.js",
    "section": "5 Wrap Providers in the app",
    "text": "5 Wrap Providers in the app\nimport Providers from \"@/context/Providers\";\n\nexport default function Home() {\n\n  return (\n    &lt;main&gt;\n      &lt;Providers&gt;\n        ...\n      &lt;/Providers&gt;\n    &lt;/main&gt;\n  );\n}"
  },
  {
    "objectID": "posts/using-rsync/index.html",
    "href": "posts/using-rsync/index.html",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync [options] source destination\n\n\n\nwhich rsync\n\n# If rsync is installed =&gt;\n# /usr/bin/rsync\n\n\n\nrsync -av ~/aaa ubuntu@slave-1:~/\n\n# Copy file `aaa` to remote machine\nrsync -avzphi ~/aaa ubuntu@slave-1:~/\n\n# Copy file from remote machine\nrsync -avzphi ubuntu@slave-1:~/aaa ~/\n\n\n\nman rsync\n\n\n\nparam\nduty\n\n\n\n\na\narchive\n\n\nv\nverbose\n\n\nz\ncompress\n\n\np\npermissions\n\n\nh\nhuman-readable\n\n\ni\nitemize-changes\n\n\n\n\n\n\n\n\nrsync -avzphi /cygdrive/c/users/Jorgnur/Desktop/xxx ubuntu@192.168.1.120:~/\n\n\n\nrsync -avzphi ubuntu@192.168.1.120:~/xxx /cygdrive/c/users/Jorgnur/Desktop/\n\n\n\npdsh -w ^hosts.txt rsync -avzphi ~/aaa ubuntu@%:~/\npdsh -H \"$(cat hosts.txt | tr '\\n' ',')\" \"rsync -avzphi ~/aaa ubuntu@%:~/\"\n# delete the files that contain `my-dir`\nfind . -name \"*my-dir*\" -exec rm {} \\;\nfind . -type f -name \"*my-dir*\" -delete"
  },
  {
    "objectID": "posts/using-rsync/index.html#display-database-schema",
    "href": "posts/using-rsync/index.html#display-database-schema",
    "title": "Cypher Query",
    "section": "",
    "text": "CALL db.schema.visualization()"
  },
  {
    "objectID": "posts/using-rsync/index.html#syntax",
    "href": "posts/using-rsync/index.html#syntax",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync [options] source destination"
  },
  {
    "objectID": "posts/using-rsync/index.html#check-if-rsync-is-installed-or-not",
    "href": "posts/using-rsync/index.html#check-if-rsync-is-installed-or-not",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "which rsync\n\n# If rsync is installed =&gt;\n# /usr/bin/rsync"
  },
  {
    "objectID": "posts/using-rsync/index.html#example",
    "href": "posts/using-rsync/index.html#example",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync -av ~/aaa ubuntu@slave-1:~/\n\n# Copy file `aaa` to remote machine\nrsync -avzphi ~/aaa ubuntu@slave-1:~/\n\n# Copy file from remote machine\nrsync -avzphi ubuntu@slave-1:~/aaa ~/"
  },
  {
    "objectID": "posts/using-rsync/index.html#get-help",
    "href": "posts/using-rsync/index.html#get-help",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "man rsync\n\n\n\nparam\nduty\n\n\n\n\na\narchive\n\n\nv\nverbose\n\n\nz\ncompress\n\n\np\npermissions\n\n\nh\nhuman-readable\n\n\ni\nitemize-changes"
  },
  {
    "objectID": "posts/using-rsync/index.html#example-for-cygwin",
    "href": "posts/using-rsync/index.html#example-for-cygwin",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync -avzphi /cygdrive/c/users/Jorgnur/Desktop/xxx ubuntu@192.168.1.120:~/\n\n\n\nrsync -avzphi ubuntu@192.168.1.120:~/xxx /cygdrive/c/users/Jorgnur/Desktop/"
  },
  {
    "objectID": "posts/using-multipass/index.html",
    "href": "posts/using-multipass/index.html",
    "title": "Using Multipass",
    "section": "",
    "text": "multipass shell &lt;image-name&gt;\n\n\n\nCreate Ubuntu Server image with yaml file. The word focal refers to Ubuntu 20.04 LTS.\nmultipass launch --name hadoop0 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\nmultipass launch --name hadoop1 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\nmultipass launch --name hadoop2 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\n\nmultipass launch --name node0 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml --disk 10G --mem 4G --cpus 2 focal\n\nmultipass launch --name node0 --network Wi-Fi --cloud-init setup-ubuntu.yaml --disk 10G --mem 4G --cpus 2 focal\npackage_update: true\npackage_upgrade: true\npackages:\n  - openjdk-8-jdk\n\n\n\nDelete created images\nmultipass delete &lt;image-name&gt;\n\n\n\nSuppose I has a text file (test.txt) and I want to transfer a file from a Window system into Ubuntu VM\nmultipass transfer test.txt hadoop0:/home/ubuntu/\n# or\nmultipass transfer C:\\Users\\Jorgnur\\Downloads\\hadoop-3.3.5.tar.gz hadoop0:/home/ubuntu/\nSuppose I has a folder (test) and I want to transfer a file from a Window system into Ubuntu VM\nmultipass transfer -r test hadoop0:/home/ubuntu/\n\n\n\nIf you do not set the password at first when creating VM, use the following command to set new password.\nsudo passwd ubuntu\n\n\n\nThe IP address assigned to instances is dynamic by default, to assign the IP address use this command\nmultipass set instance-name --address 192.168.1.22\n#\nmultipass set instance-name --address 192.168.1.22\n\n\n\nStop the instance you want to modify:\nmultipass stop hadoop0\nUpdate new CPUs, disk and memory to hadoop0\n$ multipass stop hadoop0\n$ multipass set local.hadoop0.cpus=4\n$ multipass set local.hadoop0.disk=10G\n$ multipass set local.hadoop0.memory=2G\nIncrease the amount of RAM:"
  },
  {
    "objectID": "posts/using-multipass/index.html#start-shell-image",
    "href": "posts/using-multipass/index.html#start-shell-image",
    "title": "Using Multipass",
    "section": "",
    "text": "multipass shell &lt;image-name&gt;"
  },
  {
    "objectID": "posts/using-multipass/index.html#create-virtual-machine-with-yaml",
    "href": "posts/using-multipass/index.html#create-virtual-machine-with-yaml",
    "title": "Using Multipass",
    "section": "",
    "text": "Create Ubuntu Server image with yaml file. The word focal refers to Ubuntu 20.04 LTS.\nmultipass launch --name hadoop0 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\nmultipass launch --name hadoop1 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\nmultipass launch --name hadoop2 --network Wi-Fi --cloud-init setup-ubuntu.yaml focal\n\nmultipass launch --name node0 --network Wi-Fi --cloud-init C:\\Users\\Jorgnur\\Desktop\\Clound\\multipass\\setup-ubuntu.yaml --disk 10G --mem 4G --cpus 2 focal\n\nmultipass launch --name node0 --network Wi-Fi --cloud-init setup-ubuntu.yaml --disk 10G --mem 4G --cpus 2 focal\npackage_update: true\npackage_upgrade: true\npackages:\n  - openjdk-8-jdk"
  },
  {
    "objectID": "posts/using-multipass/index.html#delete-images",
    "href": "posts/using-multipass/index.html#delete-images",
    "title": "Using Multipass",
    "section": "",
    "text": "Delete created images\nmultipass delete &lt;image-name&gt;"
  },
  {
    "objectID": "posts/using-multipass/index.html#transfer-a-file",
    "href": "posts/using-multipass/index.html#transfer-a-file",
    "title": "Using Multipass",
    "section": "",
    "text": "Suppose I has a text file (test.txt) and I want to transfer a file from a Window system into Ubuntu VM\nmultipass transfer test.txt hadoop0:/home/ubuntu/\n# or\nmultipass transfer C:\\Users\\Jorgnur\\Downloads\\hadoop-3.3.5.tar.gz hadoop0:/home/ubuntu/\nSuppose I has a folder (test) and I want to transfer a file from a Window system into Ubuntu VM\nmultipass transfer -r test hadoop0:/home/ubuntu/"
  },
  {
    "objectID": "posts/using-multipass/index.html#set-user-password",
    "href": "posts/using-multipass/index.html#set-user-password",
    "title": "Using Multipass",
    "section": "",
    "text": "If you do not set the password at first when creating VM, use the following command to set new password.\nsudo passwd ubuntu"
  },
  {
    "objectID": "posts/using-multipass/index.html#set-a-fixed-ip-address-for-your-instances",
    "href": "posts/using-multipass/index.html#set-a-fixed-ip-address-for-your-instances",
    "title": "Using Multipass",
    "section": "",
    "text": "The IP address assigned to instances is dynamic by default, to assign the IP address use this command\nmultipass set instance-name --address 192.168.1.22\n#\nmultipass set instance-name --address 192.168.1.22"
  },
  {
    "objectID": "posts/using-multipass/index.html#add-more-resource",
    "href": "posts/using-multipass/index.html#add-more-resource",
    "title": "Using Multipass",
    "section": "",
    "text": "Stop the instance you want to modify:\nmultipass stop hadoop0\nUpdate new CPUs, disk and memory to hadoop0\n$ multipass stop hadoop0\n$ multipass set local.hadoop0.cpus=4\n$ multipass set local.hadoop0.disk=10G\n$ multipass set local.hadoop0.memory=2G\nIncrease the amount of RAM:"
  },
  {
    "objectID": "posts/using-rsync/index.html#examples",
    "href": "posts/using-rsync/index.html#examples",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync -av ~/aaa ubuntu@slave-1:~/\n\n# Copy file `aaa` to remote machine\nrsync -avzphi ~/aaa ubuntu@slave-1:~/\n\n# Copy file from remote machine\nrsync -avzphi ubuntu@slave-1:~/aaa ~/"
  },
  {
    "objectID": "posts/using-rsync/index.html#examples-for-cygwin",
    "href": "posts/using-rsync/index.html#examples-for-cygwin",
    "title": "Using rsync in Linux",
    "section": "",
    "text": "rsync -avzphi /cygdrive/c/users/Jorgnur/Desktop/xxx ubuntu@192.168.1.120:~/\n\n\n\nrsync -avzphi ubuntu@192.168.1.120:~/xxx /cygdrive/c/users/Jorgnur/Desktop/\n\n\n\npdsh -w ^hosts.txt rsync -avzphi ~/aaa ubuntu@%:~/\npdsh -H \"$(cat hosts.txt | tr '\\n' ',')\" \"rsync -avzphi ~/aaa ubuntu@%:~/\"\n# delete the files that contain `my-dir`\nfind . -name \"*my-dir*\" -exec rm {} \\;\nfind . -type f -name \"*my-dir*\" -delete"
  },
  {
    "objectID": "posts/using-hadoop/index.html",
    "href": "posts/using-hadoop/index.html",
    "title": "Using Hadoop",
    "section": "",
    "text": "# check JAVA is running\njps\n\nhdfs namenode -format\nstart-dfs.sh\njps\n\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/john\nhdfs dfs -mkdir /input\nhdfs dfs -put ~/hadoop-3.3.5/etc/hadoop/*.xml /input\nhdfs dfs -ls /input\n\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep /input /output 'dfs[a-z.]+'\nhdfs dfs -ls /output\nhdfs dfs -cat /output/*\n\n# can also copy the output directory to `~/output`\nhdfs dfs -get /output ~/output\ncat ~/output/*\n\nstop-dfs.sh\n\n\n\nstart-yarn.sh\nResourceManager and NodeManager will show up\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ jps\n14357 SecondaryNameNode\n13943 NameNode\n14135 DataNode\n14569 Jps\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ start-yarn.sh\nStarting resourcemanager\nStarting nodemanagers\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ jps\n14833 NodeManager\n14357 SecondaryNameNode\n13943 NameNode\n14135 DataNode\n15145 Jps\n14652 ResourceManager\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$\nBrowse the web interface for the ResourceManager\nhttp://localhost:8088\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep input output 'dfs[a-z.]+'"
  },
  {
    "objectID": "posts/using-hadoop/index.html#run-hadoop-examples",
    "href": "posts/using-hadoop/index.html#run-hadoop-examples",
    "title": "Using Hadoop",
    "section": "",
    "text": "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep input output 'dfs[a-z.]+'"
  },
  {
    "objectID": "posts/using-hadoop/index.html#create-dir-in-hdfs",
    "href": "posts/using-hadoop/index.html#create-dir-in-hdfs",
    "title": "Using Hadoop",
    "section": "",
    "text": "Create dir in HDFS\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/john\nhdfs dfs -mkdir /input"
  },
  {
    "objectID": "posts/using-hadoop/index.html#verify-hdfs-file-system",
    "href": "posts/using-hadoop/index.html#verify-hdfs-file-system",
    "title": "Using Hadoop",
    "section": "",
    "text": "Verify HDFS file system\nstop-dfs.sh\n\nhdfs namenode -format\nstart-dfs.sh\njps\n\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/john\nhdfs dfs -mkdir /input\nhdfs dfs -put etc/hadoop/*.xml /input\nhdfs dfs -ls /input\n\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep /input /output 'dfs[a-z.]+'"
  },
  {
    "objectID": "posts/using-hadoop/index.html#run-mapreduce-examples",
    "href": "posts/using-hadoop/index.html#run-mapreduce-examples",
    "title": "Using Hadoop",
    "section": "",
    "text": "# check JAVA is running\njps\n\nhdfs namenode -format\nstart-dfs.sh\njps\n\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/john\nhdfs dfs -mkdir /input\nhdfs dfs -put ~/hadoop-3.3.5/etc/hadoop/*.xml /input\nhdfs dfs -ls /input\n\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep /input /output 'dfs[a-z.]+'\nhdfs dfs -ls /output\nhdfs dfs -cat /output/*\n\n# can also copy the output directory to `~/output`\nhdfs dfs -get /output ~/output\ncat ~/output/*\n\nstop-dfs.sh"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#obtain-public-key-of-each",
    "href": "posts/setup-hadoop-cluster/index.html#obtain-public-key-of-each",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "2.4 Obtain public key of each",
    "text": "2.4 Obtain public key of each\nUsing pdsh and regular expression to retreive the rsa public key of each machine\npdsh -w node[1-2] cat ~/.ssh/id_rsa.pub | sed 's/[^:]*: //' &gt;&gt; text.txt"
  },
  {
    "objectID": "posts/using-hadoop/index.html#solving-between-conflict-between-namenode-clusterid-and-datanode-clusterid",
    "href": "posts/using-hadoop/index.html#solving-between-conflict-between-namenode-clusterid-and-datanode-clusterid",
    "title": "Using Hadoop",
    "section": "",
    "text": "Solving between conflict between Namenode clusterID and Datanode clusterID, the problem is shown in log file as\n\n\nlogs/hadoop-ubuntu-datanode-node0.log\n\n...\n2023-06-22 06:23:24,449 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data\njava.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-ec7e5f04-e16f-4a33-afeb-ce45032aa09c; datanode clusterID = CID-8056152a-0184-4cfe-8fbe-b16cf655140e\n2023-06-22 06:23:24,460 ERROR\n\nvi /tmp/hadoop-ubuntu/dfs/data/current/VERSION\n\n\nVERSION\n\n#Wed Jun 21 22:47:29 ICT 2023\nstorageID=DS-87747f6d-f615-4b10-97ec-019c72544fe8\nclusterID=CID-ec7e5f04-e16f-4a33-afeb-ce45032aa09c\ncTime=0\ndatanodeUuid=3a51fe58-e17d-4ce9-8598-e93378acad0c\nstorageType=DATA_NODE\nlayoutVersion=-57"
  },
  {
    "objectID": "posts/using-hadoop/index.html#conflict-in-clusterids",
    "href": "posts/using-hadoop/index.html#conflict-in-clusterids",
    "title": "Using Hadoop",
    "section": "2.1 Conflict in clusterIDs",
    "text": "2.1 Conflict in clusterIDs\nIf you reinstall or recreate the Hadoop cluster on Multipass Ubuntu, the clusterID might change. The Datanode can not be started. Solving between conflict between Namenode clusterID and Datanode clusterID,\nchecking log with the command\ncat ~/hadoop-3.3.5/logs/hadoop-ubuntu-datanode-node0.log\nthe problem is shown in log file as\n\n\nlogs/hadoop-ubuntu-datanode-node0.log\n\n...\n2023-06-22 06:23:24,449 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data\njava.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-ec7e5f04-e16f-4a33-afeb-ce45032aa09c; datanode clusterID = CID-8056152a-0184-4cfe-8fbe-b16cf655140e\n2023-06-22 06:23:24,460 ERROR\n\nEdit the VERSION file with\nvi /tmp/hadoop-ubuntu/dfs/data/current/VERSION\nChange the clusterID in file as in the log as clusterID = CID-ec7e5f04-e16f-4a33-afeb-ce45032aa09c.\n\n\nVERSION\n\n#Wed Jun 21 22:47:29 ICT 2023\nstorageID=DS-87747f6d-f615-4b10-97ec-019c72544fe8\nclusterID=CID-ec7e5f04-e16f-4a33-afeb-ce45032aa09c\ncTime=0\ndatanodeUuid=3a51fe58-e17d-4ce9-8598-e93378acad0c\nstorageType=DATA_NODE\nlayoutVersion=-57"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configuration-files",
    "href": "posts/setup-hadoop-cluster/index.html#configuration-files",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "2.5 Configuration files",
    "text": "2.5 Configuration files\nHadoopâ€™s Java configuration is driven by two types of important configuration files\nRead-only default configuration\nhadoop-3.3.5\\share\\doc\\hadoop\\hadoop-project-dist\\hadoop-common\\core-default.xml\nhadoop-3.3.5\\share\\doc\\hadoop\\hadoop-project-dist\\hadoop-hdfs\\hdfs-default.xml\nhadoop-3.3.5\\share\\doc\\hadoop\\hadoop-yarn\\hadoop-yarn-common\\yarn-default.xml\nhadoop-3.3.5\\share\\doc\\hadoop\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\mapred-default.xml\nSite-specific configuration\nhadoop-3.3.5/etc/hadoop/core-site.xml\nhadoop-3.3.5/etc/hadoop/hdfs-site.xml\nhadoop-3.3.5/etc/hadoop/yarn-site.xml\nhadoop-3.3.5/etc/hadoop/mapred-site.xml\nHDFS daemons are\nNameNode\nSecondaryNameNode\nDataNode\nYARN daemons are\nResourceManager\nNodeManager\nWebAppProxy"
  },
  {
    "objectID": "posts/setup-hadoop-cluster/index.html#configuring-environment-of-hadoop-daemons",
    "href": "posts/setup-hadoop-cluster/index.html#configuring-environment-of-hadoop-daemons",
    "title": "Setup Hadoop Cluster in Ubuntu Multipass",
    "section": "2.6 Configuring Environment of Hadoop Daemons",
    "text": "2.6 Configuring Environment of Hadoop Daemons\nAdministrators should use the\netc/hadoop/hadoop-env.sh\netc/hadoop/mapred-env.sh\netc/hadoop/yarn-env.sh\nto do site-specific customization of the Hadoop daemonsâ€™ process environment.\nDo I need the specify the JAVA_HOME to all these etc/hadoop/hadoop-env.sh etc/hadoop/mapred-env.sh etc/hadoop/yarn-env.sh in Hadoop 3.3.5 ?"
  },
  {
    "objectID": "posts/using-hadoop/index.html#incompatibility-issue",
    "href": "posts/using-hadoop/index.html#incompatibility-issue",
    "title": "Using Hadoop",
    "section": "2.2 Incompatibility issue",
    "text": "2.2 Incompatibility issue\nThe error message you encountered in Hadoop indicates that the cluster ID of the NameNode does not match the cluster ID of the DataNode. This mismatch prevents the DataNode from successfully communicating with the NameNode.\nTo resolve this issue, you can follow these steps:\n\nStop all Hadoop services, including the NameNode and DataNode.\nDelete the contents of the directories specified in the dfs.namenode.name.dir and dfs.datanode.data.dir properties. In this case, you should delete the contents of /home/ubuntu/namenode and /home/ubuntu/datanode.\nAfter removing the contents of the directories specified, you need to ensure that the dfs.namenode.name.dir and dfs.datanode.data.dir properties have the same values across all nodes in your Hadoop cluster. Verify that these properties are identical on both the NameNode and DataNode.\nStart the Hadoop services, beginning with the NameNode and then the DataNode.\n\nThe cluster ID in Hadoop is a unique identifier assigned to each Hadoop cluster. It is generated when the NameNode is formatted for the first time. The cluster ID is stored in the VERSION file within the NameNodeâ€™s metadata directory.\nWhen a DataNode starts up, it reads the cluster ID from the VERSION file of the NameNode it connects to and compares it with its own cluster ID. If the cluster IDs do not match, it indicates that the DataNode belongs to a different Hadoop cluster. In such a case, the DataNode refuses to join the cluster and throws the â€œIncompatible clusterIDsâ€ error."
  },
  {
    "objectID": "posts/using-hadoop/index.html#matching-a-word-in-files",
    "href": "posts/using-hadoop/index.html#matching-a-word-in-files",
    "title": "Using Hadoop",
    "section": "",
    "text": "# check JAVA is running\njps\n\nhdfs namenode -format\nstart-dfs.sh\njps\n\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/john\nhdfs dfs -mkdir /input\nhdfs dfs -put ~/hadoop-3.3.5/etc/hadoop/*.xml /input\nhdfs dfs -ls /input\n\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep /input /output 'dfs[a-z.]+'\nhdfs dfs -ls /output\nhdfs dfs -cat /output/*\n\n# can also copy the output directory to `~/output`\nhdfs dfs -get /output ~/output\ncat ~/output/*\n\nstop-dfs.sh"
  },
  {
    "objectID": "posts/using-hadoop/index.html#starting-yarn",
    "href": "posts/using-hadoop/index.html#starting-yarn",
    "title": "Using Hadoop",
    "section": "",
    "text": "start-yarn.sh\nResourceManager and NodeManager will show up\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ jps\n14357 SecondaryNameNode\n13943 NameNode\n14135 DataNode\n14569 Jps\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ start-yarn.sh\nStarting resourcemanager\nStarting nodemanagers\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$ jps\n14833 NodeManager\n14357 SecondaryNameNode\n13943 NameNode\n14135 DataNode\n15145 Jps\n14652 ResourceManager\nubuntu@node0:~/hadoop-3.3.5/etc/hadoop$\nBrowse the web interface for the ResourceManager\nhttp://localhost:8088\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep input output 'dfs[a-z.]+'"
  }
]